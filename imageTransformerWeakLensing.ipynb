{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed531a95",
      "metadata": {
        "id": "ed531a95"
      },
      "source": [
        "# Implements encoder/decoder for weak lensing outputs\n",
        "\n",
        "The major idea is to see if I can compress the data in the snapshot files.\n",
        "The result is that the compression of many different algorithms based on CNNs (of different depths) is not so much different than averaging neighboring cells (as shown at the end).  This in retrospect is not so surprising as there are differences on the cell scale in the maps that make compression challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zQpe_fjWxGwO",
      "metadata": {
        "id": "zQpe_fjWxGwO"
      },
      "source": [
        "Set configurations for google COLAB if running there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "KEtElnI8fDj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtElnI8fDj1",
        "outputId": "59dde345-ea8f-426f-a56a-69affd2aaf3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "use_COLAB = 0 #1 is for on colab, and 2 is for on local machine but using colab\n",
        "\n",
        "if use_COLAB >= 1:\n",
        "  if use_COLAB == 2: # for running in VS CODE\n",
        "      from colabcode import ColabCode\n",
        "      ColabCode(port=10000)\n",
        "  #mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  WORK_AREA = '/content/gdrive/My Drive/weaklensing_ML/' #columbialensing/\n",
        "  os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSxBOjESwy_q",
      "metadata": {
        "id": "MSxBOjESwy_q"
      },
      "source": [
        "\n",
        "## extract tarfiles if necessary and set specs for run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a4a5315a",
      "metadata": {
        "id": "a4a5315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "#whether we are training or loading saved\n",
        "train = True\n",
        "load_saved = 1\n",
        "\n",
        "# Specify the directory containing the .tar files\n",
        "if use_COLAB >= 1:\n",
        "    directory_path = './columbialensing/'\n",
        "else:\n",
        "        directory_path = '../weaklensing_ML/columbialensing/'\n",
        "number_batches = 10\n",
        "#normalize_by_RMS = True #set to one if you want to renormalize by RMS\n",
        "\n",
        "# image_size\n",
        "image_size = 1024\n",
        "sub_image_size = 32 #needs to divide image into these units; must divide evenly image_size\n",
        "                    #division is using that it is unlikely there are learnable correlations\n",
        "                    #that allow one to compress the data on large scales in the images\n",
        "                    #dividing images gives more samples to learn correlations\n",
        "number_fits_files = 16 # just sto start\n",
        "\n",
        "\n",
        "number_subimages_across =image_size//sub_image_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extracts only if indicated (could make this more elegant by checking to see if they exist)\n",
        "extract_tarfiles = False  #if I need to extract tarfiles\n",
        "suffix = f\"_{image_size}\"\n",
        "run_suffix = rf\"im{image_size}\"\n",
        "if extract_tarfiles:\n",
        "    # Use a regular expression to match .tar files with the desired suffix\n",
        "    pattern = re.compile(rf\"{suffix}.tar$\")\n",
        "\n",
        "    # List all matching .tar files in the directory\n",
        "    all_tar_files = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "\n",
        "    # Extract the tar archive\n",
        "    for tar_file in all_tar_files:\n",
        "        #print(tar_file)\n",
        "        tar_file_path = os.path.join(directory_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as archive:\n",
        "            archive.extractall(path=directory_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0619681f",
      "metadata": {
        "id": "0619681f"
      },
      "source": [
        "# Read into memory the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "48a05090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a05090",
        "outputId": "de8579ef-f38a-414d-f362-c3a282901a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in Om0.268_si0.801\n",
            "0.01843\n",
            "RMS = True : std 0.01843\n"
          ]
        }
      ],
      "source": [
        "#parameters we need\n",
        "normalize_by_RMS = True #first time set to zero\n",
        "LOGFIELD = False\n",
        "\n",
        "def get_labels_for_file(dir_name):\n",
        "    \"\"\"\n",
        "    Extracts labels from the tar file name.\n",
        "    For the file \"Om0.183_si0.958_256.tar\", the labels will be [0.183, 0.958].\n",
        "\n",
        "    Args:\n",
        "    - tar_file_name (str): Name of the tar file.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the two labels extracted from the filename.\n",
        "    \"\"\"\n",
        "    # Split the filename on underscores\n",
        "    parts = dir_name.split('_')\n",
        "\n",
        "    # Extract the numeric values for 'Om' and 'si'\n",
        "    om_label = float(parts[0][2:])\n",
        "    si_label = float(parts[1][2:])\n",
        "\n",
        "    return [om_label, si_label]\n",
        "\n",
        "#now loop through all files in the\n",
        "pattern = re.compile(rf\"{suffix}$\")\n",
        "#all_directories = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "all_directories = [\"Om0.268_si0.801\"] # \"Om0.283_si0.805_256\"\n",
        "num_cosmologies = len(all_directories)\n",
        "\n",
        "random.shuffle(all_directories) #this makes it so that there is no particular order for the directories\n",
        "#print(all_directories)\n",
        "\n",
        "#tensor of labels; there are two labels for each\n",
        "numsubimages = number_subimages_across**2\n",
        "number_images = number_fits_files*numsubimages\n",
        "#cosmology_labels = np.empty((len(all_directories), number_images, 2), dtype=np.float16)\n",
        "\n",
        "RMS =0 #first time set to zero\n",
        "data_array = np.empty((num_cosmologies, number_images, sub_image_size, sub_image_size), dtype=np.float16)\n",
        "\n",
        "number_subimages_total = 0\n",
        "for idy, dir_name in enumerate(all_directories):\n",
        "\n",
        "\n",
        "    #if idy%10 ==0:\n",
        "    print(\"reading in\", dir_name)\n",
        "    dir_path = os.path.join(directory_path, dir_name)\n",
        "\n",
        "    all_files = os.listdir(dir_path)\n",
        "    fits_files = [f for f in all_files if f.endswith('.fits')]\n",
        "\n",
        "\n",
        "\n",
        "    for idx, file in enumerate(fits_files):\n",
        "        if idx >= number_fits_files:\n",
        "            break\n",
        "\n",
        "        with fits.open(os.path.join(dir_path, file)) as hdul:\n",
        "\n",
        "            original_data = hdul[0].data\n",
        "\n",
        "            #if RMS == 0: #get RMS to divide by for first file to normalize everything\n",
        "            #    RMS = np.sqrt(np.var(hdul[0].data))\n",
        "            #    print(f\"RMS={RMS}\")\n",
        "\n",
        "            ##get rid of NANs, which affects a few files\n",
        "            #if np.isnan(original_data).any():\n",
        "            #    continue\n",
        "            #I've cleaned this out already\n",
        "            for i in range(number_subimages_across):\n",
        "                for j in range(number_subimages_across):\n",
        "                    data_array[idy][numsubimages*idx+ number_subimages_across*i+j] = original_data[sub_image_size*i:sub_image_size*(i+1),\\\n",
        "                                                                  sub_image_size*j:sub_image_size*(j+1)]\n",
        "                number_subimages_total +=1\n",
        "\n",
        "\n",
        "\n",
        "    #since all fits files in one directory have the same label\n",
        "    cosmology = get_labels_for_file(dir_name)\n",
        "    #cosmology_labels[idy] = np.array([cosmology for i in range(number_fits_files)])\n",
        "\n",
        "\n",
        "    #flatten data_array[idy][numsubimages*idx+ number_subimages_across*i+j]\n",
        "\n",
        "if LOGFIELD:\n",
        "    meanf = np.mean(data_array)\n",
        "    stdf = np.std(data_array)\n",
        "    minf = np.min(data_array)\n",
        "    data_array = np.log((data_array-minf)/stdf + 1e-3)  #this was 1e-30 before 10/29/24, but 1e-3 makes sense to me\n",
        "    print(\"LOGFIELD = True :  mean, std, min\", meanf, stdf, minf)\n",
        "elif normalize_by_RMS:\n",
        "    stdf = np.std(data_array)\n",
        "    data_array = data_array/stdf\n",
        "    print(stdf)\n",
        "    print(\"normalize_by_RMS = True : std\", stdf)\n",
        "\n",
        "WL_tensor = tf.convert_to_tensor(data_array)\n",
        "\n",
        "WL_tensor = tf.reshape(WL_tensor, (-1, WL_tensor.shape[2], WL_tensor.shape[3]));\n",
        "\n",
        "WL_tensor = WL_tensor[..., np.newaxis]  # Add channel dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2f790bd8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZt0lEQVR4nO3db2yddf3G8aunPac97elpt26uMrUbQ0cUIwjRSIxIleAMEoOoURMQ/wQeSHCJD4zEkECyB4rEJdNgnMEHdEDwAWpMJCTMaESJxKhEsrhlc4Zmq2vpaXtOz5/29P49IH7CnLjvteymk9/7lZjA8bMv9/me+z5X7437oifLskwAAEgqrPcBAAAuHIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAq4IPz4xz9WT0+PnnvuufU+lNw0m0198Ytf1GWXXaaRkRFVKhW9613v0t69e7WysrLehwdIkvrW+wCA/y+azab++te/6qMf/ai2bdumQqGgZ555Rrt379azzz6rAwcOrPchAoQC8FrZuHGjfv/735/22h133KGRkRHt27dPDzzwgMbHx9fp6ICX8dtHuGB9/vOfV6VS0T/+8Q/dcMMNqlQq2rp1q773ve9Jkp5//nlNTk5qaGhIExMTZ/yk/dJLL+lrX/ua3vnOd6pSqaharWrXrl3685//fMY/6/jx47rxxhs1NDSkN7zhDdq9e7eefPJJ9fT06Fe/+tVps88++6w+8pGPaGRkRIODg7rmmmv029/+9pzf57Zt2yRJtVrtnNcAzhfuFHBB63a72rVrlz7wgQ/oW9/6lqampvSVr3xFQ0NDuvvuu/W5z31ON910kx588EHdcsstet/73qft27dLko4ePaonnnhCn/zkJ7V9+3bNzMzoBz/4ga655hq98MILuuiiiyRJjUZDk5OTOnHihO666y6Nj4/rwIEDOnjw4BnH8/TTT2vXrl268sordc8996hQKOihhx7S5OSkfvOb3+g973nPWd9Tp9PR4uKims2mnnvuOd1///2amJjQJZdccn43DzgXGXABeOihhzJJ2R/+8Id47dZbb80kZXv27InX5ufns3K5nPX09GSPPvpovH7o0KFMUnbPPffEa61WK+t2u6f9c44dO5b19/dn9957b7z2ne98J5OUPfHEE/Fas9nMLr300kxSdvDgwSzLsmxtbS1761vfml1//fXZ2tpazC4vL2fbt2/PrrvuuqT3+sgjj2SS4n9XXXVV9pe//CXp1wJ547ePcMH70pe+FH89OjqqnTt3amhoSJ/61Kfi9Z07d2p0dFRHjx6N1/r7+1UovHyKd7tdzc3NqVKpaOfOnfrjH/8Yc7/85S+1detW3XjjjfHawMCAvvzlL592HH/60590+PBhffazn9Xc3JxmZ2c1OzurRqOhD33oQ/r1r3+ttbW1s76fa6+9Vk899ZQef/xx3XHHHSoWi2o0Gv7GADngt49wQRsYGNDmzZtPe21kZERvetOb1NPTc8br8/Pz8fdra2vau3evvv/97+vYsWPqdrvx/42NjcVfHz9+XDt27DhjvX//7ZzDhw9Lkm699dZXPd6FhQVt2LDhv76nLVu2aMuWLZKkm2++WXv27NF1112nw4cP8wfNWHeEAi5ovb291uvZK/7rsnv27NE3v/lNfeELX9B9992njRs3qlAo6Ktf/WrST/T/7l+/5tvf/rYuv/zy/zhTqVTsdW+++Wbdfffd+ulPf6rbb7/d/vXA+UQo4HXrJz/5ia699lr96Ec/Ou31Wq2mTZs2xd9PTEzohRdeUJZlp90tHDly5LRft2PHDklStVrVhz/84fN2nM1mU9LLdxnAeuPPFPC61dvbe9qdgyQ9/vjjmp6ePu2166+/XtPT0/rZz34Wr7VaLf3whz88be7KK6/Ujh07dP/996ter5/xzzt16tR/PZ7Z2dkzjkeS9u/fL0m66qqr/vsbAl4D3CngdeuGG27Qvffeq9tuu01XX321nn/+eU1NTeniiy8+be7222/Xvn379JnPfEZ33XWX3vjGN2pqakoDAwOSFHcPhUJB+/fv165du/SOd7xDt912m7Zu3arp6WkdPHhQ1WpVP//5z1/1eB5++GE9+OCD+vjHP66LL75YS0tLevLJJ/XUU0/pYx/7mCYnJ/PbDCARoYDXrW984xtqNBo6cOCAHnvsMb373e/WL37xC339618/ba5Sqejpp5/WnXfeqb1796pSqeiWW27R1VdfrU984hMRDpL0wQ9+UL/73e903333ad++farX6xofH9d73/ves/55wPvf/34988wzeuSRRzQzM6O+vj7t3LlTDzzwgO68885c9gBw9WT/6X4WgL773e9q9+7devHFF7V169b1PhzgNUEoAHr5D3vL5XL8favV0hVXXKFut6u//e1v63hkwGuL3z4CJN100016y1veossvv1wLCwt6+OGHdejQIU1NTa33oQGvKUIB0Mv/BtL+/fs1NTWlbrert7/97Xr00Uf16U9/er0PDXhN8dtHAIDAcwoAgEAoAABC8p8puEVdfX3pf1zh/g7WK4vNzsb9b986a4+Ojlprdzqd5Fn3uKvVqjVfKpWSZ9/85jdba7/y3+s/m38voTsb57idz/Jc/KueIoV7LM418Wo9UK/GOW9f+W9kpXhlIeHZzMzMWGsvLS1Z8+12O3l2dXXVWtvZc/fzcb4n3A6v48ePn3WGOwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITkgqLl5WVr4f7+/uTZkZERa22nc8bldKC4e+J0mrh9UG4HitNN5fbC1Ov15Fn3fTodT8Vi0Vq7UPB+RnI6bdyOp7GxseTZjRs3WmtPTEwkzzpdU5J09OhRa97hdgg5XUnutezI89p0107BnQIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAkPw8tVMvIHmP9Xe7XWtt53F39zFw57jb7ba1tlPp4Dzqfi7zDrcCwNlDt0LDMTw8bM071RKSVwExODhorb1ly5bk2be97W3W2pdddlnybJ6VMrVazZqfm5uz5p1rwtlvKd9z3Lne8qjn4E4BABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAAAhuRzE7ZFxuP0qhUJ6ljn9NJLU6XSSZ1utlrW2w+lWkfwOFKe3ye1ucXp+3N6rkydPJs/W63VrbbefaGRkJHn2oosusta+9NJLk2evuOIKa22nK+nIkSPW2sViMXnW/ezd89C5hoaGhqy1ne/DRqNhre18B7n9aym4UwAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQkmsu3AoA5xF257FuSVpZWUmezfPR+N7eXmtt57hda2tr1rxbo3GhcD5Pt4bEPQ+dc7xSqVhrOzUKTu2L5NV/OLUi7vzS0pK1dl9f8teVJO+acKsonD13z0PnvMrjO4U7BQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABK9MJCdO14ckNZvN5Fm346dcLifP9vf3W2s7HSjFYtFae2BgwJrPk9vD5HC6j9xemJdeesmad87bzZs3W2ufOHEiedbd7xdffDF59siRI9bap06dSp7Nssxa2z3Hl5eXk2drtZq19tzcXPKs203lnOPud1AK7hQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhOSai06nk9tBuI/pO4/HuzUXzmPj7mP6zuPrTt2GJPX1eY0l7XY7eXZpacla26khcT97Zw/dz8epLpC8mouTJ09aazuf58zMjLW2s+du/cPCwkLybL1et9ZeXFy05p2ai0ajYa3tXJ9uPUee37UpuFMAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBILljJs6OmUPCyyekScbqMJKlSqSTPur1KxWIxeXZ4eNhau7e315p3+oycDhnJ7xxyOJ+ne165805HzbFjx6y13XPLkee16XQIueeV25XUarWSZ933ubKykjzrfnc651Ue1xp3CgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAABCcs2F8+i15NUulMtla22HU4kheY+7u1UEzvt0j9ut8+h2u7nMSlK73bbmHc555R63W0fgvM9SqWStffLkyeRZp85B8s4V9zxsNpvJs27NxYVyXrncCg33XDnfuFMAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBI7j6qVqvWwk6PTF9f8mFIklZWVpJn3U6TYrGYPOv2wqyuribPDg8PW2u7nD139kTyOmrc3h5nbbevy+ntkbx9cXuynH1xj9u5JgYHB621nWtzcXHRWtv9PJ09d7vDsizLZVby9ty9NlNwpwAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJBcgDMyMmItXCqV7INJNTs7mzzrdOVIXk9Jnt1HS0tL1tout3MoL24nkNP14uy35J8r3W43edbdb/fccjjv090Tt4cpT07Hk9u/5pxbTh+UJI2OjibP5tGRxp0CACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJD8bPfa2pq1sPOYvrt2tVpNnu10Otba7mP9Dudxd3dP3LoIh1MXIHmVAe7aTs1Ff3+/tXaetRhuzcXg4GDyrFuJsbCwkDybZ/WH8x4lqVDwfoYtl8vJs27NRaPRSJ51zytnvl6vW2un4E4BABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAAAhufDD7UCp1WrJs24HyujoaPKs2yF04sSJ5Nnl5WVrbacvxe1icTn74vYqOR01bvfRhcTpM8qzQ6hUKllrN5vNXI5DkrIss+YdTueZJF1yySXJs2431d///vfkWfdadvbc6bFKxZ0CACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJD8/LX7GLhTAeHUP0hejYJb0eA8Nl4sFq21HW79g1uj4NRcuFUhzrG4x726upo861YuuJUOnU7HmnfkWUPinLfueejsuXteDQwMWPNOvcT/6nnonrMpuFMAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBILgep1Wq5HcTo6Kg1XyqVkmfd7hbnWJxuFcnrj3KP2+1Acfqj3I4ap+vF7dRaWVlJns2zz0byj93h7LnzWbprVyoVa+1yuZw8Ozg4aK3tfPaSdPTo0eRZ9/vN6XZzvyec6yeP/jXuFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAACE5Oev3QoA5/F457FuyXtMv7+/31p7YGAgebbT6VhrO4/SuxUKzWbTmq9Wq8mzbuWG8+i9+5i+sy9uLYI7nyfnenNrLpyaGPfzca4f5zgk/3ui0Wgkz7rXsvMd5F4/4+PjybPuZ5+05nlfEQDwP4tQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABCSu4+63a61sNPd0teXfBiSpHK5nDzrdLFI0ujoaPKs060ieXvodrH09PRY805nirPfLrdvyNkXt/fK3UPn2J2uHMk7dvf6cd6n2zfkHIu73y6nt8ntJ3K/Dx3Dw8PJs+45noI7BQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhAui+8jldPE4HT+S1Gq1kmeXlpastZ2unHa7ba3t9t84++L29jh76HbrOMft9sKUSiVr3uF26zjc415eXk6edfuJnD13e8ncPXS+J4aGhqy1p6enk2fd78LFxcXk2Tz6o7hTAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABCSuxGKxaK3sFG74D7u7lQduI+YLywsJM+6NRfDw8PJs+7j606FhuTti7uHzWYzebbT6VhrO/NuxYlTiyBJExMTybNu5cbMzEzyrPv5OLUl7p44171bz+HuofOd5Z4rTi3G/Py8tbbz2bvfyym4UwAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQEguKnE7UBxub4/TreP0vEhSvV5Pnl1eXrbWdvYwyzJrbfd9tlqt5Fmn50WSent7k2fb7ba1tnPcLrdHxtmXSqVird1oNHKZlbx+Imf2XOYd7veEc2453ykut+PJOZY8rgfuFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAACE3J5JX11dTZ6dn5+31q7VasmzbnWBo9PpWPNOLUbe9QJOjYZbo+DUC7h76FQduNUfbtXB9PR08my1WrXWds4VtyrEuTadWSnfeg73WJxrolDwfj7u6elJnnVrLtYbdwoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAjJ5SBjY2PWwk5PyalTp6y1nb4cp6PE5XaxXCjHLXnH4vYTdbvd5Fl3D50+I3cP3WOZnZ1Nnm21WtbaAwMD1rzD6QRy92Rubi551v183D1x3ufw8LC1ttOV5OyJ5F1vTodZKu4UAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITk58DHx8ethWu1WvLs/Py8tbbz6H273bbWXllZyeU4JKlUKlnzDreKotlsJs+6dQTO+3QqMSSvXsDlfPaSV11RLBattfN8n86xuOe4c7251RKVSsWaHxwcTJ5199upl3DrOZaXl5Nn3es+BXcKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIyd1HbjdIuVxOnt24caO1ttOr5HT8SN77dDtNnL4Ul7v22tpaLrOSt4e9vb3W2k5XUn9/v7W2O+908TjXg+SdW+4e5tnv5eyhe846XUaStHnz5uRZ93uiXq8nz7rdYc6eu8edgjsFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAACG55sKtdHDm3bXz5Dym71Z/LC8vJ8+61RJuZUCeNQrOY/3u+2y1Wsmzbm3Fpk2brPlSqZTLrORVaBSLRWvtRqOR29pOhUZfX/LXjyT/epufn0+enZ2dtdZ2uFUUS0tLybNuDUkK7hQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABCSy0fcnpLBwcHkWbejZmFhIXnW7R1x5t3eEadDyOkPkvxuHWc+z44np4dH8o7bOQcl/306Op1ObvN5nyuObreb23G4e+h0HzmdWpLXCeVcD5LUbret+fONOwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITkQiOnt0eSZmdnk2fdro+BgYHk2dHRUWttp/vI7UsZGhpKnnX32+35cdZ3P5/FxcXk2ZWVFWtttyfLkWc/kduT5axdrVattbds2ZI86+73zMxM8qzTkyT5e+h8TzjXpuR1do2MjFhrO8fiXj8puFMAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEJJrLtzHqZ0KCPdx976+5MNWT09Pbms7s+6xOI/on8u883nW6/Xc1nbrPAqF9J9j3FoE9zwsl8vJs+77dI7FfZ/OcW/YsMFa26nnqNVq1tpZllnzzrlSKpWstZ1ruVgsWms7NRduxUkK7hQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABCSy3tmZmashZ3uFrfTxO16cWzevDl5dm5uzlrbOW63s8ndk2azmcusJK2trVnzecmzV0nyOm2cviHJ67IaHBy01nY+H7cPytlzt6/L7RpzOrjc7yDn8xweHrbWdvqMRkdHrbVTcKcAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIhAIAICQ/N16pVKyF3ZoGx9LSUvKs+2i884i5+x5brVbyrHvcbs1Fp9NJnnXfZ6lUSp516wWcfXFqKCS/0sGpUXArN5zqCvfzqdVqybPOe5SkTZs2Jc+OjY1Za7v1KU41j/s+h4aGkmfdOo9Go5E8m0elDHcKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIyUUyk5OT1sL//Oc/k2edTiBJOnToUPKs0yMiSc1mM3nW6XmRvI6a5eVla+2FhQVr3uH2MDnzbieQ22fkcHtknP4o57ySvP4od0+ceec9SlK9Xk+eHR4ettZ2z0Pn3HL7o5w97O/vt9YeHx9PnnW/g1JwpwAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgeM+NG7rdbi6z7rxbXbC0tJQ8W61WrbXHxsaSZwcGBqy1syyz5p06AmdW8uoIyuWytba7L47V1VVrvlBI/5nKrXJxjsW9fpzz1q3ncOpZ3M/SrfNYWVlJnm2329baTv2He21u27YteXbDhg3W2im4UwAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQOjJ3GIOAMDrFncKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCA8H/F5HNqU85NDgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Number of images you want to display\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Extract the i-th image tensor\n",
        "    image_tensor = WL_tensor[i, :, :, 0]\n",
        "    # Use TensorFlow operations if needed (optional)\n",
        "    # Display the image\n",
        "    plt.imshow(image_tensor, cmap='gray')\n",
        "    plt.title(f\"Image {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f2885b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number_subimages_across = 32\n",
            "total number of images = 16384 512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([16384, 32, 32, 1])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"number_subimages_across =\", number_subimages_across )\n",
        "print(\"total number of images =\", number_subimages_across*number_subimages_across*number_fits_files *len(all_directories), number_subimages_total)\n",
        "np.shape(WL_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019e88b1",
      "metadata": {},
      "source": [
        "# Renormalize the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1363e4fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaNs: 0\n",
            "Number of Infinities: 0\n",
            "Min value: -1.548828125\n",
            "Max value: 9.53125\n",
            "Tensor shape: (17, 32, 32, 1)\n",
            "Mean value: -0.07708740234375\n",
            "Variance value: 0.6669921875\n",
            "Mean value: -0.07707119115409092\n",
            "Variance value: 0.6670937639813692\n",
            "Standard Deviation: 0.81689453125\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Implement Welford's algorithm for numerically stable mean and variance calculation\n",
        "def welford_algorithm(data):\n",
        "    n = 0\n",
        "    mean = 0.0\n",
        "    M2 = 0.0\n",
        "\n",
        "    for x in data:\n",
        "        n += 1\n",
        "        delta = x - mean\n",
        "        mean += delta / n\n",
        "        delta2 = x - mean\n",
        "        M2 += delta * delta2\n",
        "\n",
        "    variance = M2 / (n - 1) if n > 1 else float('nan')\n",
        "    return mean, variance\n",
        "\n",
        "calculate_statistics = True\n",
        "\n",
        "if calculate_statistics:\n",
        "    # Assuming WL_tensor is a TensorFlow tensor of floating-point numbers\n",
        "\n",
        "    # Convert the TensorFlow tensor to a NumPy array\n",
        "    WL_tensor_np = WL_tensor.numpy()\n",
        "    WL_tensor_np = WL_tensor_np[::1000, :, :, :]  # Downsample the tensor for faster computation\n",
        "\n",
        "    # Check for NaNs and Infinities\n",
        "    num_nans = np.isnan(WL_tensor_np).sum()\n",
        "    num_infs = np.isinf(WL_tensor_np).sum()\n",
        "    print(f\"Number of NaNs: {num_nans}\")\n",
        "    print(f\"Number of Infinities: {num_infs}\")\n",
        "\n",
        "\n",
        "    # Inspect the range of values\n",
        "    min_value = WL_tensor_np.min()\n",
        "    max_value = WL_tensor_np.max()\n",
        "    print(f\"Min value: {min_value}\")\n",
        "    print(f\"Max value: {max_value}\")\n",
        "\n",
        "    # Check the shape of the tensor\n",
        "    tensor_shape = WL_tensor_np.shape\n",
        "    print(f\"Tensor shape: {tensor_shape}\")\n",
        "\n",
        "    # Manually calculate the mean and variance\n",
        "    mean_value = np.mean(WL_tensor_np)\n",
        "    variance_value = np.var(WL_tensor_np)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Flatten the tensor to 1D for easier processing\n",
        "    WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate mean and variance using Welford's algorithm\n",
        "    mean_value, variance_value = welford_algorithm(WL_tensor_flat)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Calculate the standard deviation\n",
        "    std_dev = np.std(WL_tensor_np)\n",
        "    print(f\"Standard Deviation: {std_dev}\")\n",
        "\n",
        "    DO_KDE= True\n",
        "    if DO_KDE:\n",
        "        # Flatten the tensor to 1D for PDF calculation\n",
        "        WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
        "        kde = gaussian_kde(WL_tensor_flat)\n",
        "        x_values = np.linspace(WL_tensor_flat.min(), WL_tensor_flat.max(), 1000)\n",
        "        pdf_values = kde(x_values)\n",
        "\n",
        "        # Plot the PDF\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.semilogx(x_values, pdf_values, label='PDF')\n",
        "        plt.hist(WL_tensor_flat, bins=50, density=True, alpha=0.6, color='g', label='Histogram')\n",
        "        plt.title('Probability Density Function of WL_tensor')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a635a7",
      "metadata": {},
      "source": [
        "# Motivated binning method that partitions so that variance in each bin should be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2439072c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Variance: 0.0001\n",
            "Target Variance per Bin: 2e-06\n",
            "num bins= 123\n",
            "Target Variance: 0.0001\n",
            "Target Variance per Bin: 8.130081300813008e-07\n",
            "num bins= 168\n",
            "Target Variance: 0.0001\n",
            "Target Variance per Bin: 5.952380952380953e-07\n",
            "num bins= 189\n",
            "mean values =  [-1.528, -1.426, -1.366, -1.314, -1.2705, -1.232, -1.195, -1.16, -1.133, -1.103, -1.069, -1.039, -1.007, -0.973, -0.9463, -0.921, -0.896, -0.8745, -0.8516, -0.825, -0.8, -0.7764, -0.757, -0.741, -0.722, -0.7007, -0.6787, -0.6567, -0.6353, -0.6143, -0.593, -0.572, -0.5503, -0.5273, -0.503, -0.4797, -0.4578, -0.4355, -0.4133, -0.3906, -0.3691, -0.348, -0.3254, -0.303, -0.2805, -0.2576, -0.2343, -0.2112, -0.188, -0.1648, -0.1411, -0.11743, -0.0936, -0.06964, -0.0455, -0.0212, 0.0034, 0.02818, 0.05328, 0.07855, 0.1041, 0.1299, 0.1558, 0.1819, 0.2083, 0.2347, 0.262, 0.2896, 0.3171, 0.345, 0.3723, 0.4001, 0.4294, 0.459, 0.489, 0.5195, 0.5503, 0.5806, 0.6113, 0.642, 0.6733, 0.7056, 0.737, 0.767, 0.799, 0.8345, 0.87, 0.9043, 0.94, 0.978, 1.02, 1.06, 1.099, 1.137, 1.172, 1.212, 1.247, 1.284, 1.328, 1.368, 1.411, 1.456, 1.49, 1.513, 1.549, 1.598, 1.651, 1.704, 1.751, 1.795, 1.848, 1.902, 1.959, 2.025, 2.092, 2.158, 2.22, 2.273, 2.328, 2.389, 2.457, 2.504, 2.553, 2.623, 2.693, 2.762, 2.834, 2.912, 2.975, 3.008, 3.055, 3.135, 3.223, 3.322, 3.426, 3.502, 3.574, 3.674, 3.777, 3.879, 4.008, 4.1, 4.176, 4.3, 4.43, 4.543, 4.652, 4.773, 4.91, 5.027, 5.156, 5.305, 5.445, 5.6, 5.76, 5.91, 6.0, 6.082, 6.242, 6.445, 6.668, 6.883, 7.09, 7.31, 7.535, 7.758, 8.02, 8.31, 8.58, 8.88, 9.16, 9.47, 9.805, 10.164, 10.52, 10.89, 11.34, 11.805, 12.3, 12.836, 13.39, 13.93, 14.56, 15.35, 16.28, 17.19, 18.7, 22.83]\n",
            "bin_edges =  [-1.5576e+00 -1.4648e+00 -1.3965e+00 -1.3408e+00 -1.2920e+00 -1.2520e+00\n",
            " -1.2158e+00 -1.1768e+00 -1.1465e+00 -1.1201e+00 -1.0869e+00 -1.0537e+00\n",
            " -1.0264e+00 -9.8926e-01 -9.5850e-01 -9.3555e-01 -9.0723e-01 -8.8574e-01\n",
            " -8.6426e-01 -8.3984e-01 -8.1152e-01 -7.8809e-01 -7.6514e-01 -7.4951e-01\n",
            " -7.3340e-01 -7.1143e-01 -6.8994e-01 -6.6797e-01 -6.4600e-01 -6.2500e-01\n",
            " -6.0449e-01 -5.8203e-01 -5.6152e-01 -5.3906e-01 -5.1562e-01 -4.9097e-01\n",
            " -4.6899e-01 -4.4702e-01 -4.2456e-01 -4.0210e-01 -3.7964e-01 -3.5913e-01\n",
            " -3.3667e-01 -3.1421e-01 -2.9175e-01 -2.6929e-01 -2.4585e-01 -2.2290e-01\n",
            " -1.9946e-01 -1.7664e-01 -1.5308e-01 -1.2939e-01 -1.0559e-01 -8.1665e-02\n",
            " -5.7587e-02 -3.3386e-02 -8.9111e-03  1.5778e-02  4.0680e-02  6.5979e-02\n",
            "  9.1248e-02  1.1700e-01  1.4270e-01  1.6882e-01  1.9495e-01  2.2144e-01\n",
            "  2.4817e-01  2.7588e-01  3.0322e-01  3.3081e-01  3.5889e-01  3.8574e-01\n",
            "  4.1479e-01  4.4409e-01  4.7339e-01  5.0391e-01  5.3467e-01  5.6543e-01\n",
            "  5.9570e-01  6.2646e-01  6.5771e-01  6.8896e-01  7.2168e-01  7.5146e-01\n",
            "  7.8174e-01  8.1592e-01  8.5254e-01  8.8672e-01  9.2139e-01  9.5801e-01\n",
            "  9.9756e-01  1.0410e+00  1.0781e+00  1.1191e+00  1.1533e+00  1.1904e+00\n",
            "  1.2334e+00  1.2607e+00  1.3076e+00  1.3477e+00  1.3887e+00  1.4326e+00\n",
            "  1.4795e+00  1.5000e+00  1.5244e+00  1.5723e+00  1.6230e+00  1.6797e+00\n",
            "  1.7285e+00  1.7715e+00  1.8184e+00  1.8760e+00  1.9268e+00  1.9902e+00\n",
            "  2.0605e+00  2.1230e+00  2.1934e+00  2.2480e+00  2.2988e+00  2.3535e+00\n",
            "  2.4219e+00  2.4902e+00  2.5176e+00  2.5859e+00  2.6582e+00  2.7285e+00\n",
            "  2.7949e+00  2.8730e+00  2.9512e+00  2.9941e+00  3.0176e+00  3.0918e+00\n",
            "  3.1777e+00  3.2676e+00  3.3750e+00  3.4727e+00  3.5273e+00  3.6191e+00\n",
            "  3.7246e+00  3.8281e+00  3.9277e+00  4.0898e+00  4.1133e+00  4.2344e+00\n",
            "  4.3672e+00  4.4922e+00  4.5938e+00  4.7070e+00  4.8359e+00  4.9805e+00\n",
            "  5.0742e+00  5.2383e+00  5.3672e+00  5.5195e+00  5.6836e+00  5.8359e+00\n",
            "  5.9805e+00  6.0195e+00  6.1406e+00  6.3438e+00  6.5508e+00  6.7812e+00\n",
            "  6.9805e+00  7.1914e+00  7.4258e+00  7.6484e+00  7.8594e+00  8.1953e+00\n",
            "  8.4141e+00  8.7422e+00  9.0156e+00  9.2969e+00  9.6328e+00  9.9766e+00\n",
            "  1.0336e+01  1.0703e+01  1.1102e+01  1.1602e+01  1.2047e+01  1.2539e+01\n",
            "  1.3180e+01  1.3641e+01  1.4188e+01  1.4953e+01  1.5758e+01  1.6828e+01\n",
            "  1.7500e+01  2.0484e+01  2.8609e+01]\n",
            "num_points =  [240, 757, 1415, 2087, 2901, 3271, 3802, 5080, 5038, 5181, 7097, 8092, 7858, 11425, 10502, 8887, 11292, 9302, 9605, 11436, 13695, 12312, 12242, 8431, 9047, 12365, 12462, 12541, 13330, 12587, 12742, 13986, 12899, 14540, 14653, 15445, 13712, 14067, 14076, 14361, 13973, 12915, 13654, 13711, 13427, 13414, 13661, 13343, 13241, 12918, 12836, 12735, 12573, 12338, 12237, 12101, 11917, 11622, 11532, 11270, 11139, 10778, 10761, 10539, 10130, 10080, 9921, 9647, 9326, 9098, 8789, 8436, 8449, 8473, 8123, 8044, 7826, 7535, 7003, 6941, 6912, 6434, 6569, 5521, 5610, 5921, 6044, 5308, 5327, 5296, 5471, 5549, 4591, 4889, 3901, 3948, 4363, 2661, 4270, 3444, 3336, 3329, 3268, 1522, 1730, 3136, 3173, 3330, 2658, 2182, 2222, 2612, 2141, 2544, 2501, 2227, 2252, 1606, 1396, 1461, 1657, 1538, 599, 1460, 1418, 1276, 1124, 1243, 1157, 548, 327, 908, 958, 907, 1041, 815, 481, 699, 743, 651, 581, 802, 112, 608, 520, 485, 324, 376, 388, 354, 211, 369, 270, 246, 246, 211, 204, 56, 164, 205, 177, 180, 153, 138, 116, 106, 93, 139, 92, 83, 56, 61, 59, 58, 44, 43, 39, 35, 30, 25, 21, 21, 11, 15, 11, 11, 12, 11]\n",
            "total_variance_bin = [5.965100531796996e-07, 5.959258193059199e-07, 5.959614237089019e-07, 5.955965068628447e-07, 5.952478366069712e-07, 5.954697681863299e-07, 5.956272274788795e-07, 5.952752059456371e-07, 5.954579966198053e-07, 5.955307947616976e-07, 5.956844459670202e-07, 5.954227181142631e-07, 5.954906147533054e-07, 5.953051248967865e-07, 5.952432423744476e-07, 5.954350439677263e-07, 5.955293313478647e-07, 5.953713246746931e-07, 5.953339028734117e-07, 5.953223097735919e-07, 5.955100975116561e-07, 5.952915162681814e-07, 5.953295991046317e-07, 5.953665071245238e-07, 5.954957463801789e-07, 5.954939983344883e-07, 5.95342359518737e-07, 5.95272424917819e-07, 5.952772491368698e-07, 5.952506020469258e-07, 5.952748644538978e-07, 5.953428658491741e-07, 5.954228532201027e-07, 5.952973223771922e-07, 5.953394330951877e-07, 5.953946430979632e-07, 5.953666464950726e-07, 5.95315912870363e-07, 5.9524564575376e-07, 5.952633593247165e-07, 5.953088505470355e-07, 5.952794731672753e-07, 5.952485315065573e-07, 5.952773566492805e-07, 5.953220343306981e-07, 5.952841285839389e-07, 5.953268071643016e-07, 5.953006802092563e-07, 5.953573226208111e-07, 5.952633830926344e-07, 5.953327948683203e-07, 5.952817964470445e-07, 5.953260887541096e-07, 5.953147909835318e-07, 5.95304355119596e-07, 5.953642177926591e-07, 5.952544597518876e-07, 5.953438790866707e-07, 5.952754465024918e-07, 5.953400112160949e-07, 5.952853820729641e-07, 5.95371008725435e-07, 5.953957443883746e-07, 5.954047460261121e-07, 5.952642905472733e-07, 5.952944014665345e-07, 5.953746728118633e-07, 5.953143519866881e-07, 5.953355832585793e-07, 5.95265941382804e-07, 5.954248687056457e-07, 5.952439695898999e-07, 5.953878581904467e-07, 5.953197415928077e-07, 5.953997497679263e-07, 5.954250432016516e-07, 5.95421332581761e-07, 5.952906474404552e-07, 5.953620072942858e-07, 5.955116274378393e-07, 5.95476150788767e-07, 5.954110812162164e-07, 5.955603874848103e-07, 5.956608259162646e-07, 5.953049927093652e-07, 5.953202428045529e-07, 5.956452497677963e-07, 5.953790734035616e-07, 5.956087874773021e-07, 5.955457703140058e-07, 5.955907280745742e-07, 5.952992716020319e-07, 5.953440871882157e-07, 5.960102042268416e-07, 5.953579438044448e-07, 5.953255402810918e-07, 5.959149725342782e-07, 5.952662188297029e-07, 5.958779699814641e-07, 5.954298395565825e-07, 5.954318068015071e-07, 5.958195787217376e-07, 5.957651024461147e-07, 5.952610207732816e-07, 5.954020441165488e-07, 5.958067919839132e-07, 5.953354730293379e-07, 5.958642429611765e-07, 5.955754665225091e-07, 5.954640142313729e-07, 5.958331647829232e-07, 5.956570608134183e-07, 5.953790611197754e-07, 5.956800684311881e-07, 5.956480281816839e-07, 5.958340517168862e-07, 5.961530101623732e-07, 5.962559756462906e-07, 5.953578032905598e-07, 5.953927236057216e-07, 5.971265322913471e-07, 5.954112894894888e-07, 5.961324570567675e-07, 5.960777863138809e-07, 5.954974678611427e-07, 5.96045675838946e-07, 5.971283713098418e-07, 5.95848378352665e-07, 5.971277230337223e-07, 5.966572526163603e-07, 5.971368426922846e-07, 5.984730200065638e-07, 5.975186139697114e-07, 5.979326597639245e-07, 5.975122855264648e-07, 5.989812855655447e-07, 5.963944673539667e-07, 5.994680672469581e-07, 5.963939605055761e-07, 5.958640641423532e-07, 5.969300302964696e-07, 6.044533071154035e-07, 5.96077996331325e-07, 5.962239851458021e-07, 5.987675543672347e-07, 6.021838227584416e-07, 5.99810243022514e-07, 5.988587239584692e-07, 6.031386501084012e-07, 6.021538321086905e-07, 5.99055916923269e-07, 5.964658985983632e-07, 6.032540133450561e-07, 6.027333162282178e-07, 6.081208521007766e-07, 6.017385573625861e-07, 6.105364719644479e-07, 6.044111772013738e-07, 6.054626652064139e-07, 6.09559003044537e-07, 5.983261628597968e-07, 6.240008476711741e-07, 5.970764160163131e-07, 5.972997289505245e-07, 6.098823879087159e-07, 6.026586623413701e-07, 6.110015537432268e-07, 6.283684329701841e-07, 6.140726948816426e-07, 6.350447491896685e-07, 5.954623135645472e-07, 6.015614827475929e-07, 6.268089557509892e-07, 6.248790004797149e-07, 6.103047215656261e-07, 6.026785714284199e-07, 6.274092824839356e-07, 6.178032370178912e-07, 6.18189318426536e-07, 6.777928670241333e-07, 7.731323242190002e-07, 6.443817138674376e-07, 7.414489746094659e-07, 6.039167131699026e-07, 7.948974609372726e-07, 1.0621093750002727e-06, 7.004394531243386e-07, 9.261889648436772e-06]\n",
            "##########################################\n",
            "Standard Deviation of Quantization Error: 0.01182 0.01196 0.011060541614720174\n",
            "##########################################\n",
            "tf.Tensor(\n",
            "[-0.7007 -0.722  -0.8    -0.8745 -0.7764 -0.6787 -0.503  -0.4578 -0.6143\n",
            " -0.7764], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[-0.697  -0.717  -0.811  -0.8657 -0.7764 -0.684  -0.4966 -0.4624 -0.618\n",
            " -0.7754], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[-0.003906 -0.00537   0.01123  -0.00879   0.        0.00537  -0.006348\n",
            "  0.00464   0.003906 -0.000977], shape=(10,), dtype=float16)\n",
            "[240, 757, 1415, 2087, 2901, 3271, 3802, 5080, 5038, 5181, 7097, 8092, 7858, 11425, 10502, 8887, 11292, 9302, 9605, 11436, 13695, 12312, 12242, 8431, 9047, 12365, 12462, 12541, 13330, 12587, 12742, 13986, 12899, 14540, 14653, 15445, 13712, 14067, 14076, 14361, 13973, 12915, 13654, 13711, 13427, 13414, 13661, 13343, 13241, 12918, 12836, 12735, 12573, 12338, 12237, 12101, 11917, 11622, 11532, 11270, 11139, 10778, 10761, 10539, 10130, 10080, 9921, 9647, 9326, 9098, 8789, 8436, 8449, 8473, 8123, 8044, 7826, 7535, 7003, 6941, 6912, 6434, 6569, 5521, 5610, 5921, 6044, 5308, 5327, 5296, 5471, 5549, 4591, 4889, 3901, 3948, 4363, 2661, 4270, 3444, 3336, 3329, 3268, 1522, 1730, 3136, 3173, 3330, 2658, 2182, 2222, 2612, 2141, 2544, 2501, 2227, 2252, 1606, 1396, 1461, 1657, 1538, 599, 1460, 1418, 1276, 1124, 1243, 1157, 548, 327, 908, 958, 907, 1041, 815, 481, 699, 743, 651, 581, 802, 112, 608, 520, 485, 324, 376, 388, 354, 211, 369, 270, 246, 246, 211, 204, 56, 164, 205, 177, 180, 153, 138, 116, 106, 93, 139, 92, 83, 56, 61, 59, 58, 44, 43, 39, 35, 30, 25, 21, 21, 11, 15, 11, 11, 12, 11] 999994\n",
            "[5.965100531796996e-07, 5.959258193059199e-07, 5.959614237089019e-07, 5.955965068628447e-07, 5.952478366069712e-07, 5.954697681863299e-07, 5.956272274788795e-07, 5.952752059456371e-07, 5.954579966198053e-07, 5.955307947616976e-07, 5.956844459670202e-07, 5.954227181142631e-07, 5.954906147533054e-07, 5.953051248967865e-07, 5.952432423744476e-07, 5.954350439677263e-07, 5.955293313478647e-07, 5.953713246746931e-07, 5.953339028734117e-07, 5.953223097735919e-07, 5.955100975116561e-07, 5.952915162681814e-07, 5.953295991046317e-07, 5.953665071245238e-07, 5.954957463801789e-07, 5.954939983344883e-07, 5.95342359518737e-07, 5.95272424917819e-07, 5.952772491368698e-07, 5.952506020469258e-07, 5.952748644538978e-07, 5.953428658491741e-07, 5.954228532201027e-07, 5.952973223771922e-07, 5.953394330951877e-07, 5.953946430979632e-07, 5.953666464950726e-07, 5.95315912870363e-07, 5.9524564575376e-07, 5.952633593247165e-07, 5.953088505470355e-07, 5.952794731672753e-07, 5.952485315065573e-07, 5.952773566492805e-07, 5.953220343306981e-07, 5.952841285839389e-07, 5.953268071643016e-07, 5.953006802092563e-07, 5.953573226208111e-07, 5.952633830926344e-07, 5.953327948683203e-07, 5.952817964470445e-07, 5.953260887541096e-07, 5.953147909835318e-07, 5.95304355119596e-07, 5.953642177926591e-07, 5.952544597518876e-07, 5.953438790866707e-07, 5.952754465024918e-07, 5.953400112160949e-07, 5.952853820729641e-07, 5.95371008725435e-07, 5.953957443883746e-07, 5.954047460261121e-07, 5.952642905472733e-07, 5.952944014665345e-07, 5.953746728118633e-07, 5.953143519866881e-07, 5.953355832585793e-07, 5.95265941382804e-07, 5.954248687056457e-07, 5.952439695898999e-07, 5.953878581904467e-07, 5.953197415928077e-07, 5.953997497679263e-07, 5.954250432016516e-07, 5.95421332581761e-07, 5.952906474404552e-07, 5.953620072942858e-07, 5.955116274378393e-07, 5.95476150788767e-07, 5.954110812162164e-07, 5.955603874848103e-07, 5.956608259162646e-07, 5.953049927093652e-07, 5.953202428045529e-07, 5.956452497677963e-07, 5.953790734035616e-07, 5.956087874773021e-07, 5.955457703140058e-07, 5.955907280745742e-07, 5.952992716020319e-07, 5.953440871882157e-07, 5.960102042268416e-07, 5.953579438044448e-07, 5.953255402810918e-07, 5.959149725342782e-07, 5.952662188297029e-07, 5.958779699814641e-07, 5.954298395565825e-07, 5.954318068015071e-07, 5.958195787217376e-07, 5.957651024461147e-07, 5.952610207732816e-07, 5.954020441165488e-07, 5.958067919839132e-07, 5.953354730293379e-07, 5.958642429611765e-07, 5.955754665225091e-07, 5.954640142313729e-07, 5.958331647829232e-07, 5.956570608134183e-07, 5.953790611197754e-07, 5.956800684311881e-07, 5.956480281816839e-07, 5.958340517168862e-07, 5.961530101623732e-07, 5.962559756462906e-07, 5.953578032905598e-07, 5.953927236057216e-07, 5.971265322913471e-07, 5.954112894894888e-07, 5.961324570567675e-07, 5.960777863138809e-07, 5.954974678611427e-07, 5.96045675838946e-07, 5.971283713098418e-07, 5.95848378352665e-07, 5.971277230337223e-07, 5.966572526163603e-07, 5.971368426922846e-07, 5.984730200065638e-07, 5.975186139697114e-07, 5.979326597639245e-07, 5.975122855264648e-07, 5.989812855655447e-07, 5.963944673539667e-07, 5.994680672469581e-07, 5.963939605055761e-07, 5.958640641423532e-07, 5.969300302964696e-07, 6.044533071154035e-07, 5.96077996331325e-07, 5.962239851458021e-07, 5.987675543672347e-07, 6.021838227584416e-07, 5.99810243022514e-07, 5.988587239584692e-07, 6.031386501084012e-07, 6.021538321086905e-07, 5.99055916923269e-07, 5.964658985983632e-07, 6.032540133450561e-07, 6.027333162282178e-07, 6.081208521007766e-07, 6.017385573625861e-07, 6.105364719644479e-07, 6.044111772013738e-07, 6.054626652064139e-07, 6.09559003044537e-07, 5.983261628597968e-07, 6.240008476711741e-07, 5.970764160163131e-07, 5.972997289505245e-07, 6.098823879087159e-07, 6.026586623413701e-07, 6.110015537432268e-07, 6.283684329701841e-07, 6.140726948816426e-07, 6.350447491896685e-07, 5.954623135645472e-07, 6.015614827475929e-07, 6.268089557509892e-07, 6.248790004797149e-07, 6.103047215656261e-07, 6.026785714284199e-07, 6.274092824839356e-07, 6.178032370178912e-07, 6.18189318426536e-07, 6.777928670241333e-07, 7.731323242190002e-07, 6.443817138674376e-07, 7.414489746094659e-07, 6.039167131699026e-07, 7.948974609372726e-07, 1.0621093750002727e-06, 7.004394531243386e-07, 9.261889648436772e-06] 0.00012233558081095676\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Flatten the tensor to 1D for easier processing\n",
        "WL_tensor_flat =  tf.reshape(WL_tensor, [-1]) # WL_tensor_np.flatten()\n",
        "\n",
        "# Ensure WL_tensor_flat is a NumPy array\n",
        "if isinstance(WL_tensor_flat, tf.Tensor):\n",
        "    WL_tensor_flat_np = WL_tensor_flat.numpy()\n",
        "else:\n",
        "    WL_tensor_flat_np = WL_tensor_flat\n",
        "\n",
        "# Step 1: Sample a Subset of the Data\n",
        "sample_size = 1000000  # Adjust based on available memory and desired accuracy\n",
        "indices = np.random.choice(len(WL_tensor_flat_np), size=sample_size, replace=False)\n",
        "sampled_data = WL_tensor_flat_np[indices]\n",
        "\n",
        "\n",
        "# Step 2: Sort the Sampled Data\n",
        "sorted_data = np.sort(sampled_data)\n",
        "n = len(sorted_data)\n",
        "\n",
        "\n",
        "#print(\"num zeros = \", np.sum(sampled_data == 0), np.sum(WL_tensor_flat_np == 0), np.sum(sorted_data ==0))\n",
        "\n",
        "#print(\"sorted_data =\", sorted_data)\n",
        "\n",
        "# Step 3: Compute Cumulative Sums and Cumulative Sum of Squares\n",
        "cum_sum = np.cumsum(sorted_data, dtype=float)\n",
        "cum_sum_sq = np.cumsum(sorted_data**2, dtype=float)\n",
        "\n",
        "# Step 4: Compute Total Variance and Target Variance per Bin\n",
        "target_std = .01\n",
        "target_variance = target_std**2\n",
        "num_bins= 50 # just a guess to start\n",
        "\n",
        "for iiter in range(3): # iterate five times to reach target std\n",
        "\n",
        "    target_variance_per_bin = target_variance/ num_bins\n",
        "\n",
        "    print(f\"Target Variance: {target_variance}\")\n",
        "    print(f\"Target Variance per Bin: {target_variance_per_bin}\")\n",
        "\n",
        "\n",
        "    # Step 5: Find Bin Edges\n",
        "    bin_edges = []\n",
        "    num_points_arr = []\n",
        "    total_variance_bin_arr = []\n",
        "    mean_bin_arr = []\n",
        "\n",
        "    start_index = 0\n",
        "    while start_index < n:\n",
        "        found = False\n",
        "        for end_index in range(start_index + 10, n):\n",
        "            num_points = end_index - start_index + 1\n",
        "            #if num_points <= 3:\n",
        "            #    continue\n",
        "            # Sum and sum of squares in the bin\n",
        "            sum_bin = cum_sum[end_index] - (cum_sum[start_index - 1] if start_index > 0 else 0)\n",
        "            sum_sq_bin = cum_sum_sq[end_index] - (cum_sum_sq[start_index - 1] if start_index > 0 else 0)\n",
        "            # Mean and variance in the bin\n",
        "            mean_bin = sum_bin / num_points\n",
        "            total_variance_bin = (sum_sq_bin - num_points*mean_bin**2)/(num_points-1)*num_points/n  #total contribute to variance\n",
        "            if total_variance_bin >= target_variance_per_bin:\n",
        "                bin_edges.append(sorted_data[end_index])\n",
        "                num_points_arr.append(num_points)\n",
        "                total_variance_bin_arr.append(total_variance_bin)\n",
        "                mean_bin_arr.append(mean_bin)\n",
        "                #print(\"sums = \",  sum_bin , sum_sq_bin, mean_bin, sum_sq_bin / num_points, num_points, \"total_variance_bin = \", total_variance_bin, target_variance_per_bin, num_points/n, ((sum_sq_bin / num_points) - mean_bin**2) )\n",
        "                #print(\"indexes =\", start_index, end_index, cum_sum[end_index], cum_sum_sq[start_index - 1]) \n",
        "                start_index = end_index + 1\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # Include all remaining data in the last bin\n",
        "            bin_edges.append(tf.reduce_max(WL_tensor)) #       sorted_data[-1])\n",
        "            break\n",
        "\n",
        "    num_bins = len(bin_edges)\n",
        "    print(\"num bins=\", num_bins)\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Digitize the Original Data Using Bin Edges\n",
        "# Convert bin_edges to numpy array for consistency\n",
        "bin_edges = np.array(bin_edges)\n",
        "\n",
        "# Use np.digitize to find bin indices for the entire dataset\n",
        "bin_indices = np.digitize(WL_tensor_flat_np, bin_edges, right=True)-1\n",
        "\n",
        "# Adjust bin_indices to ensure they are within the valid range\n",
        "# Since mean_values has length len(bin_edges) + 1, bin_indices should be in [0, len(bin_edges)]\n",
        "#may be necessary since our bins are not based on everything\n",
        "bin_indices = np.clip(bin_indices, 0, len(bin_edges))\n",
        "\n",
        "\n",
        "# Digitize the tensor values according to the binning scheme\n",
        "#bin_indices = tf.searchsorted(bin_edges, WL_tensor_flat, side='right') - 1\n",
        "\n",
        "\n",
        "#let's calculate the mean we expect in each bin\n",
        "#MIGHT NEED TO GAURD AGAINST ZEROS\n",
        "mean_values = [np.mean(WL_tensor_flat[bin_indices == index]) for index in range(len(bin_edges)-1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"mean values = \", mean_values)\n",
        "print(\"bin_edges = \", bin_edges)\n",
        "print(\"num_points = \", num_points_arr)\n",
        "print(\"total_variance_bin =\", total_variance_bin_arr)\n",
        "\n",
        "encoded_tensor = tf.gather(mean_values, bin_indices) #realization of quantized tensor\n",
        "\n",
        "\n",
        "#reshape\n",
        "bin_indices = tf.reshape(bin_indices, [number_images,  sub_image_size*sub_image_size])\n",
        "\n",
        "\n",
        "# Compute mean values\n",
        "# mean_values = np.zeros(len(bin_edges) + 1)\n",
        "# for i in range(len(mean_values)):\n",
        "#     indices_in_bin = bin_indices == i\n",
        "#     if np.any(indices_in_bin):\n",
        "#         mean_values[i] = np.mean(WL_tensor_flat_np[indices_in_bin])\n",
        "#     else:\n",
        "#         # Handle empty bins if necessary\n",
        "#         mean_values[i] = 0\n",
        "\n",
        "# # Map bin indices to mean values\n",
        "# encoded_tensor_np = mean_values[bin_indices]\n",
        "\n",
        "# # If you need to convert back to a TensorFlow tensor\n",
        "# encoded_tensor = tf.convert_to_tensor(encoded_tensor_np, dtype=WL_tensor_flat.dtype)\n",
        "\n",
        "# # Compute Quantization Error\n",
        "# \n",
        "# \n",
        "diff_tensor = encoded_tensor - WL_tensor_flat\n",
        "\n",
        "# Compute the standard deviation of the quantization error\n",
        "std_quantized = tf.math.reduce_std(diff_tensor).numpy()\n",
        "\n",
        "# Convert indices to a TensorFlow tensor\n",
        "indices_tf = tf.constant(indices, dtype=tf.int32)\n",
        "# Use tf.gather to index diff_tensor\n",
        "std_quantized_sampled = tf.math.reduce_std(tf.gather(diff_tensor, indices_tf)).numpy()\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Standard Deviation of Quantization Error:\", std_quantized, std_quantized_sampled, np.sum(total_variance_bin_arr)**.5)\n",
        "print(\"##########################################\")\n",
        "# If needed, reshape encoded_tensor back to the original tensor shape\n",
        "# encoded_tensor = tf.reshape(encoded_tensor, WL_tensor.shape)\n",
        "\n",
        "#just to check quantization works\n",
        "print(encoded_tensor[-10:])\n",
        "print(WL_tensor_flat[-10:])\n",
        "\n",
        "\n",
        "print(diff_tensor[-10:])\n",
        "\n",
        "\n",
        "print(num_points_arr, np.sum(num_points_arr))\n",
        "print(total_variance_bin_arr, np.sum(total_variance_bin_arr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e5f23c29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "np.shape(WL_tensor), np.shape(bin_indices) =  (16384, 32, 32, 1) (16384, 1024) <dtype: 'int64'>\n"
          ]
        }
      ],
      "source": [
        "print(\"np.shape(WL_tensor), np.shape(bin_indices) = \", np.shape(WL_tensor), np.shape(bin_indices), bin_indices.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684c8c58",
      "metadata": {},
      "source": [
        "# Autoregressive image transformer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab491d",
      "metadata": {},
      "source": [
        "## Simple autoencoder with sines and cosines sampling to Nyquist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "98c92482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encoding  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalization  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              "\n",
              " add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
              "                                                     dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              "\n",
              " add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,285</span>  add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m12,096\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encoding  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalization  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda (\u001b[38;5;33mLambda\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m,             \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              "\n",
              " add (\u001b[38;5;33mAdd\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_1 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
              "                                                     dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              "\n",
              " add_2 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_5 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_3 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m)      \u001b[38;5;34m12,285\u001b[0m  add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,349</span> (485.74 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m124,349\u001b[0m (485.74 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,349</span> (485.74 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m124,349\u001b[0m (485.74 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              "\n",
              " add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_10           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_11           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_13           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_14           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_16           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_17           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">189</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,285</span>  add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m12,096\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_1 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m,             \u001b[38;5;34m0\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_7 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              "\n",
              " add_4 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_8 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_5 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_10           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_6 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_7 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_11           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_7 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_13           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_8 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_9 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_10 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_14           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_9 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_16           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_10 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_12 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_17           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_11 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_13 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m189\u001b[0m)      \u001b[38;5;34m12,285\u001b[0m  add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224,317</span> (876.24 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m224,317\u001b[0m (876.24 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">224,317</span> (876.24 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m224,317\u001b[0m (876.24 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  #don't think need L1 and dropout  (I've tested and it performs best with this equal to zero)\n",
        "num_classes = num_bins  # Set num_classes to num_bins: this is the number of values that can be populated\n",
        "d_model = number_channels*2  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.d_model = d_model\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "'''\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    #inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)  # Accept variable-length sequences\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(x)  # Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Create the causal mask once as a constant tensor\n",
        "    #causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    #causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    def create_causal_mask(x):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        return causal_mask\n",
        "\n",
        "    causal_mask = layers.Lambda(create_causal_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "'''\n",
        "\n",
        "# Function to create the combined attention mask\n",
        "def create_attention_mask(inputs):\n",
        "    # Create padding mask (1 for valid tokens, 0 for padding tokens)\n",
        "    padding_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
        "    padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]  # Shape: (batch_size, 1, 1, seq_length)\n",
        "\n",
        "    # Create causal mask (lower triangular matrix)\n",
        "    seq_length = tf.shape(inputs)[1]\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # Shape: (1, 1, seq_length, seq_length)\n",
        "\n",
        "    # Combine masks: only attend to previous tokens and non-padding tokens\n",
        "    attention_mask = tf.cast(causal_mask, tf.float32) * padding_mask\n",
        "\n",
        "    return attention_mask  # Shape: (batch_size, 1, seq_length, seq_length)\n",
        "\n",
        "# Function to create the Transformer model with manual attention mask\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Remove mask_zero to prevent automatic masking\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(inputs)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Manually create the attention mask\n",
        "    attention_mask = layers.Lambda(create_attention_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with manual attention mask\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=attention_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_length = sub_image_size * sub_image_size\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_deep = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers*2,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "\n",
        "autoregressive_transformer.summary()\n",
        "\n",
        "autoregressive_transformer_deep.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6de246",
      "metadata": {},
      "source": [
        "## Compile models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2cb4fc3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_deep.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1a6b2",
      "metadata": {},
      "source": [
        "### Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "650102df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-30 08:26:45.318896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m 14/922\u001b[0m \u001b[37m\u001b[0m \u001b[1m7:38\u001b[0m 504ms/step - accuracy: 0.0085 - loss: 5.4509"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b344ac9",
      "metadata": {},
      "source": [
        "### Let's fit the deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "49bec05f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1124s\u001b[0m 1s/step - accuracy: 0.0447 - loss: 3.9281 - val_accuracy: 0.0796 - val_loss: 3.1744\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 1s/step - accuracy: 0.0798 - loss: 3.1678 - val_accuracy: 0.0901 - val_loss: 3.0447\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1108s\u001b[0m 1s/step - accuracy: 0.0882 - loss: 3.0651 - val_accuracy: 0.0948 - val_loss: 2.9914\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1475s\u001b[0m 2s/step - accuracy: 0.0926 - loss: 3.0179 - val_accuracy: 0.0973 - val_loss: 2.9681\n",
            "Epoch 5/5\n",
            "\u001b[1m 87/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21:23\u001b[0m 2s/step - accuracy: 0.0942 - loss: 2.9992"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoregressive_transformer_deep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust epochs as needed\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m autoregressive_transformer\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_deep.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a3d9481",
      "metadata": {},
      "source": [
        "# Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba17c87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Much slower code that I don't use anymore\n",
        "def autoregressive_predict(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model, one step at a time,\n",
        "    using the original sequence up to the current position as context.\n",
        "    \"\"\"\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for i in range(1, sequence_length+1):\n",
        "        # Prepare the context (all previous pixels)\n",
        "        context = original_sequence[:i]\n",
        "\n",
        "        # No padding required; model expects input of length seq_length\n",
        "        # For positions where context is shorter, we need to pad or adjust the input\n",
        "        # Since we're avoiding padding, we'll adjust the input sequence accordingly\n",
        "\n",
        "        # Create a context of length 'i'\n",
        "        model_input = np.array(context, dtype=np.int32)[np.newaxis, :]\n",
        "\n",
        "        # Since the model expects input of shape (batch_size, seq_length),\n",
        "        # we need to handle the varying lengths\n",
        "        # One way is to slice the model to accept variable input lengths\n",
        "\n",
        "        # Predict\n",
        "        next_pixel_probs = model.predict(model_input, verbose=0)\n",
        "        # Get the prediction for the current position\n",
        "        next_pixel = np.argmax(next_pixel_probs[0, -1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Step {i}: Context length: {len(context)}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "#should be much faster....need to check\n",
        "def autoregressive_predict_batched(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model by batching inputs,\n",
        "    which significantly speeds up the prediction process.\n",
        "    \"\"\"\n",
        "    # Prepare all contexts at once\n",
        "    contexts = [original_sequence[:i] for i in range(1, sequence_length + 1)]\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = sequence_length\n",
        "    padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        contexts, maxlen=max_len, padding='post', value=0\n",
        "    )\n",
        "\n",
        "    # Convert to array\n",
        "    model_inputs = np.array(padded_contexts, dtype=np.int32)\n",
        "\n",
        "    # Predict all at once\n",
        "    next_pixel_probs = model.predict(model_inputs, verbose=0)\n",
        "\n",
        "    # Extract the predictions\n",
        "    predicted_sequence = []\n",
        "    for i, context in enumerate(contexts):\n",
        "        # Get the prediction for the current position\n",
        "        seq_length = len(context)\n",
        "        next_pixel = np.argmax(next_pixel_probs[i, seq_length - 1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1}: Context length: {seq_length}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "def process_random_images(X_val, y_val, model, sub_image_size, num_samples=5):\n",
        "    \"\"\"\n",
        "    Process and visualize random images from the validation set.\n",
        "    \"\"\"\n",
        "    total_samples = len(X_val)\n",
        "    random_indices = np.random.choice(total_samples, num_samples, replace=False)\n",
        "    \n",
        "    \n",
        "    for i, idx in enumerate(random_indices):\n",
        "        val_sample = X_val[idx]\n",
        "        original_sequence = y_val[idx]\n",
        "        \n",
        "        # Determine the sequence length\n",
        "        sequence_length = len(original_sequence)\n",
        "        \n",
        "        # Predict using the original sequence as context\n",
        "        predicted_sequence = autoregressive_predict_batched(\n",
        "            model,\n",
        "            original_sequence,\n",
        "            sequence_length,\n",
        "        )\n",
        "        \n",
        "        print(\"val_sample and original_sequence shapes\", np.shape(val_sample), np.shape(original_sequence), np.shape(predicted_sequence))\n",
        "    \n",
        "        visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, i+1)\n",
        "        \n",
        "        mse = np.mean((original_sequence - predicted_sequence)**2)\n",
        "        mae = np.mean(np.abs(original_sequence - predicted_sequence))\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.4f}\")\n",
        "        print(f\"  Original sequence shape: {original_sequence.shape}\")\n",
        "        print(f\"  Predicted sequence shape: {predicted_sequence.shape}\")\n",
        "        print(f\" fraction of pixles correct\", np.sum(original_sequence - predicted_sequence == 0)/len(original_sequence))\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# The visualize_autoregressive_prediction function remains the same\n",
        "\n",
        "def visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, predicted, and difference images with equal sizes.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "    fig.suptitle(f'Sample {index}', fontsize=16)\n",
        "\n",
        "    # Prepare images\n",
        "    original_2d = sequence_to_image(original_sequence, sub_image_size, original=True)\n",
        "    predicted_2d = sequence_to_image(predicted_sequence, sub_image_size, original=False)\n",
        "    diff = original_2d - predicted_2d\n",
        "\n",
        "    # Set up common parameters for imshow\n",
        "    imshow_args = {'interpolation': 'nearest', 'aspect': 'equal'}\n",
        "\n",
        "    # Visualize original sequence\n",
        "    im1 = axes[0].imshow(original_2d, cmap='gray', **imshow_args)\n",
        "    axes[0].set_title('Original Image')\n",
        "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize predicted sequence\n",
        "    im2 = axes[1].imshow(predicted_2d, cmap='gray', **imshow_args)\n",
        "    axes[1].set_title('Predicted Image (Autoregressive)')\n",
        "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize difference\n",
        "    im3 = axes[2].imshow(diff, cmap='bwr', **imshow_args)\n",
        "    axes[2].set_title('Difference (Original - Predicted)')\n",
        "    plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sequence_to_image(sequence, sub_image_size, original=True):\n",
        "    \"\"\"\n",
        "    Convert a 1D sequence of length 1023 to a 2D image of size sub_image_size x sub_image_size,\n",
        "    by prepending the first_token to the sequence.\n",
        "    \"\"\"\n",
        "    # Concatenate the first token to the sequence\n",
        "    if original:\n",
        "        full_sequence = np.concatenate((sequence, [0]))\n",
        "    else:\n",
        "        full_sequence = np.concatenate(([0], sequence))\n",
        "    # Reshape to image\n",
        "    image = full_sequence.reshape(sub_image_size, sub_image_size)\n",
        "    return image\n",
        "\n",
        "# Run the generalized code\n",
        "num_samples_to_visualize = 3  # Change this to the number of random samples you want to visualize\n",
        "process_random_images(X_val, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc300667",
      "metadata": {},
      "source": [
        "### Plot probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d23252",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot predictions vs actual values\n",
        "def plot_predictions(model, sequences, targets, num_samples=10):\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(sequences[:num_samples])\n",
        "    predicted_pixels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Plot the actual vs predicted pixel values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(sequences[i].reshape(-1, 1), cmap='gray', aspect='auto')\n",
        "        plt.title(f\"Sequence {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(num_classes), predictions[i])\n",
        "        plt.axvline(x=targets[i], color='r', linestyle='--')\n",
        "        plt.axvline(x=predicted_pixels[i], color='g', linestyle='--')\n",
        "        plt.title(f\"True: {targets[i]}, Pred: {predicted_pixels[i]}\")\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions for some sequences\n",
        "plot_predictions(autoregressive_transformer, train_sequences, train_targets, num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489870fe",
      "metadata": {},
      "source": [
        "## This has a free positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6cb767fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_5        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_5          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">5,952</span>  input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">32,736</span>  embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  positional_embed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_31           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_embed \n",
              "                                                     dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_32           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_34           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_35           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,138</span>  add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_5        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_5          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m5,952\u001b[0m  input_layer_5[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       \u001b[38;5;34m32,736\u001b[0m  embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEmbeddi\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  positional_embed \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_4 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,                \u001b[38;5;34m0\u001b[0m  input_layer_5[\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_31           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_20 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_embed \n",
              "                                                     dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_24 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_25 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_32           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_21 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_34           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_22 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_26 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_27 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_35           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_23 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_28 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m)       \u001b[38;5;34m6,138\u001b[0m  add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,234</span> (274.35 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m70,234\u001b[0m (274.35 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">70,234</span> (274.35 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m70,234\u001b[0m (274.35 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  # zero is no regularizer (at least with set positional encoding performs better without regularization)\n",
        "num_classes = num_bins+1  # Number of quantization bins\n",
        "d_model = number_channels  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "class PositionalEmbedding_learned(tf.keras.layers.Layer):\n",
        "    def __init__(self, seq_length, d_model, **kwargs):\n",
        "        super(PositionalEmbedding_learned, self).__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.d_model = d_model\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            shape=(seq_length, d_model),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='position_embeddings'\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding_learned, self).get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        # Slice the positional embeddings to match the input sequence length\n",
        "        position_embeddings = self.position_embeddings[:seq_len, :]\n",
        "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
        "        return inputs + position_embeddings\n",
        "\n",
        "\n",
        "def create_attention_mask2(inputs):\n",
        "    # Create padding mask: 1 for valid tokens, 0 for padding tokens\n",
        "    padding_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)  # Shape: (batch_size, seq_length)\n",
        "\n",
        "    # Create causal mask: lower triangular matrix\n",
        "    seq_length = tf.shape(inputs)[1]\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)  # Shape: (seq_length, seq_length)\n",
        "\n",
        "    # Combine masks: only attend to previous tokens and non-padding tokens\n",
        "    attention_mask = tf.expand_dims(padding_mask, axis=1) * causal_mask  # Shape: (batch_size, seq_length, seq_length)\n",
        "\n",
        "    return attention_mask  # Shape: (batch_size, seq_length, seq_length)\n",
        "\n",
        "\n",
        "def create_autoregressive_transformer_freeposemb(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Embed the input tokens (indices) without automatic masking\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(inputs)\n",
        "\n",
        "    # Apply learnable positional embeddings\n",
        "    x = PositionalEmbedding_learned(seq_length, d_model)(x)\n",
        "\n",
        "    # Manually create the attention mask\n",
        "    attention_mask = layers.Lambda(create_attention_mask2)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with manual attention mask\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=attention_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Adjusted sequence length for autoregressive prediction\n",
        "seq_length = sub_image_size * sub_image_size - 1\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_freeposemb = create_autoregressive_transformer_freeposemb(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_freeposemb.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# View the model summary\n",
        "autoregressive_transformer_freeposemb.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f974a0b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 483ms/step - accuracy: 0.0476 - loss: 3.8486 - val_accuracy: 0.0580 - val_loss: 3.5442\n",
            "Epoch 2/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 497ms/step - accuracy: 0.0580 - loss: 3.5309 - val_accuracy: 0.0659 - val_loss: 3.3921\n",
            "Epoch 3/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 533ms/step - accuracy: 0.0698 - loss: 3.3314 - val_accuracy: 0.0808 - val_loss: 3.1690\n",
            "Epoch 4/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 505ms/step - accuracy: 0.0810 - loss: 3.1638 - val_accuracy: 0.0859 - val_loss: 3.1049\n",
            "Epoch 5/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 502ms/step - accuracy: 0.0853 - loss: 3.1139 - val_accuracy: 0.0878 - val_loss: 3.0774\n",
            "Epoch 6/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 508ms/step - accuracy: 0.0873 - loss: 3.0876 - val_accuracy: 0.0895 - val_loss: 3.0581\n",
            "Epoch 7/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 477ms/step - accuracy: 0.0893 - loss: 3.0654 - val_accuracy: 0.0916 - val_loss: 3.0344\n",
            "Epoch 8/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 565ms/step - accuracy: 0.0916 - loss: 3.0405 - val_accuracy: 0.0931 - val_loss: 3.0154\n",
            "Epoch 9/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 527ms/step - accuracy: 0.0933 - loss: 3.0206 - val_accuracy: 0.0962 - val_loss: 2.9894\n",
            "Epoch 10/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 528ms/step - accuracy: 0.0951 - loss: 3.0011 - val_accuracy: 0.0969 - val_loss: 2.9747\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predictions shape: (1, 1023, 186)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 457ms/step - accuracy: 0.0966 - loss: 2.9750\n",
            "Validation Loss: 2.9747  bits per pixel : 4.2916\n",
            "Validation Accuracy: 0.0969\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_freeposemb.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1]  # Take the first sample\n",
        "predictions = autoregressive_transformer_freeposemb.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# Reconstruct and visualize the predicted image\n",
        "#first_token = sample_input.numpy()[0, 0]\n",
        "#reconstructed_sequence = np.concatenate(([first_token], predicted_classes[0]))\n",
        "#reconstructed_image = reconstructed_sequence.reshape(sub_image_size, sub_image_size)\n",
        "\n",
        "#plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.title('Predicted Image')\n",
        "#plt.axis('off')\n",
        "#plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_freeposemb.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c0608c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
