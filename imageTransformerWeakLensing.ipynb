{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed531a95",
      "metadata": {
        "id": "ed531a95"
      },
      "source": [
        "# Implements encoder/decoder for weak lensing outputs\n",
        "\n",
        "The major idea is to see if I can compress the data in the snapshot files.\n",
        "The result is that the compression of many different algorithms based on CNNs (of different depths) is not so much different than averaging neighboring cells (as shown at the end).  This in retrospect is not so surprising as there are differences on the cell scale in the maps that make compression challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zQpe_fjWxGwO",
      "metadata": {
        "id": "zQpe_fjWxGwO"
      },
      "source": [
        "Set configurations for google COLAB if running there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KEtElnI8fDj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtElnI8fDj1",
        "outputId": "59dde345-ea8f-426f-a56a-69affd2aaf3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "use_COLAB = 0 #1 is for on colab, and 2 is for on local machine but using colab\n",
        "\n",
        "if use_COLAB >= 1:\n",
        "  if use_COLAB == 2: # for running in VS CODE\n",
        "      from colabcode import ColabCode\n",
        "      ColabCode(port=10000)\n",
        "  #mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  WORK_AREA = '/content/gdrive/My Drive/weaklensing_ML/' #columbialensing/\n",
        "  os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSxBOjESwy_q",
      "metadata": {
        "id": "MSxBOjESwy_q"
      },
      "source": [
        "\n",
        "## extract tarfiles if necessary and set specs for run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4a5315a",
      "metadata": {
        "id": "a4a5315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "#whether we are training or loading saved\n",
        "train = True\n",
        "load_saved = 1\n",
        "\n",
        "#regularization parameters:\n",
        "## generalization seems great on cosmological data and so generally I run with these off\n",
        "L1weight = 0 #1e-8\n",
        "dropout_rate = 0\n",
        "\n",
        "# Specify the directory containing the .tar files\n",
        "if use_COLAB >= 1:\n",
        "    directory_path = './columbialensing/'\n",
        "else:\n",
        "        directory_path = '../weaklensing_ML/columbialensing/'\n",
        "number_batches = 10\n",
        "normalize_by_RMS = False #set to one if you want to renormalize by RMS\n",
        "\n",
        "# image_size\n",
        "image_size = 1024\n",
        "sub_image_size = 32 #needs to divide image into these units; must divide evenly image_size\n",
        "                    #division is using that it is unlikely there are learnable correlations\n",
        "                    #that allow one to compress the data on large scales in the images\n",
        "                    #dividing images gives more samples to learn correlations\n",
        "number_fits_files = 16 # just sto start\n",
        "\n",
        "\n",
        "number_subimages_across =image_size//sub_image_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extracts only if indicated (could make this more elegant by checking to see if they exist)\n",
        "extract_tarfiles = False  #if I need to extract tarfiles\n",
        "suffix = f\"_{image_size}\"\n",
        "run_suffix = rf\"im{image_size}\"\n",
        "if extract_tarfiles:\n",
        "    # Use a regular expression to match .tar files with the desired suffix\n",
        "    pattern = re.compile(rf\"{suffix}.tar$\")\n",
        "\n",
        "    # List all matching .tar files in the directory\n",
        "    all_tar_files = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "\n",
        "    # Extract the tar archive\n",
        "    for tar_file in all_tar_files:\n",
        "        #print(tar_file)\n",
        "        tar_file_path = os.path.join(directory_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as archive:\n",
        "            archive.extractall(path=directory_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0619681f",
      "metadata": {
        "id": "0619681f"
      },
      "source": [
        "# Read into memory the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "48a05090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a05090",
        "outputId": "de8579ef-f38a-414d-f362-c3a282901a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in Om0.268_si0.801\n",
            "RMS=0.018752897158265114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-26 09:19:53.858924: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
            "2024-10-26 09:19:53.858941: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
            "2024-10-26 09:19:53.858945: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
            "2024-10-26 09:19:53.858958: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-26 09:19:53.858968: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "#import wl_auxiliary\n",
        "\n",
        "def get_labels_for_file(dir_name):\n",
        "    \"\"\"\n",
        "    Extracts labels from the tar file name.\n",
        "    For the file \"Om0.183_si0.958_256.tar\", the labels will be [0.183, 0.958].\n",
        "\n",
        "    Args:\n",
        "    - tar_file_name (str): Name of the tar file.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the two labels extracted from the filename.\n",
        "    \"\"\"\n",
        "    # Split the filename on underscores\n",
        "    parts = dir_name.split('_')\n",
        "\n",
        "    # Extract the numeric values for 'Om' and 'si'\n",
        "    om_label = float(parts[0][2:])\n",
        "    si_label = float(parts[1][2:])\n",
        "\n",
        "    return [om_label, si_label]\n",
        "\n",
        "#now loop through all files in the\n",
        "pattern = re.compile(rf\"{suffix}$\")\n",
        "#all_directories = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "all_directories = [\"Om0.268_si0.801\"] # \"Om0.283_si0.805_256\"\n",
        "num_cosmologies = len(all_directories)\n",
        "\n",
        "random.shuffle(all_directories) #this makes it so that there is no particular order for the directories\n",
        "#print(all_directories)\n",
        "\n",
        "#tensor of labels; there are two labels for each\n",
        "numsubimages = number_subimages_across**2\n",
        "number_images = number_fits_files*numsubimages\n",
        "#cosmology_labels = np.empty((len(all_directories), number_images, 2), dtype=np.float16)\n",
        "\n",
        "RMS =0 #first time set to zero\n",
        "data_array = np.empty((num_cosmologies, number_images, sub_image_size, sub_image_size), dtype=np.float16)\n",
        "\n",
        "number_subimages_total = 0\n",
        "for idy, dir_name in enumerate(all_directories):\n",
        "\n",
        "\n",
        "    #if idy%10 ==0:\n",
        "    print(\"reading in\", dir_name)\n",
        "    dir_path = os.path.join(directory_path, dir_name)\n",
        "\n",
        "    all_files = os.listdir(dir_path)\n",
        "    fits_files = [f for f in all_files if f.endswith('.fits')]\n",
        "\n",
        "\n",
        "\n",
        "    for idx, file in enumerate(fits_files):\n",
        "        if idx >= number_fits_files:\n",
        "            break\n",
        "\n",
        "        with fits.open(os.path.join(dir_path, file)) as hdul:\n",
        "\n",
        "            original_data = hdul[0].data\n",
        "\n",
        "            if RMS == 0: #get RMS to divide by for first file to normalize everything\n",
        "                RMS = np.sqrt(np.var(hdul[0].data))\n",
        "                print(f\"RMS={RMS}\")\n",
        "\n",
        "            ##get rid of NANs, which affects a few files\n",
        "            #if np.isnan(original_data).any():\n",
        "            #    continue\n",
        "            #I've cleaned this out already\n",
        "            for i in range(number_subimages_across):\n",
        "                for j in range(number_subimages_across):\n",
        "                    data_array[idy][numsubimages*idx+ number_subimages_across*i+j] = original_data[sub_image_size*i:sub_image_size*(i+1),\\\n",
        "                                                                  sub_image_size*j:sub_image_size*(j+1)]/RMS\n",
        "                number_subimages_total +=1\n",
        "\n",
        "\n",
        "\n",
        "    #since all fits files in one directory have the same label\n",
        "    cosmology = get_labels_for_file(dir_name)\n",
        "    #cosmology_labels[idy] = np.array([cosmology for i in range(number_fits_files)])\n",
        "\n",
        "\n",
        "    #flatten data_array[idy][numsubimages*idx+ number_subimages_across*i+j]\n",
        "WL_tensor = tf.convert_to_tensor(data_array)\n",
        "\n",
        "WL_tensor = tf.reshape(WL_tensor, (-1, WL_tensor.shape[2], WL_tensor.shape[3]));\n",
        "\n",
        "WL_tensor = WL_tensor[..., np.newaxis]  # Add channel dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f790bd8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Number of images you want to display\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Extract the i-th image tensor\n",
        "    image_tensor = WL_tensor[i, :, :, 0]\n",
        "    # Use TensorFlow operations if needed (optional)\n",
        "    # Display the image\n",
        "    plt.imshow(image_tensor, cmap='gray')\n",
        "    plt.title(f\"Image {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f2885b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number_subimages_across = 32\n",
            "total number of images = 16384 512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([16384, 32, 32, 1])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"number_subimages_across =\", number_subimages_across )\n",
        "print(\"total number of images =\", number_subimages_across*number_subimages_across*number_fits_files *len(all_directories), number_subimages_total)\n",
        "np.shape(WL_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019e88b1",
      "metadata": {},
      "source": [
        "# Renormalize the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1363e4fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Implement Welford's algorithm for numerically stable mean and variance calculation\n",
        "def welford_algorithm(data):\n",
        "    n = 0\n",
        "    mean = 0.0\n",
        "    M2 = 0.0\n",
        "\n",
        "    for x in data:\n",
        "        n += 1\n",
        "        delta = x - mean\n",
        "        mean += delta / n\n",
        "        delta2 = x - mean\n",
        "        M2 += delta * delta2\n",
        "\n",
        "    variance = M2 / (n - 1) if n > 1 else float('nan')\n",
        "    return mean, variance\n",
        "\n",
        "calculate_statistics = False\n",
        "\n",
        "if calculate_statistics:\n",
        "    # Assuming WL_tensor is a TensorFlow tensor of floating-point numbers\n",
        "\n",
        "    # Convert the TensorFlow tensor to a NumPy array\n",
        "    WL_tensor_np = WL_tensor.numpy()\n",
        "    WL_tensor_np = WL_tensor_np[::1000, :, :, :]  # Downsample the tensor for faster computation\n",
        "\n",
        "    # Check for NaNs and Infinities\n",
        "    num_nans = np.isnan(WL_tensor_np).sum()\n",
        "    num_infs = np.isinf(WL_tensor_np).sum()\n",
        "    print(f\"Number of NaNs: {num_nans}\")\n",
        "    print(f\"Number of Infinities: {num_infs}\")\n",
        "\n",
        "\n",
        "    # Inspect the range of values\n",
        "    min_value = WL_tensor_np.min()\n",
        "    max_value = WL_tensor_np.max()\n",
        "    print(f\"Min value: {min_value}\")\n",
        "    print(f\"Max value: {max_value}\")\n",
        "\n",
        "    # Check the shape of the tensor\n",
        "    tensor_shape = WL_tensor_np.shape\n",
        "    print(f\"Tensor shape: {tensor_shape}\")\n",
        "\n",
        "    # Manually calculate the mean and variance\n",
        "    mean_value = np.mean(WL_tensor_np)\n",
        "    variance_value = np.var(WL_tensor_np)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Flatten the tensor to 1D for easier processing\n",
        "    WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate mean and variance using Welford's algorithm\n",
        "    mean_value, variance_value = welford_algorithm(WL_tensor_flat)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Calculate the standard deviation\n",
        "    std_dev = np.std(WL_tensor_np)\n",
        "    print(f\"Standard Deviation: {std_dev}\")\n",
        "\n",
        "    DO_KDE= True\n",
        "    if DO_KDE:\n",
        "        # Flatten the tensor to 1D for PDF calculation\n",
        "        WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
        "        kde = gaussian_kde(WL_tensor_flat)\n",
        "        x_values = np.linspace(WL_tensor_flat.min(), WL_tensor_flat.max(), 1000)\n",
        "        pdf_values = kde(x_values)\n",
        "\n",
        "        # Plot the PDF\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.semilogx(x_values, pdf_values, label='PDF')\n",
        "        plt.hist(WL_tensor_flat, bins=50, density=True, alpha=0.6, color='g', label='Histogram')\n",
        "        plt.title('Probability Density Function of WL_tensor')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0cc9a74",
      "metadata": {},
      "source": [
        "# now I'd like to bin all the pixels into logrithmic bins "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8e46b14b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min value: -1.816\n",
            "Max value: 28.12\n",
            "numbins = 127\n",
            "mean values =  [-1.786, -1.735, -1.688, -1.64, -1.59, -1.54, -1.49, -1.44, -1.392, -1.342, -1.292, -1.243, -1.193, -1.144, -1.094, -1.044, -0.994, -0.9443, -0.8945, -0.845, -0.7954, -0.7456, -0.696, -0.646, -0.596, -0.5464, -0.4968, -0.447, -0.3972, -0.3474, -0.2976, -0.2478, -0.198, -0.1482, -0.0983, -0.04852, 0.001258, 0.05103, 0.10077, 0.1505, 0.2003, 0.25, 0.2998, 0.3496, 0.3994, 0.4492, 0.499, 0.549, 0.5986, 0.6484, 0.698, 0.748, 0.798, 0.8477, 0.8975, 0.9473, 0.997, 1.047, 1.097, 1.146, 1.196, 1.246, 1.296, 1.346, 1.3955, 1.445, 1.495, 1.545, 1.595, 1.645, 1.694, 1.744, 1.794, 1.844, 1.894, 1.943, 1.993, 2.043, 2.092, 2.14, 2.191, 2.242, 2.291, 2.34, 2.39, 2.441, 2.49, 2.54, 2.59, 2.64, 2.69, 2.738, 2.79, 2.84, 2.889, 2.938, 2.988, 3.04, 3.088, 3.137, 3.188, 3.238, 3.287, 3.338, 3.387, 3.438, 3.486, 3.537, 3.586, 3.637, 3.686, 3.734, 3.787, 3.836, 3.885, 3.934, 3.979, 4.42, 5.484, 6.81, 8.43, 10.46, 12.98, 16.19, 20.06, 24.27, nan]\n",
            "binning_scheme =  [-1.8164e+00 -1.7666e+00 -1.7168e+00 -1.6670e+00 -1.6172e+00 -1.5674e+00\n",
            " -1.5176e+00 -1.4678e+00 -1.4180e+00 -1.3682e+00 -1.3184e+00 -1.2686e+00\n",
            " -1.2188e+00 -1.1689e+00 -1.1191e+00 -1.0693e+00 -1.0195e+00 -9.6973e-01\n",
            " -9.1992e-01 -8.7012e-01 -8.2031e-01 -7.7051e-01 -7.2070e-01 -6.7090e-01\n",
            " -6.2109e-01 -5.7129e-01 -5.2148e-01 -4.7168e-01 -4.2188e-01 -3.7207e-01\n",
            " -3.2227e-01 -2.7246e-01 -2.2266e-01 -1.7285e-01 -1.2305e-01 -7.3242e-02\n",
            " -2.3438e-02  2.6367e-02  7.6172e-02  1.2598e-01  1.7578e-01  2.2559e-01\n",
            "  2.7539e-01  3.2520e-01  3.7500e-01  4.2480e-01  4.7461e-01  5.2441e-01\n",
            "  5.7422e-01  6.2402e-01  6.7383e-01  7.2363e-01  7.7344e-01  8.2324e-01\n",
            "  8.7305e-01  9.2285e-01  9.7266e-01  1.0225e+00  1.0723e+00  1.1221e+00\n",
            "  1.1719e+00  1.2217e+00  1.2715e+00  1.3213e+00  1.3711e+00  1.4209e+00\n",
            "  1.4707e+00  1.5205e+00  1.5703e+00  1.6201e+00  1.6699e+00  1.7197e+00\n",
            "  1.7695e+00  1.8193e+00  1.8691e+00  1.9189e+00  1.9688e+00  2.0195e+00\n",
            "  2.0684e+00  2.1172e+00  2.1680e+00  2.2188e+00  2.2676e+00  2.3164e+00\n",
            "  2.3672e+00  2.4180e+00  2.4668e+00  2.5156e+00  2.5664e+00  2.6172e+00\n",
            "  2.6660e+00  2.7148e+00  2.7656e+00  2.8164e+00  2.8652e+00  2.9141e+00\n",
            "  2.9648e+00  3.0156e+00  3.0645e+00  3.1133e+00  3.1641e+00  3.2148e+00\n",
            "  3.2637e+00  3.3125e+00  3.3633e+00  3.4141e+00  3.4629e+00  3.5117e+00\n",
            "  3.5625e+00  3.6133e+00  3.6621e+00  3.7109e+00  3.7617e+00  3.8125e+00\n",
            "  3.8613e+00  3.9102e+00  3.9609e+00  4.0000e+00  4.9688e+00  6.1719e+00\n",
            "  7.6641e+00  9.5156e+00  1.1820e+01  1.4688e+01  1.8234e+01  2.2656e+01\n",
            "  2.8141e+01]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = arr.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "#number_pixels = 100\n",
        "#dlogval = np.log10(max_value - min_value)/number_pixels\n",
        "#print(dlogval)\n",
        "\n",
        "# Find the minimum value\n",
        "min_value = tf.reduce_min(WL_tensor)\n",
        "\n",
        "# Find the maximum value\n",
        "max_value = tf.reduce_max(WL_tensor)\n",
        "\n",
        "print(\"Min value:\", min_value.numpy())\n",
        "print(\"Max value:\", max_value.numpy())\n",
        "\n",
        "\n",
        "# Flatten the tensor to 1D for easier processing\n",
        "WL_tensor_flat = tf.reshape(WL_tensor, [-1])\n",
        " \n",
        "ddelta = 0.05 #value tracked in terms of RMS\n",
        "\n",
        "#binning_scheme = np.arange(min_value, -3, ddelta)\n",
        "binning_scheme = np.arange(min_value, 4, ddelta) #since there will be more rate pixels in other direction\n",
        "binning_scheme = np.append(binning_scheme, np.logspace(np.log10(4), np.log10(max_value), 10)) #ten bins for very high values\n",
        "\n",
        "# Convert binning_scheme to float16\n",
        "binning_scheme = binning_scheme.astype(np.float16)\n",
        "\n",
        "num_bins = len(binning_scheme)\n",
        "print(\"numbins =\", num_bins)\n",
        "\n",
        "WL_tensor_binned =  np.floor((WL_tensor_flat[WL_tensor_flat <4]-min_value)/ddelta)*ddelta - WL_tensor_flat[WL_tensor_flat <4] + min_value\n",
        "\n",
        "#print(np.std(WL_tensor_binned))\n",
        "\n",
        "\n",
        "# Digitize the tensor values according to the binning scheme\n",
        "bin_indices = tf.searchsorted(binning_scheme, WL_tensor_flat, side='right') - 1\n",
        "\n",
        "\n",
        "\n",
        "#let's calculate the mean we expect in each bin\n",
        "mean_values = [np.mean(WL_tensor_flat[bin_indices == index]) for index in range(len(binning_scheme))]\n",
        "\n",
        "print(\"mean values = \", mean_values)\n",
        "\n",
        "encoded_tensor = tf.gather(mean_values, bin_indices) #realization of quantized tensor\n",
        "\n",
        "print(\"binning_scheme = \",  binning_scheme)\n",
        "\n",
        "#reshape\n",
        "bin_indices = tf.reshape(bin_indices, [number_images,  sub_image_size*sub_image_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f8b7750",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-0.696  -0.696  -0.7954 -0.845  -0.7456 -0.696  -0.4968 -0.447  -0.596\n",
            " -0.7456], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[-0.685  -0.7046 -0.7974 -0.851  -0.763  -0.673  -0.4883 -0.4543 -0.608\n",
            " -0.762 ], shape=(10,), dtype=float16)\n",
            "now calculating std on dataset of size 16778:\n",
            "std_quantized = 0.03424\n"
          ]
        }
      ],
      "source": [
        "skip = 1000\n",
        "diff_tensor = encoded_tensor[::skip] - WL_tensor_flat[::skip]\n",
        "\n",
        "#just to check quantization works\n",
        "print(encoded_tensor[-10:])\n",
        "print(WL_tensor_flat[-10:])\n",
        "\n",
        "print(f\"now calculating std on dataset of size {len(diff_tensor)}:\")\n",
        "std_quantized = np.std(diff_tensor)\n",
        "print(\"std_quantized =\", std_quantized)\n",
        "\n",
        "####this is very slow\n",
        "#print(\"now calculating std using welford algorithm:\")\n",
        "#mean_value_diff, variance_value_diff = welford_algorithm(diff_tensor)\n",
        "#print(\"std_quantized =\", np.sqrt(variance_value_diff))\n",
        "\n",
        "#plot histogram of values ....very show\n",
        "plot_hist = False\n",
        "if plot_hist:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Plot the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(diff_tensor, bins=30, edgecolor='black', alpha=0.7)\n",
        "    plt.title('Histogram of std_quantized')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5f23c29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices) =  (16384, 32, 32, 1) (16654944,) (16384, 1024) <dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "print(\"np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices) = \", np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices), bin_indices.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684c8c58",
      "metadata": {},
      "source": [
        "# Autoregressive image transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9dfa8755",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: tf.Tensor([16384  1024], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size*2  # Embedding dimension (d_model); sub_image_size*2 results in Nyquist sampling of the image\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0.01\n",
        "num_classes = num_bins  # Set num_classes to num_bins\n",
        "num_heads = 8 #number of attention heads\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(x)  # (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)  # Use width-1 to match seq_length\n",
        "\n",
        "    # Create causal mask\n",
        "    mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    mask = tf.cast(mask, dtype=tf.bool)\n",
        "\n",
        "    \n",
        "    for _ in range(n_layers):\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=d_model // num_heads\n",
        "        )(x, x, attention_mask=mask)\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(x)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Assume bin_indices is provided and has shape (num_samples, seq_length)\n",
        "# For demonstration, let's create a dummy bin_indices\n",
        "\n",
        "height = sub_image_size\n",
        "width = sub_image_size\n",
        "#seq_length = height * width\n",
        "\n",
        "\n",
        "print(\"Shape of bin_indices:\", tf.shape(bin_indices))\n",
        "\n",
        "# Adjust bin_indices to be zero-based indices\n",
        "#bin_indices_zero_based = bin_indices - 1  # Subtract 1 to make indices start from 0\n",
        "\n",
        "# Ensure that bin_indices_zero_based contains valid indices in the range [0, num_bins - 1]\n",
        "#assert bin_indices_zero_based.min() >= 0 and bin_indices_zero_based.max() < num_bins\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_old = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=number_channels,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_old.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d795f89c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_13        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,128</span> │ input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ positional_encod… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ positional_encod… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_43          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encod… │\n",
              "│                     │                   │            │ dropout_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_44          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_46          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_47          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,255</span> │ layer_normalizat… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_13      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_13        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m8,128\u001b[0m │ input_layer_13[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ positional_encod… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ positional_encod… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_43          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_28 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ positional_encod… │\n",
              "│                     │                   │            │ dropout_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m4,160\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_44          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_29 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_46          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_30 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m4,160\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_47          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_31 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
              "│                     │                   │            │ dropout_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m127\u001b[0m) │      \u001b[38;5;34m8,255\u001b[0m │ layer_normalizat… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,495</span> (228.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,495\u001b[0m (228.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,495</span> (228.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,495\u001b[0m (228.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "autoregressive_transformer_old.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "9d6a013b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 522ms/step - accuracy: 0.0742 - loss: 5.8166 - val_accuracy: 0.1024 - val_loss: 2.9617\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 508ms/step - accuracy: 0.1026 - loss: 2.9563 - val_accuracy: 0.1063 - val_loss: 2.9269\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 513ms/step - accuracy: 0.1121 - loss: 2.8702 - val_accuracy: 0.1449 - val_loss: 2.5900\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2558s\u001b[0m 3s/step - accuracy: 0.1425 - loss: 2.6099 - val_accuracy: 0.1573 - val_loss: 2.4978\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 512ms/step - accuracy: 0.1521 - loss: 2.5343 - val_accuracy: 0.1625 - val_loss: 2.4635\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step\n",
            "Predictions shape: (1, 1023, 127)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 396ms/step - accuracy: 0.1632 - loss: 2.4644\n",
            "Validation Loss: 2.4635  bits per pixel : 3.5541\n",
            "Validation Accuracy: 0.1625\n"
          ]
        }
      ],
      "source": [
        "# Optionally split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_old.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1].numpy()  # Take the first sample\n",
        "predictions = autoregressive_transformer_old.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Optionally, you can get the most likely class for each position\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# You can also evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_old.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275e2ddb",
      "metadata": {},
      "source": [
        "# Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d07388d",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 169\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Run the generalized code\u001b[39;00m\n\u001b[1;32m    168\u001b[0m num_samples_to_visualize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Change this to the number of random samples you want to visualize\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m process_random_images(\u001b[43mX_val\u001b[49m, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Much slower code that I don't use anymore\n",
        "def autoregressive_predict(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model, one step at a time,\n",
        "    using the original sequence up to the current position as context.\n",
        "    \"\"\"\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for i in range(1, sequence_length+1):\n",
        "        # Prepare the context (all previous pixels)\n",
        "        context = original_sequence[:i]\n",
        "\n",
        "        # No padding required; model expects input of length seq_length\n",
        "        # For positions where context is shorter, we need to pad or adjust the input\n",
        "        # Since we're avoiding padding, we'll adjust the input sequence accordingly\n",
        "\n",
        "        # Create a context of length 'i'\n",
        "        model_input = np.array(context, dtype=np.int32)[np.newaxis, :]\n",
        "\n",
        "        # Since the model expects input of shape (batch_size, seq_length),\n",
        "        # we need to handle the varying lengths\n",
        "        # One way is to slice the model to accept variable input lengths\n",
        "\n",
        "        # Predict\n",
        "        next_pixel_probs = model.predict(model_input, verbose=0)\n",
        "        # Get the prediction for the current position\n",
        "        next_pixel = np.argmax(next_pixel_probs[0, -1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Step {i}: Context length: {len(context)}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "#should be much faster....need to check\n",
        "def autoregressive_predict_batched(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model by batching inputs,\n",
        "    which significantly speeds up the prediction process.\n",
        "    \"\"\"\n",
        "    # Prepare all contexts at once\n",
        "    contexts = [original_sequence[:i] for i in range(1, sequence_length + 1)]\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = sequence_length\n",
        "    padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        contexts, maxlen=max_len, padding='post', value=0\n",
        "    )\n",
        "\n",
        "    # Convert to array\n",
        "    model_inputs = np.array(padded_contexts, dtype=np.int32)\n",
        "\n",
        "    # Predict all at once\n",
        "    next_pixel_probs = model.predict(model_inputs, verbose=0)\n",
        "\n",
        "    # Extract the predictions\n",
        "    predicted_sequence = []\n",
        "    for i, context in enumerate(contexts):\n",
        "        # Get the prediction for the current position\n",
        "        seq_length = len(context)\n",
        "        next_pixel = np.argmax(next_pixel_probs[i, seq_length - 1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1}: Context length: {seq_length}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "def process_random_images(X_val, y_val, model, sub_image_size, num_samples=5):\n",
        "    \"\"\"\n",
        "    Process and visualize random images from the validation set.\n",
        "    \"\"\"\n",
        "    total_samples = len(X_val)\n",
        "    random_indices = np.random.choice(total_samples, num_samples, replace=False)\n",
        "    \n",
        "    \n",
        "    for i, idx in enumerate(random_indices):\n",
        "        val_sample = X_val[idx]\n",
        "        original_sequence = y_val[idx]\n",
        "        \n",
        "        # Determine the sequence length\n",
        "        sequence_length = len(original_sequence)\n",
        "        \n",
        "        # Predict using the original sequence as context\n",
        "        predicted_sequence = autoregressive_predict_batched(\n",
        "            model,\n",
        "            original_sequence,\n",
        "            sequence_length,\n",
        "        )\n",
        "        \n",
        "        print(\"val_sample and original_sequence shapes\", np.shape(val_sample), np.shape(original_sequence), np.shape(predicted_sequence))\n",
        "    \n",
        "        visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, i+1)\n",
        "        \n",
        "        mse = np.mean((original_sequence - predicted_sequence)**2)\n",
        "        mae = np.mean(np.abs(original_sequence - predicted_sequence))\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.4f}\")\n",
        "        print(f\"  Original sequence shape: {original_sequence.shape}\")\n",
        "        print(f\"  Predicted sequence shape: {predicted_sequence.shape}\")\n",
        "        print(f\" fraction of pixles correct\", np.sum(original_sequence - predicted_sequence == 0)/len(original_sequence))\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# The visualize_autoregressive_prediction function remains the same\n",
        "\n",
        "def visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, predicted, and difference images with equal sizes.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "    fig.suptitle(f'Sample {index}', fontsize=16)\n",
        "\n",
        "    # Prepare images\n",
        "    original_2d = sequence_to_image(original_sequence, sub_image_size, original=True)\n",
        "    predicted_2d = sequence_to_image(predicted_sequence, sub_image_size, original=False)\n",
        "    diff = original_2d - predicted_2d\n",
        "\n",
        "    # Set up common parameters for imshow\n",
        "    imshow_args = {'interpolation': 'nearest', 'aspect': 'equal'}\n",
        "\n",
        "    # Visualize original sequence\n",
        "    im1 = axes[0].imshow(original_2d, cmap='gray', **imshow_args)\n",
        "    axes[0].set_title('Original Image')\n",
        "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize predicted sequence\n",
        "    im2 = axes[1].imshow(predicted_2d, cmap='gray', **imshow_args)\n",
        "    axes[1].set_title('Predicted Image (Autoregressive)')\n",
        "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize difference\n",
        "    im3 = axes[2].imshow(diff, cmap='bwr', **imshow_args)\n",
        "    axes[2].set_title('Difference (Original - Predicted)')\n",
        "    plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sequence_to_image(sequence, sub_image_size, original=True):\n",
        "    \"\"\"\n",
        "    Convert a 1D sequence of length 1023 to a 2D image of size sub_image_size x sub_image_size,\n",
        "    by prepending the first_token to the sequence.\n",
        "    \"\"\"\n",
        "    # Concatenate the first token to the sequence\n",
        "    if original:\n",
        "        full_sequence = np.concatenate((sequence, [0]))\n",
        "    else:\n",
        "        full_sequence = np.concatenate(([0], sequence))\n",
        "    # Reshape to image\n",
        "    image = full_sequence.reshape(sub_image_size, sub_image_size)\n",
        "    return image\n",
        "\n",
        "# Run the generalized code\n",
        "num_samples_to_visualize = 3  # Change this to the number of random samples you want to visualize\n",
        "process_random_images(X_val, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349c2e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot predictions vs actual values\n",
        "def plot_predictions(model, sequences, targets, num_samples=10):\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(sequences[:num_samples])\n",
        "    predicted_pixels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Plot the actual vs predicted pixel values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(sequences[i].reshape(-1, 1), cmap='gray', aspect='auto')\n",
        "        plt.title(f\"Sequence {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(num_classes), predictions[i])\n",
        "        plt.axvline(x=targets[i], color='r', linestyle='--')\n",
        "        plt.axvline(x=predicted_pixels[i], color='g', linestyle='--')\n",
        "        plt.title(f\"True: {targets[i]}, Pred: {predicted_pixels[i]}\")\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions for some sequences\n",
        "plot_predictions(autoregressive_transformer, train_sequences, train_targets, num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3f181ef2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab491d",
      "metadata": {},
      "source": [
        "## Simple autoencoder with sines and cosines sampling to Nyquist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98c92482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,128</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ positional_encod… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "│                     │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encod… │\n",
              "│                     │                   │            │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "│                     │                   │            │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│                     │                   │            │ dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,255</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │      \u001b[38;5;34m8,128\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_encodin… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ positional_encod… │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "│                     │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ positional_encod… │\n",
              "│                     │                   │            │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,448\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_5 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "│                     │                   │            │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_6 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m128\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │     \u001b[38;5;34m16,640\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m16,448\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│                     │                   │            │ dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m) │      \u001b[38;5;34m8,255\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,351</span> (454.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,351\u001b[0m (454.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,351</span> (454.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,351\u001b[0m (454.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  #don't think need L1 and dropout  (I've tested and it performs best with this equal to zero)\n",
        "num_classes = num_bins  # Set num_classes to num_bins: this is the number of values that can be populated\n",
        "d_model = number_channels*2  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.d_model = d_model\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    #inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)  # Accept variable-length sequences\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(x)  # Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Create the causal mask once as a constant tensor\n",
        "    #causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    #causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    def create_causal_mask(x):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        return causal_mask\n",
        "\n",
        "    causal_mask = layers.Lambda(create_causal_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_length = sub_image_size * sub_image_size\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_deep = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers*2,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "\n",
        "autoregressive_transformer.summary()\n",
        "\n",
        "autoregressive_transformer_deep.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6de246",
      "metadata": {},
      "source": [
        "## Compile models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb4fc3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_deep.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1a6b2",
      "metadata": {},
      "source": [
        "### Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "650102df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "2024-10-26 09:20:18.755213: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 522ms/step - accuracy: 0.0862 - loss: 3.3005 - val_accuracy: 0.1213 - val_loss: 2.7849\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 549ms/step - accuracy: 0.1242 - loss: 2.7409 - val_accuracy: 0.1484 - val_loss: 2.5348\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 558ms/step - accuracy: 0.1467 - loss: 2.5444 - val_accuracy: 0.1628 - val_loss: 2.4392\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 550ms/step - accuracy: 0.1598 - loss: 2.4570 - val_accuracy: 0.1752 - val_loss: 2.3618\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 545ms/step - accuracy: 0.1688 - loss: 2.3998 - val_accuracy: 0.1790 - val_loss: 2.3372\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 418ms/step - accuracy: 0.1798 - loss: 2.3376\n",
            "Validation Loss: 2.3372  bits per pixel : 3.3719\n",
            "Validation Accuracy: 0.1790\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b344ac9",
      "metadata": {},
      "source": [
        "### Let's fit the deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "49bec05f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.0871 - loss: 3.2729 - val_accuracy: 0.1239 - val_loss: 2.7536\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1021s\u001b[0m 1s/step - accuracy: 0.1277 - loss: 2.7058 - val_accuracy: 0.1580 - val_loss: 2.4666\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 1s/step - accuracy: 0.1560 - loss: 2.4744 - val_accuracy: 0.1778 - val_loss: 2.3402\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1109s\u001b[0m 1s/step - accuracy: 0.1720 - loss: 2.3700 - val_accuracy: 0.1864 - val_loss: 2.2889\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 1s/step - accuracy: 0.1809 - loss: 2.3185 - val_accuracy: 0.1920 - val_loss: 2.2581\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 471ms/step - accuracy: 0.1798 - loss: 2.3376\n",
            "Validation Loss: 2.3372  bits per pixel : 3.3719\n",
            "Validation Accuracy: 0.1790\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_deep.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489870fe",
      "metadata": {},
      "source": [
        "## This has a free positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6cb767fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'positional_embedding' (of type PositionalEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,736</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ positional_embed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_embed… │\n",
              "│                     │                   │            │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ layer_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_28          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_29          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │ add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m4,096\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m32,736\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ positional_embed… │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m4,224\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_16 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ positional_embed… │\n",
              "│                     │                   │            │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,224\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m4,128\u001b[0m │ dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_17 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m4,224\u001b[0m │ layer_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_28          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_18 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,224\u001b[0m │ layer_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m4,128\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_29          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add_19 (\u001b[38;5;33mAdd\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m4,224\u001b[0m │ add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  # zero is no regularizer (at least with set positional encoding performs better without regularization)\n",
        "num_classes = num_bins+1  # Number of quantization bins\n",
        "d_model = number_channels  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Embedding Layer with learnable parameters\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, seq_length, d_model, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.d_model = d_model\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            shape=(seq_length, d_model),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='position_embeddings'\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, seq_length, d_model)\n",
        "        return inputs + self.position_embeddings\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Adjusted for autoregressive prediction\n",
        "    inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(inputs)\n",
        "\n",
        "    # Apply learnable positional embeddings\n",
        "    x = PositionalEmbedding(seq_length, d_model)(x)\n",
        "\n",
        "    # Create the causal mask\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Adjusted sequence length for autoregressive prediction\n",
        "seq_length = sub_image_size * sub_image_size - 1\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_freeposemb = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_freeposemb.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# View the model summary\n",
        "autoregressive_transformer_freeposemb.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f974a0b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 473ms/step - accuracy: 0.0892 - loss: 3.2488 - val_accuracy: 0.1038 - val_loss: 2.9402\n",
            "Epoch 2/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 474ms/step - accuracy: 0.1047 - loss: 2.9234 - val_accuracy: 0.1215 - val_loss: 2.7774\n",
            "Epoch 3/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 494ms/step - accuracy: 0.1269 - loss: 2.7261 - val_accuracy: 0.1461 - val_loss: 2.5764\n",
            "Epoch 4/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 514ms/step - accuracy: 0.1474 - loss: 2.5605 - val_accuracy: 0.1595 - val_loss: 2.4724\n",
            "Epoch 5/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 500ms/step - accuracy: 0.1589 - loss: 2.4741 - val_accuracy: 0.1677 - val_loss: 2.4133\n",
            "Epoch 6/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 493ms/step - accuracy: 0.1668 - loss: 2.4227 - val_accuracy: 0.1720 - val_loss: 2.3865\n",
            "Epoch 7/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 499ms/step - accuracy: 0.1709 - loss: 2.3979 - val_accuracy: 0.1746 - val_loss: 2.3719\n",
            "Epoch 8/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 468ms/step - accuracy: 0.1735 - loss: 2.3823 - val_accuracy: 0.1765 - val_loss: 2.3580\n",
            "Epoch 9/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 461ms/step - accuracy: 0.1751 - loss: 2.3716 - val_accuracy: 0.1785 - val_loss: 2.3528\n",
            "Epoch 10/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 464ms/step - accuracy: 0.1768 - loss: 2.3613 - val_accuracy: 0.1794 - val_loss: 2.3425\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "Predictions shape: (1, 1023, 128)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 414ms/step - accuracy: 0.1799 - loss: 2.3426\n",
            "Validation Loss: 2.3425  bits per pixel : 3.3795\n",
            "Validation Accuracy: 0.1794\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_freeposemb.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1]  # Take the first sample\n",
        "predictions = autoregressive_transformer_freeposemb.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# Reconstruct and visualize the predicted image\n",
        "#first_token = sample_input.numpy()[0, 0]\n",
        "#reconstructed_sequence = np.concatenate(([first_token], predicted_classes[0]))\n",
        "#reconstructed_image = reconstructed_sequence.reshape(sub_image_size, sub_image_size)\n",
        "\n",
        "#plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.title('Predicted Image')\n",
        "#plt.axis('off')\n",
        "#plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_freeposemb.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c0608c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
