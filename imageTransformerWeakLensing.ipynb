{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed531a95",
      "metadata": {
        "id": "ed531a95"
      },
      "source": [
        "# Implements encoder/decoder for weak lensing outputs\n",
        "\n",
        "The major idea is to see if I can compress the data in the snapshot files.\n",
        "The result is that the compression of many different algorithms based on CNNs (of different depths) is not so much different than averaging neighboring cells (as shown at the end).  This in retrospect is not so surprising as there are differences on the cell scale in the maps that make compression challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zQpe_fjWxGwO",
      "metadata": {
        "id": "zQpe_fjWxGwO"
      },
      "source": [
        "Set configurations for google COLAB if running there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "KEtElnI8fDj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtElnI8fDj1",
        "outputId": "59dde345-ea8f-426f-a56a-69affd2aaf3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "use_COLAB = 0 #1 is for on colab, and 2 is for on local machine but using colab\n",
        "\n",
        "if use_COLAB >= 1:\n",
        "  if use_COLAB == 2: # for running in VS CODE\n",
        "      from colabcode import ColabCode\n",
        "      ColabCode(port=10000)\n",
        "  #mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  WORK_AREA = '/content/gdrive/My Drive/weaklensing_ML/' #columbialensing/\n",
        "  os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSxBOjESwy_q",
      "metadata": {
        "id": "MSxBOjESwy_q"
      },
      "source": [
        "\n",
        "## extract tarfiles if necessary and set specs for run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4a5315a",
      "metadata": {
        "id": "a4a5315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "#whether we are training or loading saved\n",
        "train = True\n",
        "load_saved = 1\n",
        "\n",
        "# Specify the directory containing the .tar files\n",
        "if use_COLAB >= 1:\n",
        "    directory_path = './columbialensing/'\n",
        "else:\n",
        "        directory_path = '../weaklensing_ML/columbialensing/'\n",
        "number_batches = 10\n",
        "normalize_by_RMS = False #set to one if you want to renormalize by RMS\n",
        "\n",
        "# image_size\n",
        "image_size = 1024\n",
        "sub_image_size = 32 #needs to divide image into these units; must divide evenly image_size\n",
        "                    #division is using that it is unlikely there are learnable correlations\n",
        "                    #that allow one to compress the data on large scales in the images\n",
        "                    #dividing images gives more samples to learn correlations\n",
        "number_fits_files = 16 # just sto start\n",
        "\n",
        "\n",
        "number_subimages_across =image_size//sub_image_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extracts only if indicated (could make this more elegant by checking to see if they exist)\n",
        "extract_tarfiles = False  #if I need to extract tarfiles\n",
        "suffix = f\"_{image_size}\"\n",
        "run_suffix = rf\"im{image_size}\"\n",
        "if extract_tarfiles:\n",
        "    # Use a regular expression to match .tar files with the desired suffix\n",
        "    pattern = re.compile(rf\"{suffix}.tar$\")\n",
        "\n",
        "    # List all matching .tar files in the directory\n",
        "    all_tar_files = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "\n",
        "    # Extract the tar archive\n",
        "    for tar_file in all_tar_files:\n",
        "        #print(tar_file)\n",
        "        tar_file_path = os.path.join(directory_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as archive:\n",
        "            archive.extractall(path=directory_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0619681f",
      "metadata": {
        "id": "0619681f"
      },
      "source": [
        "# Read into memory the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "48a05090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a05090",
        "outputId": "de8579ef-f38a-414d-f362-c3a282901a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in Om0.268_si0.801\n",
            "RMS=0.018752897158265114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-27 11:54:43.017884: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
            "2024-10-27 11:54:43.017903: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
            "2024-10-27 11:54:43.017908: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
            "2024-10-27 11:54:43.017922: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-27 11:54:43.017931: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "#import wl_auxiliary\n",
        "\n",
        "def get_labels_for_file(dir_name):\n",
        "    \"\"\"\n",
        "    Extracts labels from the tar file name.\n",
        "    For the file \"Om0.183_si0.958_256.tar\", the labels will be [0.183, 0.958].\n",
        "\n",
        "    Args:\n",
        "    - tar_file_name (str): Name of the tar file.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the two labels extracted from the filename.\n",
        "    \"\"\"\n",
        "    # Split the filename on underscores\n",
        "    parts = dir_name.split('_')\n",
        "\n",
        "    # Extract the numeric values for 'Om' and 'si'\n",
        "    om_label = float(parts[0][2:])\n",
        "    si_label = float(parts[1][2:])\n",
        "\n",
        "    return [om_label, si_label]\n",
        "\n",
        "#now loop through all files in the\n",
        "pattern = re.compile(rf\"{suffix}$\")\n",
        "#all_directories = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "all_directories = [\"Om0.268_si0.801\"] # \"Om0.283_si0.805_256\"\n",
        "num_cosmologies = len(all_directories)\n",
        "\n",
        "random.shuffle(all_directories) #this makes it so that there is no particular order for the directories\n",
        "#print(all_directories)\n",
        "\n",
        "#tensor of labels; there are two labels for each\n",
        "numsubimages = number_subimages_across**2\n",
        "number_images = number_fits_files*numsubimages\n",
        "#cosmology_labels = np.empty((len(all_directories), number_images, 2), dtype=np.float16)\n",
        "\n",
        "RMS =0 #first time set to zero\n",
        "data_array = np.empty((num_cosmologies, number_images, sub_image_size, sub_image_size), dtype=np.float16)\n",
        "\n",
        "number_subimages_total = 0\n",
        "for idy, dir_name in enumerate(all_directories):\n",
        "\n",
        "\n",
        "    #if idy%10 ==0:\n",
        "    print(\"reading in\", dir_name)\n",
        "    dir_path = os.path.join(directory_path, dir_name)\n",
        "\n",
        "    all_files = os.listdir(dir_path)\n",
        "    fits_files = [f for f in all_files if f.endswith('.fits')]\n",
        "\n",
        "\n",
        "\n",
        "    for idx, file in enumerate(fits_files):\n",
        "        if idx >= number_fits_files:\n",
        "            break\n",
        "\n",
        "        with fits.open(os.path.join(dir_path, file)) as hdul:\n",
        "\n",
        "            original_data = hdul[0].data\n",
        "\n",
        "            if RMS == 0: #get RMS to divide by for first file to normalize everything\n",
        "                RMS = np.sqrt(np.var(hdul[0].data))\n",
        "                print(f\"RMS={RMS}\")\n",
        "\n",
        "            ##get rid of NANs, which affects a few files\n",
        "            #if np.isnan(original_data).any():\n",
        "            #    continue\n",
        "            #I've cleaned this out already\n",
        "            for i in range(number_subimages_across):\n",
        "                for j in range(number_subimages_across):\n",
        "                    data_array[idy][numsubimages*idx+ number_subimages_across*i+j] = original_data[sub_image_size*i:sub_image_size*(i+1),\\\n",
        "                                                                  sub_image_size*j:sub_image_size*(j+1)]/RMS\n",
        "                number_subimages_total +=1\n",
        "\n",
        "\n",
        "\n",
        "    #since all fits files in one directory have the same label\n",
        "    cosmology = get_labels_for_file(dir_name)\n",
        "    #cosmology_labels[idy] = np.array([cosmology for i in range(number_fits_files)])\n",
        "\n",
        "\n",
        "    #flatten data_array[idy][numsubimages*idx+ number_subimages_across*i+j]\n",
        "WL_tensor = tf.convert_to_tensor(data_array)\n",
        "\n",
        "WL_tensor = tf.reshape(WL_tensor, (-1, WL_tensor.shape[2], WL_tensor.shape[3]));\n",
        "\n",
        "WL_tensor = WL_tensor[..., np.newaxis]  # Add channel dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f790bd8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Number of images you want to display\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Extract the i-th image tensor\n",
        "    image_tensor = WL_tensor[i, :, :, 0]\n",
        "    # Use TensorFlow operations if needed (optional)\n",
        "    # Display the image\n",
        "    plt.imshow(image_tensor, cmap='gray')\n",
        "    plt.title(f\"Image {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2885b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number_subimages_across = 32\n",
            "total number of images = 16384 512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([16384, 32, 32, 1])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"number_subimages_across =\", number_subimages_across )\n",
        "print(\"total number of images =\", number_subimages_across*number_subimages_across*number_fits_files *len(all_directories), number_subimages_total)\n",
        "np.shape(WL_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019e88b1",
      "metadata": {},
      "source": [
        "# Renormalize the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1363e4fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Implement Welford's algorithm for numerically stable mean and variance calculation\n",
        "def welford_algorithm(data):\n",
        "    n = 0\n",
        "    mean = 0.0\n",
        "    M2 = 0.0\n",
        "\n",
        "    for x in data:\n",
        "        n += 1\n",
        "        delta = x - mean\n",
        "        mean += delta / n\n",
        "        delta2 = x - mean\n",
        "        M2 += delta * delta2\n",
        "\n",
        "    variance = M2 / (n - 1) if n > 1 else float('nan')\n",
        "    return mean, variance\n",
        "\n",
        "calculate_statistics = False\n",
        "\n",
        "if calculate_statistics:\n",
        "    # Assuming WL_tensor is a TensorFlow tensor of floating-point numbers\n",
        "\n",
        "    # Convert the TensorFlow tensor to a NumPy array\n",
        "    WL_tensor_np = WL_tensor.numpy()\n",
        "    WL_tensor_np = WL_tensor_np[::1000, :, :, :]  # Downsample the tensor for faster computation\n",
        "\n",
        "    # Check for NaNs and Infinities\n",
        "    num_nans = np.isnan(WL_tensor_np).sum()\n",
        "    num_infs = np.isinf(WL_tensor_np).sum()\n",
        "    print(f\"Number of NaNs: {num_nans}\")\n",
        "    print(f\"Number of Infinities: {num_infs}\")\n",
        "\n",
        "\n",
        "    # Inspect the range of values\n",
        "    min_value = WL_tensor_np.min()\n",
        "    max_value = WL_tensor_np.max()\n",
        "    print(f\"Min value: {min_value}\")\n",
        "    print(f\"Max value: {max_value}\")\n",
        "\n",
        "    # Check the shape of the tensor\n",
        "    tensor_shape = WL_tensor_np.shape\n",
        "    print(f\"Tensor shape: {tensor_shape}\")\n",
        "\n",
        "    # Manually calculate the mean and variance\n",
        "    mean_value = np.mean(WL_tensor_np)\n",
        "    variance_value = np.var(WL_tensor_np)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Flatten the tensor to 1D for easier processing\n",
        "    WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate mean and variance using Welford's algorithm\n",
        "    mean_value, variance_value = welford_algorithm(WL_tensor_flat)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Calculate the standard deviation\n",
        "    std_dev = np.std(WL_tensor_np)\n",
        "    print(f\"Standard Deviation: {std_dev}\")\n",
        "\n",
        "    DO_KDE= True\n",
        "    if DO_KDE:\n",
        "        # Flatten the tensor to 1D for PDF calculation\n",
        "        WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
        "        kde = gaussian_kde(WL_tensor_flat)\n",
        "        x_values = np.linspace(WL_tensor_flat.min(), WL_tensor_flat.max(), 1000)\n",
        "        pdf_values = kde(x_values)\n",
        "\n",
        "        # Plot the PDF\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.semilogx(x_values, pdf_values, label='PDF')\n",
        "        plt.hist(WL_tensor_flat, bins=50, density=True, alpha=0.6, color='g', label='Histogram')\n",
        "        plt.title('Probability Density Function of WL_tensor')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a635a7",
      "metadata": {},
      "source": [
        "# Motivated binning method that partitions so that variance in each bin should be the same"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "2439072c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Variance: 0.1\n",
            "Target Variance per Bin: 2e-06\n",
            "num bins= 122\n",
            "Target Variance: 0.1\n",
            "Target Variance per Bin: 8.19672131147541e-07\n",
            "num bins= 166\n",
            "Target Variance: 0.1\n",
            "Target Variance per Bin: 6.024096385542169e-07\n",
            "num bins= 186\n",
            "mean values =  [-1.493, -1.393, -1.335, -1.283, -1.242, -1.205, -1.17, -1.139, -1.108, -1.077, -1.05, -1.02, -0.985, -0.955, -0.9277, -0.902, -0.8794, -0.8564, -0.8296, -0.8037, -0.78, -0.7593, -0.745, -0.7275, -0.704, -0.682, -0.661, -0.639, -0.619, -0.5986, -0.577, -0.556, -0.535, -0.512, -0.488, -0.465, -0.443, -0.421, -0.398, -0.3765, -0.3547, -0.3325, -0.31, -0.2876, -0.2646, -0.2416, -0.2185, -0.1954, -0.1721, -0.1487, -0.1251, -0.10126, -0.0772, -0.0531, -0.0288, -0.004242, 0.02051, 0.0455, 0.0707, 0.096, 0.12164, 0.1476, 0.1737, 0.2001, 0.2268, 0.254, 0.2815, 0.3088, 0.3364, 0.364, 0.392, 0.4211, 0.4507, 0.4805, 0.5117, 0.543, 0.573, 0.6035, 0.6343, 0.665, 0.6973, 0.7295, 0.7603, 0.792, 0.8276, 0.8633, 0.899, 0.9355, 0.9736, 1.015, 1.055, 1.094, 1.131, 1.168, 1.209, 1.245, 1.281, 1.322, 1.362, 1.404, 1.45, 1.487, 1.52, 1.562, 1.61, 1.663, 1.718, 1.768, 1.816, 1.869, 1.925, 1.986, 2.055, 2.121, 2.182, 2.238, 2.293, 2.346, 2.402, 2.465, 2.512, 2.57, 2.64, 2.707, 2.777, 2.85, 2.932, 2.986, 3.012, 3.068, 3.156, 3.246, 3.34, 3.44, 3.53, 3.621, 3.719, 3.826, 3.943, 4.066, 4.2, 4.316, 4.43, 4.547, 4.66, 4.78, 4.914, 5.023, 5.145, 5.29, 5.434, 5.598, 5.777, 5.934, 6.023, 6.152, 6.35, 6.555, 6.77, 6.992, 7.22, 7.457, 7.707, 8.016, 8.35, 8.67, 9.0, 9.32, 9.7, 10.02, 10.32, 10.664, 11.02, 11.45, 11.88, 12.34, 12.85, 13.43, 14.08, 14.65, 15.55, 16.98, 18.94, 21.98, 25.52]\n",
            "bin_edges =  [-1.5254e+00 -1.4297e+00 -1.3643e+00 -1.3105e+00 -1.2607e+00 -1.2266e+00\n",
            " -1.1865e+00 -1.1553e+00 -1.1240e+00 -1.0938e+00 -1.0615e+00 -1.0391e+00\n",
            " -1.0010e+00 -9.6973e-01 -9.4141e-01 -9.1504e-01 -8.8965e-01 -8.6963e-01\n",
            " -8.4375e-01 -8.1641e-01 -7.9199e-01 -7.6855e-01 -7.5049e-01 -7.4072e-01\n",
            " -7.1533e-01 -6.9336e-01 -6.7188e-01 -6.5088e-01 -6.2842e-01 -6.1035e-01\n",
            " -5.8789e-01 -5.6689e-01 -5.4590e-01 -5.2490e-01 -4.9976e-01 -4.7656e-01\n",
            " -4.5410e-01 -4.3237e-01 -4.0942e-01 -3.8696e-01 -3.6597e-01 -3.4375e-01\n",
            " -3.2129e-01 -2.9883e-01 -2.7637e-01 -2.5317e-01 -2.3010e-01 -2.0703e-01\n",
            " -1.8384e-01 -1.6040e-01 -1.3696e-01 -1.1322e-01 -8.9233e-02 -6.5186e-02\n",
            " -4.0985e-02 -1.6495e-02  8.1177e-03  3.2990e-02  5.8136e-02  8.3313e-02\n",
            "  1.0883e-01  1.3452e-01  1.6064e-01  1.8677e-01  2.1338e-01  2.4023e-01\n",
            "  2.6758e-01  2.9517e-01  3.2251e-01  3.5010e-01  3.7769e-01  4.0625e-01\n",
            "  4.3579e-01  4.6533e-01  4.9561e-01  5.2734e-01  5.5811e-01  5.8789e-01\n",
            "  6.1914e-01  6.4893e-01  6.8066e-01  7.1338e-01  7.4561e-01  7.7490e-01\n",
            "  8.0908e-01  8.4570e-01  8.8086e-01  9.1650e-01  9.5410e-01  9.9316e-01\n",
            "  1.0352e+00  1.0742e+00  1.1133e+00  1.1484e+00  1.1865e+00  1.2305e+00\n",
            "  1.2598e+00  1.3027e+00  1.3418e+00  1.3828e+00  1.4258e+00  1.4736e+00\n",
            "  1.5010e+00  1.5371e+00  1.5859e+00  1.6338e+00  1.6914e+00  1.7441e+00\n",
            "  1.7900e+00  1.8418e+00  1.8955e+00  1.9541e+00  2.0176e+00  2.0898e+00\n",
            "  2.1504e+00  2.2129e+00  2.2617e+00  2.3223e+00  2.3691e+00  2.4355e+00\n",
            "  2.4941e+00  2.5293e+00  2.6094e+00  2.6699e+00  2.7422e+00  2.8125e+00\n",
            "  2.8867e+00  2.9746e+00  2.9980e+00  3.0254e+00  3.1113e+00  3.2012e+00\n",
            "  3.2891e+00  3.3906e+00  3.4883e+00  3.5703e+00  3.6699e+00  3.7695e+00\n",
            "  3.8828e+00  4.0039e+00  4.1289e+00  4.2656e+00  4.3672e+00  4.4922e+00\n",
            "  4.5977e+00  4.7188e+00  4.8438e+00  4.9805e+00  5.0664e+00  5.2227e+00\n",
            "  5.3555e+00  5.5078e+00  5.6875e+00  5.8672e+00  5.9961e+00  6.0508e+00\n",
            "  6.2539e+00  6.4531e+00  6.6562e+00  6.8867e+00  7.0977e+00  7.3398e+00\n",
            "  7.5781e+00  7.8398e+00  8.2031e+00  8.5000e+00  8.8516e+00  9.1406e+00\n",
            "  9.4922e+00  9.9141e+00  1.0133e+01  1.0500e+01  1.0836e+01  1.1219e+01\n",
            "  1.1688e+01  1.2086e+01  1.2594e+01  1.3094e+01  1.3828e+01  1.4344e+01\n",
            "  1.5008e+01  1.6188e+01  1.7938e+01  2.0422e+01  2.3906e+01  2.8125e+01]\n",
            "num_points =  [271, 812, 1471, 2263, 3195, 3041, 4495, 4404, 5203, 5981, 7289, 5756, 11046, 9971, 10244, 10211, 10342, 8646, 11886, 13267, 12182, 12380, 9769, 5670, 14357, 12730, 12460, 12684, 13885, 11181, 14141, 13249, 13307, 13664, 15917, 15110, 14481, 13835, 14909, 14013, 13358, 14079, 13994, 13801, 13538, 13761, 13763, 13579, 13192, 13094, 12963, 12915, 12589, 12438, 12340, 12121, 11966, 11613, 11492, 11336, 11108, 10846, 10713, 10569, 10169, 10105, 9781, 9462, 9268, 9134, 8708, 8788, 8531, 8161, 8118, 8100, 7584, 7143, 7218, 6607, 6607, 6574, 6309, 5433, 6134, 6060, 5581, 5465, 5396, 5511, 5525, 4875, 4517, 3881, 3969, 4185, 2843, 3906, 3448, 3438, 3428, 3608, 1836, 2426, 3045, 2971, 3277, 2710, 2241, 2450, 2329, 2508, 2417, 2384, 1968, 1887, 1404, 1648, 1166, 1591, 1281, 755, 1672, 1122, 1247, 1111, 1093, 1157, 327, 335, 1041, 932, 888, 899, 821, 613, 679, 633, 698, 630, 612, 578, 433, 453, 334, 396, 362, 319, 189, 347, 249, 263, 256, 243, 163, 81, 235, 176, 184, 170, 128, 135, 113, 119, 113, 78, 78, 47, 72, 72, 33, 42, 41, 37, 31, 27, 25, 16, 18, 18, 15, 15, 11, 11, 11]\n",
            "total_variance_bin = [6.034722222222484e-07, 6.037926338752217e-07, 6.026921895084665e-07, 6.030420523426661e-07, 6.026280423443136e-07, 6.02765409569962e-07, 6.030080037262965e-07, 6.024719201888109e-07, 6.024684642020305e-07, 6.025197823304485e-07, 6.024844998452297e-07, 6.028074660583517e-07, 6.024977221640052e-07, 6.025769338921797e-07, 6.025554089185299e-07, 6.024459681007707e-07, 6.024201141071686e-07, 6.024789592864705e-07, 6.024454003670412e-07, 6.02410767137474e-07, 6.026904468662018e-07, 6.025088458196514e-07, 6.025167862761575e-07, 6.02436068869729e-07, 6.025078384839959e-07, 6.024745315118114e-07, 6.025230376285206e-07, 6.024401722525041e-07, 6.024653394193535e-07, 6.024727178174167e-07, 6.02465785779934e-07, 6.024295117538077e-07, 6.025652206527228e-07, 6.02447315426007e-07, 6.024127419038412e-07, 6.024819973422609e-07, 6.024212625763812e-07, 6.024559804673334e-07, 6.024306350617764e-07, 6.025476557059658e-07, 6.024539747479914e-07, 6.024485290819205e-07, 6.024112999885257e-07, 6.02516614112436e-07, 6.024658261976472e-07, 6.024471909327052e-07, 6.025198894850208e-07, 6.024467649741904e-07, 6.024114039719888e-07, 6.024912937350136e-07, 6.024760724474858e-07, 6.025452468891727e-07, 6.024112270338019e-07, 6.025391165968201e-07, 6.025364172582759e-07, 6.025412814766781e-07, 6.024163772662986e-07, 6.025134535029342e-07, 6.025213595820438e-07, 6.024617268424955e-07, 6.025247408725019e-07, 6.024407886906178e-07, 6.025668371946935e-07, 6.024185627315195e-07, 6.025596095818951e-07, 6.025354746340434e-07, 6.024222791072742e-07, 6.025134475535365e-07, 6.025483570361769e-07, 6.024676469791535e-07, 6.026040101563247e-07, 6.025035363070746e-07, 6.025467002166138e-07, 6.026315715309877e-07, 6.024589701886389e-07, 6.024967291757212e-07, 6.024182160986676e-07, 6.025221052032846e-07, 6.024273416083975e-07, 6.025298476541876e-07, 6.026497406560477e-07, 6.025344148430442e-07, 6.024837608417487e-07, 6.025267654648856e-07, 6.024578423126104e-07, 6.024983464379067e-07, 6.024731598450663e-07, 6.024811297530201e-07, 6.024826458150067e-07, 6.026877179991901e-07, 6.024246419105942e-07, 6.025374155823492e-07, 6.026156958048587e-07, 6.028153446539782e-07, 6.027760265333649e-07, 6.025136540537774e-07, 6.025202010906858e-07, 6.028460943443592e-07, 6.025610498529026e-07, 6.024347149365451e-07, 6.030465271575531e-07, 6.031723667592424e-07, 6.035347208347253e-07, 6.030617810278929e-07, 6.034348518558334e-07, 6.029741585860608e-07, 6.032142022015424e-07, 6.02920649102748e-07, 6.034113505066188e-07, 6.02741356722175e-07, 6.025530722540902e-07, 6.0283092374015e-07, 6.026325288995171e-07, 6.033354890094341e-07, 6.037806174913595e-07, 6.027324267937544e-07, 6.028649648952191e-07, 6.030889996330641e-07, 6.02532723227438e-07, 6.028878625826728e-07, 6.030985564007659e-07, 6.037195342296531e-07, 6.036062075077296e-07, 6.026396772671608e-07, 6.03026547743977e-07, 6.028508229322741e-07, 6.051489162795348e-07, 6.055696480838013e-07, 6.063276302594521e-07, 6.050999692816965e-07, 6.032694743219018e-07, 6.060842766025303e-07, 6.02874859076323e-07, 6.085254652218407e-07, 6.086483653014001e-07, 6.05403326695459e-07, 6.034162403198494e-07, 6.049242381788495e-07, 6.029372742051005e-07, 6.040861860567052e-07, 6.047263137653166e-07, 6.136441817532116e-07, 6.098163392809418e-07, 6.092318644550083e-07, 6.068776906780807e-07, 6.04614837258694e-07, 6.069836391986107e-07, 6.107722828217722e-07, 6.045167801231071e-07, 6.096691947449119e-07, 6.09623078376213e-07, 6.161093093048454e-07, 6.070503384459253e-07, 6.101677003971644e-07, 6.205749511733009e-07, 6.08798027038415e-07, 6.226629273482508e-07, 6.056939697269782e-07, 6.131043981345574e-07, 6.26641087278128e-07, 6.086709330401082e-07, 6.128921508795442e-07, 6.068551199770532e-07, 6.125276209951014e-07, 6.272621154787186e-07, 6.20928875813082e-07, 6.393749682944649e-07, 6.079937478761978e-07, 6.194973797865316e-07, 6.082712092860495e-07, 6.231002807617472e-07, 6.116958246002025e-07, 6.07078552245207e-07, 6.432545979810886e-07, 6.41536458333106e-07, 6.860328087443052e-07, 6.200256347663451e-07, 6.848307291666666e-07, 7.484920726102941e-07, 6.029411764709628e-07, 6.695731026788313e-07, 9.957406180241637e-07, 3.875683593750091e-06, 1.0643334960936773e-05, 2.274677734374982e-05]\n",
            "##########################################\n",
            "Standard Deviation of Quantization Error: 0.01101 0.01198 0.01217342883619799\n",
            "##########################################\n",
            "tf.Tensor(\n",
            "[-0.682  -0.704  -0.8037 -0.8564 -0.7593 -0.682  -0.488  -0.465  -0.5986\n",
            " -0.7593], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[-0.685  -0.7046 -0.7974 -0.851  -0.763  -0.673  -0.4883 -0.4543 -0.608\n",
            " -0.762 ], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[ 0.00293    0.0004883 -0.006348  -0.00537    0.003906  -0.00928\n",
            "  0.0002441 -0.01074    0.00928    0.00293  ], shape=(10,), dtype=float16)\n",
            "[271, 812, 1471, 2263, 3195, 3041, 4495, 4404, 5203, 5981, 7289, 5756, 11046, 9971, 10244, 10211, 10342, 8646, 11886, 13267, 12182, 12380, 9769, 5670, 14357, 12730, 12460, 12684, 13885, 11181, 14141, 13249, 13307, 13664, 15917, 15110, 14481, 13835, 14909, 14013, 13358, 14079, 13994, 13801, 13538, 13761, 13763, 13579, 13192, 13094, 12963, 12915, 12589, 12438, 12340, 12121, 11966, 11613, 11492, 11336, 11108, 10846, 10713, 10569, 10169, 10105, 9781, 9462, 9268, 9134, 8708, 8788, 8531, 8161, 8118, 8100, 7584, 7143, 7218, 6607, 6607, 6574, 6309, 5433, 6134, 6060, 5581, 5465, 5396, 5511, 5525, 4875, 4517, 3881, 3969, 4185, 2843, 3906, 3448, 3438, 3428, 3608, 1836, 2426, 3045, 2971, 3277, 2710, 2241, 2450, 2329, 2508, 2417, 2384, 1968, 1887, 1404, 1648, 1166, 1591, 1281, 755, 1672, 1122, 1247, 1111, 1093, 1157, 327, 335, 1041, 932, 888, 899, 821, 613, 679, 633, 698, 630, 612, 578, 433, 453, 334, 396, 362, 319, 189, 347, 249, 263, 256, 243, 163, 81, 235, 176, 184, 170, 128, 135, 113, 119, 113, 78, 78, 47, 72, 72, 33, 42, 41, 37, 31, 27, 25, 16, 18, 18, 15, 15, 11, 11, 11] 999997\n",
            "[6.034722222222484e-07, 6.037926338752217e-07, 6.026921895084665e-07, 6.030420523426661e-07, 6.026280423443136e-07, 6.02765409569962e-07, 6.030080037262965e-07, 6.024719201888109e-07, 6.024684642020305e-07, 6.025197823304485e-07, 6.024844998452297e-07, 6.028074660583517e-07, 6.024977221640052e-07, 6.025769338921797e-07, 6.025554089185299e-07, 6.024459681007707e-07, 6.024201141071686e-07, 6.024789592864705e-07, 6.024454003670412e-07, 6.02410767137474e-07, 6.026904468662018e-07, 6.025088458196514e-07, 6.025167862761575e-07, 6.02436068869729e-07, 6.025078384839959e-07, 6.024745315118114e-07, 6.025230376285206e-07, 6.024401722525041e-07, 6.024653394193535e-07, 6.024727178174167e-07, 6.02465785779934e-07, 6.024295117538077e-07, 6.025652206527228e-07, 6.02447315426007e-07, 6.024127419038412e-07, 6.024819973422609e-07, 6.024212625763812e-07, 6.024559804673334e-07, 6.024306350617764e-07, 6.025476557059658e-07, 6.024539747479914e-07, 6.024485290819205e-07, 6.024112999885257e-07, 6.02516614112436e-07, 6.024658261976472e-07, 6.024471909327052e-07, 6.025198894850208e-07, 6.024467649741904e-07, 6.024114039719888e-07, 6.024912937350136e-07, 6.024760724474858e-07, 6.025452468891727e-07, 6.024112270338019e-07, 6.025391165968201e-07, 6.025364172582759e-07, 6.025412814766781e-07, 6.024163772662986e-07, 6.025134535029342e-07, 6.025213595820438e-07, 6.024617268424955e-07, 6.025247408725019e-07, 6.024407886906178e-07, 6.025668371946935e-07, 6.024185627315195e-07, 6.025596095818951e-07, 6.025354746340434e-07, 6.024222791072742e-07, 6.025134475535365e-07, 6.025483570361769e-07, 6.024676469791535e-07, 6.026040101563247e-07, 6.025035363070746e-07, 6.025467002166138e-07, 6.026315715309877e-07, 6.024589701886389e-07, 6.024967291757212e-07, 6.024182160986676e-07, 6.025221052032846e-07, 6.024273416083975e-07, 6.025298476541876e-07, 6.026497406560477e-07, 6.025344148430442e-07, 6.024837608417487e-07, 6.025267654648856e-07, 6.024578423126104e-07, 6.024983464379067e-07, 6.024731598450663e-07, 6.024811297530201e-07, 6.024826458150067e-07, 6.026877179991901e-07, 6.024246419105942e-07, 6.025374155823492e-07, 6.026156958048587e-07, 6.028153446539782e-07, 6.027760265333649e-07, 6.025136540537774e-07, 6.025202010906858e-07, 6.028460943443592e-07, 6.025610498529026e-07, 6.024347149365451e-07, 6.030465271575531e-07, 6.031723667592424e-07, 6.035347208347253e-07, 6.030617810278929e-07, 6.034348518558334e-07, 6.029741585860608e-07, 6.032142022015424e-07, 6.02920649102748e-07, 6.034113505066188e-07, 6.02741356722175e-07, 6.025530722540902e-07, 6.0283092374015e-07, 6.026325288995171e-07, 6.033354890094341e-07, 6.037806174913595e-07, 6.027324267937544e-07, 6.028649648952191e-07, 6.030889996330641e-07, 6.02532723227438e-07, 6.028878625826728e-07, 6.030985564007659e-07, 6.037195342296531e-07, 6.036062075077296e-07, 6.026396772671608e-07, 6.03026547743977e-07, 6.028508229322741e-07, 6.051489162795348e-07, 6.055696480838013e-07, 6.063276302594521e-07, 6.050999692816965e-07, 6.032694743219018e-07, 6.060842766025303e-07, 6.02874859076323e-07, 6.085254652218407e-07, 6.086483653014001e-07, 6.05403326695459e-07, 6.034162403198494e-07, 6.049242381788495e-07, 6.029372742051005e-07, 6.040861860567052e-07, 6.047263137653166e-07, 6.136441817532116e-07, 6.098163392809418e-07, 6.092318644550083e-07, 6.068776906780807e-07, 6.04614837258694e-07, 6.069836391986107e-07, 6.107722828217722e-07, 6.045167801231071e-07, 6.096691947449119e-07, 6.09623078376213e-07, 6.161093093048454e-07, 6.070503384459253e-07, 6.101677003971644e-07, 6.205749511733009e-07, 6.08798027038415e-07, 6.226629273482508e-07, 6.056939697269782e-07, 6.131043981345574e-07, 6.26641087278128e-07, 6.086709330401082e-07, 6.128921508795442e-07, 6.068551199770532e-07, 6.125276209951014e-07, 6.272621154787186e-07, 6.20928875813082e-07, 6.393749682944649e-07, 6.079937478761978e-07, 6.194973797865316e-07, 6.082712092860495e-07, 6.231002807617472e-07, 6.116958246002025e-07, 6.07078552245207e-07, 6.432545979810886e-07, 6.41536458333106e-07, 6.860328087443052e-07, 6.200256347663451e-07, 6.848307291666666e-07, 7.484920726102941e-07, 6.029411764709628e-07, 6.695731026788313e-07, 9.957406180241637e-07, 3.875683593750091e-06, 1.0643334960936773e-05, 2.274677734374982e-05] 0.00014819236962997675\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Ensure WL_tensor_flat is a NumPy array\n",
        "if isinstance(WL_tensor_flat, tf.Tensor):\n",
        "    WL_tensor_flat_np = WL_tensor_flat.numpy()\n",
        "else:\n",
        "    WL_tensor_flat_np = WL_tensor_flat\n",
        "\n",
        "# Step 1: Sample a Subset of the Data\n",
        "sample_size = 1000000  # Adjust based on available memory and desired accuracy\n",
        "indices = np.random.choice(len(WL_tensor_flat_np), size=sample_size, replace=False)\n",
        "sampled_data = WL_tensor_flat_np[indices]\n",
        "\n",
        "\n",
        "# Step 2: Sort the Sampled Data\n",
        "sorted_data = np.sort(sampled_data)\n",
        "n = len(sorted_data)\n",
        "\n",
        "\n",
        "#print(\"num zeros = \", np.sum(sampled_data == 0), np.sum(WL_tensor_flat_np == 0), np.sum(sorted_data ==0))\n",
        "\n",
        "#print(\"sorted_data =\", sorted_data)\n",
        "\n",
        "# Step 3: Compute Cumulative Sums and Cumulative Sum of Squares\n",
        "cum_sum = np.cumsum(sorted_data, dtype=float)\n",
        "cum_sum_sq = np.cumsum(sorted_data**2, dtype=float)\n",
        "\n",
        "# Step 4: Compute Total Variance and Target Variance per Bin\n",
        "target_std = .01\n",
        "num_bins= 50 # just a guess to start\n",
        "\n",
        "for iiter in range(3): # iterate five times to reach target std\n",
        "\n",
        "    target_variance_per_bin = target_std**2/ num_bins\n",
        "\n",
        "    print(f\"Target Variance: {target_variance}\")\n",
        "    print(f\"Target Variance per Bin: {target_variance_per_bin}\")\n",
        "\n",
        "\n",
        "    # Step 5: Find Bin Edges\n",
        "    bin_edges = []\n",
        "    num_points_arr = []\n",
        "    total_variance_bin_arr = []\n",
        "    mean_bin_arr = []\n",
        "\n",
        "    start_index = 0\n",
        "    while start_index < n:\n",
        "        found = False\n",
        "        for end_index in range(start_index + 10, n):\n",
        "            num_points = end_index - start_index + 1\n",
        "            #if num_points <= 3:\n",
        "            #    continue\n",
        "            # Sum and sum of squares in the bin\n",
        "            sum_bin = cum_sum[end_index] - (cum_sum[start_index - 1] if start_index > 0 else 0)\n",
        "            sum_sq_bin = cum_sum_sq[end_index] - (cum_sum_sq[start_index - 1] if start_index > 0 else 0)\n",
        "            # Mean and variance in the bin\n",
        "            mean_bin = sum_bin / num_points\n",
        "            total_variance_bin = (sum_sq_bin - num_points*mean_bin**2)/(num_points-1)*num_points/n  #total contribute to variance\n",
        "            if total_variance_bin >= target_variance_per_bin:\n",
        "                bin_edges.append(sorted_data[end_index])\n",
        "                num_points_arr.append(num_points)\n",
        "                total_variance_bin_arr.append(total_variance_bin)\n",
        "                mean_bin_arr.append(mean_bin)\n",
        "                #print(\"sums = \",  sum_bin , sum_sq_bin, mean_bin, sum_sq_bin / num_points, num_points, \"total_variance_bin = \", total_variance_bin, target_variance_per_bin, num_points/n, ((sum_sq_bin / num_points) - mean_bin**2) )\n",
        "                #print(\"indexes =\", start_index, end_index, cum_sum[end_index], cum_sum_sq[start_index - 1]) \n",
        "                start_index = end_index + 1\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # Include all remaining data in the last bin\n",
        "            bin_edges.append(tf.reduce_max(WL_tensor)) #       sorted_data[-1])\n",
        "            break\n",
        "\n",
        "    num_bins = len(bin_edges)\n",
        "    print(\"num bins=\", num_bins)\n",
        "\n",
        "\n",
        "\n",
        "# Step 6: Digitize the Original Data Using Bin Edges\n",
        "# Convert bin_edges to numpy array for consistency\n",
        "bin_edges = np.array(bin_edges)\n",
        "\n",
        "# Use np.digitize to find bin indices for the entire dataset\n",
        "bin_indices = np.digitize(WL_tensor_flat_np, bin_edges, right=True)-1\n",
        "\n",
        "# Adjust bin_indices to ensure they are within the valid range\n",
        "# Since mean_values has length len(bin_edges) + 1, bin_indices should be in [0, len(bin_edges)]\n",
        "#may be necessary since our bins are not based on everything\n",
        "bin_indices = np.clip(bin_indices, 0, len(bin_edges))\n",
        "\n",
        "\n",
        "# Digitize the tensor values according to the binning scheme\n",
        "#bin_indices = tf.searchsorted(bin_edges, WL_tensor_flat, side='right') - 1\n",
        "\n",
        "\n",
        "#let's calculate the mean we expect in each bin\n",
        "#MIGHT NEED TO GAURD AGAINST ZEROS\n",
        "mean_values = [np.mean(WL_tensor_flat[bin_indices == index]) for index in range(len(bin_edges)-1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"mean values = \", mean_values)\n",
        "print(\"bin_edges = \", bin_edges)\n",
        "print(\"num_points = \", num_points_arr)\n",
        "print(\"total_variance_bin =\", total_variance_bin_arr)\n",
        "\n",
        "encoded_tensor = tf.gather(mean_values, bin_indices) #realization of quantized tensor\n",
        "\n",
        "\n",
        "#reshape\n",
        "bin_indices = tf.reshape(bin_indices, [number_images,  sub_image_size*sub_image_size])\n",
        "\n",
        "\n",
        "# Compute mean values\n",
        "# mean_values = np.zeros(len(bin_edges) + 1)\n",
        "# for i in range(len(mean_values)):\n",
        "#     indices_in_bin = bin_indices == i\n",
        "#     if np.any(indices_in_bin):\n",
        "#         mean_values[i] = np.mean(WL_tensor_flat_np[indices_in_bin])\n",
        "#     else:\n",
        "#         # Handle empty bins if necessary\n",
        "#         mean_values[i] = 0\n",
        "\n",
        "# # Map bin indices to mean values\n",
        "# encoded_tensor_np = mean_values[bin_indices]\n",
        "\n",
        "# # If you need to convert back to a TensorFlow tensor\n",
        "# encoded_tensor = tf.convert_to_tensor(encoded_tensor_np, dtype=WL_tensor_flat.dtype)\n",
        "\n",
        "# # Compute Quantization Error\n",
        "# \n",
        "# \n",
        "diff_tensor = encoded_tensor - WL_tensor_flat\n",
        "\n",
        "# Compute the standard deviation of the quantization error\n",
        "std_quantized = tf.math.reduce_std(diff_tensor).numpy()\n",
        "\n",
        "# Convert indices to a TensorFlow tensor\n",
        "indices_tf = tf.constant(indices, dtype=tf.int32)\n",
        "# Use tf.gather to index diff_tensor\n",
        "std_quantized_sampled = tf.math.reduce_std(tf.gather(diff_tensor, indices_tf)).numpy()\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Standard Deviation of Quantization Error:\", std_quantized, std_quantized_sampled, np.sum(total_variance_bin_arr)**.5)\n",
        "print(\"##########################################\")\n",
        "# If needed, reshape encoded_tensor back to the original tensor shape\n",
        "# encoded_tensor = tf.reshape(encoded_tensor, WL_tensor.shape)\n",
        "\n",
        "#just to check quantization works\n",
        "print(encoded_tensor[-10:])\n",
        "print(WL_tensor_flat[-10:])\n",
        "\n",
        "\n",
        "print(diff_tensor[-10:])\n",
        "\n",
        "\n",
        "print(num_points_arr, np.sum(num_points_arr))\n",
        "print(total_variance_bin_arr, np.sum(total_variance_bin_arr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5f23c29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices) =  (16384, 32, 32, 1) (16654944,) (16384, 1024) <dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "print(\"np.shape(WL_tensor), np.shape(bin_indices) = \", np.shape(WL_tensor), np.shape(bin_indices), bin_indices.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684c8c58",
      "metadata": {},
      "source": [
        "# Autoregressive image transformer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab491d",
      "metadata": {},
      "source": [
        "## Simple autoencoder with sines and cosines sampling to Nyquist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "98c92482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">11,904</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " not_equal            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)                                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n",
              "                                                     not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              "\n",
              " add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_10           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_11           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,090</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m11,904\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda (\u001b[38;5;33mLambda\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " not_equal            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mNotEqual\u001b[0m)                                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n",
              "                                                     not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_7 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              "\n",
              " add_4 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_8 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_5 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_10           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_6 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_11           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_7 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_7 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m)      \u001b[38;5;34m12,090\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,962</span> (484.23 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m123,962\u001b[0m (484.23 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">123,962</span> (484.23 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m123,962\u001b[0m (484.23 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">11,904</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " not_equal_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)                                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n",
              "                                                     not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "                                                     not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " dropout_13           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_14           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_16           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_17           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_20           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_22           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_23           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">12,090</span>  add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m11,904\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_1 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " not_equal_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mNotEqual\u001b[0m)                                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n",
              "                                                     not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m \n",
              "                                                     not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " dropout_13           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_8 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_9 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_14           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_9 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_16           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_10 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_10 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_17           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_11 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_12 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_12 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_13 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_20           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_13 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_22           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_14 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_14 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_15 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_23           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_15 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_16 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186\u001b[0m)      \u001b[38;5;34m12,090\u001b[0m  add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,930</span> (874.73 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m223,930\u001b[0m (874.73 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,930</span> (874.73 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m223,930\u001b[0m (874.73 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  #don't think need L1 and dropout  (I've tested and it performs best with this equal to zero)\n",
        "num_classes = num_bins  # Set num_classes to num_bins: this is the number of values that can be populated\n",
        "d_model = number_channels*2  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.d_model = d_model\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    #inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)  # Accept variable-length sequences\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(x)  # Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Create the causal mask once as a constant tensor\n",
        "    #causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    #causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    def create_causal_mask(x):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        return causal_mask\n",
        "\n",
        "    causal_mask = layers.Lambda(create_causal_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_length = sub_image_size * sub_image_size\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_deep = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers*2,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "\n",
        "autoregressive_transformer.summary()\n",
        "\n",
        "autoregressive_transformer_deep.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6de246",
      "metadata": {},
      "source": [
        "## Compile models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "2cb4fc3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_deep.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1a6b2",
      "metadata": {},
      "source": [
        "### Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "650102df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 536ms/step - accuracy: 0.0439 - loss: 3.9466 - val_accuracy: 0.0735 - val_loss: 3.2685\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 564ms/step - accuracy: 0.0747 - loss: 3.2307 - val_accuracy: 0.0906 - val_loss: 3.0423\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 568ms/step - accuracy: 0.0876 - loss: 3.0709"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b344ac9",
      "metadata": {},
      "source": [
        "### Let's fit the deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "49bec05f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.0871 - loss: 3.2729 - val_accuracy: 0.1239 - val_loss: 2.7536\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1021s\u001b[0m 1s/step - accuracy: 0.1277 - loss: 2.7058 - val_accuracy: 0.1580 - val_loss: 2.4666\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 1s/step - accuracy: 0.1560 - loss: 2.4744 - val_accuracy: 0.1778 - val_loss: 2.3402\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1109s\u001b[0m 1s/step - accuracy: 0.1720 - loss: 2.3700 - val_accuracy: 0.1864 - val_loss: 2.2889\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 1s/step - accuracy: 0.1809 - loss: 2.3185 - val_accuracy: 0.1920 - val_loss: 2.2581\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 471ms/step - accuracy: 0.1798 - loss: 2.3376\n",
            "Validation Loss: 2.3372  bits per pixel : 3.3719\n",
            "Validation Accuracy: 0.1790\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_deep.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a3d9481",
      "metadata": {},
      "source": [
        "# Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba17c87",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[8], line 169\u001b[0m\n",
            "\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Run the generalized code\u001b[39;00m\n",
            "\u001b[1;32m    168\u001b[0m num_samples_to_visualize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Change this to the number of random samples you want to visualize\u001b[39;00m\n",
            "\u001b[0;32m--> 169\u001b[0m process_random_images(\u001b[43mX_val\u001b[49m, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Much slower code that I don't use anymore\n",
        "def autoregressive_predict(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model, one step at a time,\n",
        "    using the original sequence up to the current position as context.\n",
        "    \"\"\"\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for i in range(1, sequence_length+1):\n",
        "        # Prepare the context (all previous pixels)\n",
        "        context = original_sequence[:i]\n",
        "\n",
        "        # No padding required; model expects input of length seq_length\n",
        "        # For positions where context is shorter, we need to pad or adjust the input\n",
        "        # Since we're avoiding padding, we'll adjust the input sequence accordingly\n",
        "\n",
        "        # Create a context of length 'i'\n",
        "        model_input = np.array(context, dtype=np.int32)[np.newaxis, :]\n",
        "\n",
        "        # Since the model expects input of shape (batch_size, seq_length),\n",
        "        # we need to handle the varying lengths\n",
        "        # One way is to slice the model to accept variable input lengths\n",
        "\n",
        "        # Predict\n",
        "        next_pixel_probs = model.predict(model_input, verbose=0)\n",
        "        # Get the prediction for the current position\n",
        "        next_pixel = np.argmax(next_pixel_probs[0, -1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Step {i}: Context length: {len(context)}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "#should be much faster....need to check\n",
        "def autoregressive_predict_batched(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model by batching inputs,\n",
        "    which significantly speeds up the prediction process.\n",
        "    \"\"\"\n",
        "    # Prepare all contexts at once\n",
        "    contexts = [original_sequence[:i] for i in range(1, sequence_length + 1)]\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = sequence_length\n",
        "    padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        contexts, maxlen=max_len, padding='post', value=0\n",
        "    )\n",
        "\n",
        "    # Convert to array\n",
        "    model_inputs = np.array(padded_contexts, dtype=np.int32)\n",
        "\n",
        "    # Predict all at once\n",
        "    next_pixel_probs = model.predict(model_inputs, verbose=0)\n",
        "\n",
        "    # Extract the predictions\n",
        "    predicted_sequence = []\n",
        "    for i, context in enumerate(contexts):\n",
        "        # Get the prediction for the current position\n",
        "        seq_length = len(context)\n",
        "        next_pixel = np.argmax(next_pixel_probs[i, seq_length - 1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1}: Context length: {seq_length}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "def process_random_images(X_val, y_val, model, sub_image_size, num_samples=5):\n",
        "    \"\"\"\n",
        "    Process and visualize random images from the validation set.\n",
        "    \"\"\"\n",
        "    total_samples = len(X_val)\n",
        "    random_indices = np.random.choice(total_samples, num_samples, replace=False)\n",
        "    \n",
        "    \n",
        "    for i, idx in enumerate(random_indices):\n",
        "        val_sample = X_val[idx]\n",
        "        original_sequence = y_val[idx]\n",
        "        \n",
        "        # Determine the sequence length\n",
        "        sequence_length = len(original_sequence)\n",
        "        \n",
        "        # Predict using the original sequence as context\n",
        "        predicted_sequence = autoregressive_predict_batched(\n",
        "            model,\n",
        "            original_sequence,\n",
        "            sequence_length,\n",
        "        )\n",
        "        \n",
        "        print(\"val_sample and original_sequence shapes\", np.shape(val_sample), np.shape(original_sequence), np.shape(predicted_sequence))\n",
        "    \n",
        "        visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, i+1)\n",
        "        \n",
        "        mse = np.mean((original_sequence - predicted_sequence)**2)\n",
        "        mae = np.mean(np.abs(original_sequence - predicted_sequence))\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.4f}\")\n",
        "        print(f\"  Original sequence shape: {original_sequence.shape}\")\n",
        "        print(f\"  Predicted sequence shape: {predicted_sequence.shape}\")\n",
        "        print(f\" fraction of pixles correct\", np.sum(original_sequence - predicted_sequence == 0)/len(original_sequence))\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# The visualize_autoregressive_prediction function remains the same\n",
        "\n",
        "def visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, predicted, and difference images with equal sizes.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "    fig.suptitle(f'Sample {index}', fontsize=16)\n",
        "\n",
        "    # Prepare images\n",
        "    original_2d = sequence_to_image(original_sequence, sub_image_size, original=True)\n",
        "    predicted_2d = sequence_to_image(predicted_sequence, sub_image_size, original=False)\n",
        "    diff = original_2d - predicted_2d\n",
        "\n",
        "    # Set up common parameters for imshow\n",
        "    imshow_args = {'interpolation': 'nearest', 'aspect': 'equal'}\n",
        "\n",
        "    # Visualize original sequence\n",
        "    im1 = axes[0].imshow(original_2d, cmap='gray', **imshow_args)\n",
        "    axes[0].set_title('Original Image')\n",
        "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize predicted sequence\n",
        "    im2 = axes[1].imshow(predicted_2d, cmap='gray', **imshow_args)\n",
        "    axes[1].set_title('Predicted Image (Autoregressive)')\n",
        "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize difference\n",
        "    im3 = axes[2].imshow(diff, cmap='bwr', **imshow_args)\n",
        "    axes[2].set_title('Difference (Original - Predicted)')\n",
        "    plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sequence_to_image(sequence, sub_image_size, original=True):\n",
        "    \"\"\"\n",
        "    Convert a 1D sequence of length 1023 to a 2D image of size sub_image_size x sub_image_size,\n",
        "    by prepending the first_token to the sequence.\n",
        "    \"\"\"\n",
        "    # Concatenate the first token to the sequence\n",
        "    if original:\n",
        "        full_sequence = np.concatenate((sequence, [0]))\n",
        "    else:\n",
        "        full_sequence = np.concatenate(([0], sequence))\n",
        "    # Reshape to image\n",
        "    image = full_sequence.reshape(sub_image_size, sub_image_size)\n",
        "    return image\n",
        "\n",
        "# Run the generalized code\n",
        "num_samples_to_visualize = 3  # Change this to the number of random samples you want to visualize\n",
        "process_random_images(X_val, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc300667",
      "metadata": {},
      "source": [
        "### Plot probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d23252",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'autoregressive_transformer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[0;32mIn[85], line 29\u001b[0m\n",
            "\u001b[1;32m     26\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Plot predictions for some sequences\u001b[39;00m\n",
            "\u001b[0;32m---> 29\u001b[0m plot_predictions(\u001b[43mautoregressive_transformer\u001b[49m, train_sequences, train_targets, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
            "\n",
            "\u001b[0;31mNameError\u001b[0m: name 'autoregressive_transformer' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot predictions vs actual values\n",
        "def plot_predictions(model, sequences, targets, num_samples=10):\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(sequences[:num_samples])\n",
        "    predicted_pixels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Plot the actual vs predicted pixel values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(sequences[i].reshape(-1, 1), cmap='gray', aspect='auto')\n",
        "        plt.title(f\"Sequence {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(num_classes), predictions[i])\n",
        "        plt.axvline(x=targets[i], color='r', linestyle='--')\n",
        "        plt.axvline(x=predicted_pixels[i], color='g', linestyle='--')\n",
        "        plt.title(f\"True: {targets[i]}, Pred: {predicted_pixels[i]}\")\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions for some sequences\n",
        "plot_predictions(autoregressive_transformer, train_sequences, train_targets, num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489870fe",
      "metadata": {},
      "source": [
        "## This has a free positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6cb767fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'positional_embedding' (of type PositionalEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_3        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_3          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">32,736</span>  embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  positional_embed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "\n",
              " dropout_25           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_embed \n",
              "                                                     dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_26           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "\n",
              " dropout_28           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_29           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_3        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_3          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,096\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)       \u001b[38;5;34m32,736\u001b[0m  embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEmbeddi\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  positional_embed \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "\n",
              " dropout_25           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_16 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_embed \n",
              "                                                     dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_17 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_18 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_26           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_17 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "\n",
              " dropout_28           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_18 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_19 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_20 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_29           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_19 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  # zero is no regularizer (at least with set positional encoding performs better without regularization)\n",
        "num_classes = num_bins+1  # Number of quantization bins\n",
        "d_model = number_channels  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Embedding Layer with learnable parameters\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, seq_length, d_model, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.d_model = d_model\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            shape=(seq_length, d_model),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='position_embeddings'\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, seq_length, d_model)\n",
        "        return inputs + self.position_embeddings\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Adjusted for autoregressive prediction\n",
        "    inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(inputs)\n",
        "\n",
        "    # Apply learnable positional embeddings\n",
        "    x = PositionalEmbedding(seq_length, d_model)(x)\n",
        "\n",
        "    # Create the causal mask\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Adjusted sequence length for autoregressive prediction\n",
        "seq_length = sub_image_size * sub_image_size - 1\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_freeposemb = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_freeposemb.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# View the model summary\n",
        "autoregressive_transformer_freeposemb.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f974a0b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 473ms/step - accuracy: 0.0892 - loss: 3.2488 - val_accuracy: 0.1038 - val_loss: 2.9402\n",
            "Epoch 2/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 474ms/step - accuracy: 0.1047 - loss: 2.9234 - val_accuracy: 0.1215 - val_loss: 2.7774\n",
            "Epoch 3/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 494ms/step - accuracy: 0.1269 - loss: 2.7261 - val_accuracy: 0.1461 - val_loss: 2.5764\n",
            "Epoch 4/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 514ms/step - accuracy: 0.1474 - loss: 2.5605 - val_accuracy: 0.1595 - val_loss: 2.4724\n",
            "Epoch 5/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 500ms/step - accuracy: 0.1589 - loss: 2.4741 - val_accuracy: 0.1677 - val_loss: 2.4133\n",
            "Epoch 6/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 493ms/step - accuracy: 0.1668 - loss: 2.4227 - val_accuracy: 0.1720 - val_loss: 2.3865\n",
            "Epoch 7/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 499ms/step - accuracy: 0.1709 - loss: 2.3979 - val_accuracy: 0.1746 - val_loss: 2.3719\n",
            "Epoch 8/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 468ms/step - accuracy: 0.1735 - loss: 2.3823 - val_accuracy: 0.1765 - val_loss: 2.3580\n",
            "Epoch 9/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 461ms/step - accuracy: 0.1751 - loss: 2.3716 - val_accuracy: 0.1785 - val_loss: 2.3528\n",
            "Epoch 10/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 464ms/step - accuracy: 0.1768 - loss: 2.3613 - val_accuracy: 0.1794 - val_loss: 2.3425\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "Predictions shape: (1, 1023, 128)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 414ms/step - accuracy: 0.1799 - loss: 2.3426\n",
            "Validation Loss: 2.3425  bits per pixel : 3.3795\n",
            "Validation Accuracy: 0.1794\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_freeposemb.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1]  # Take the first sample\n",
        "predictions = autoregressive_transformer_freeposemb.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# Reconstruct and visualize the predicted image\n",
        "#first_token = sample_input.numpy()[0, 0]\n",
        "#reconstructed_sequence = np.concatenate(([first_token], predicted_classes[0]))\n",
        "#reconstructed_image = reconstructed_sequence.reshape(sub_image_size, sub_image_size)\n",
        "\n",
        "#plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.title('Predicted Image')\n",
        "#plt.axis('off')\n",
        "#plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_freeposemb.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c0608c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
