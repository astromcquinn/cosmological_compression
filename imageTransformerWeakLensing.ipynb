{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed531a95",
      "metadata": {
        "id": "ed531a95"
      },
      "source": [
        "# Implements encoder/decoder for weak lensing outputs\n",
        "\n",
        "The major idea is to see if I can compress the data in the snapshot files.\n",
        "The result is that the compression of many different algorithms based on CNNs (of different depths) is not so much different than averaging neighboring cells (as shown at the end).  This in retrospect is not so surprising as there are differences on the cell scale in the maps that make compression challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zQpe_fjWxGwO",
      "metadata": {
        "id": "zQpe_fjWxGwO"
      },
      "source": [
        "Set configurations for google COLAB if running there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KEtElnI8fDj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtElnI8fDj1",
        "outputId": "59dde345-ea8f-426f-a56a-69affd2aaf3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "use_COLAB = 0 #1 is for on colab, and 2 is for on local machine but using colab\n",
        "\n",
        "if use_COLAB >= 1:\n",
        "  if use_COLAB == 2: # for running in VS CODE\n",
        "      from colabcode import ColabCode\n",
        "      ColabCode(port=10000)\n",
        "  #mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  WORK_AREA = '/content/gdrive/My Drive/weaklensing_ML/' #columbialensing/\n",
        "  os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSxBOjESwy_q",
      "metadata": {
        "id": "MSxBOjESwy_q"
      },
      "source": [
        "\n",
        "## extract tarfiles if necessary and set specs for run\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4a5315a",
      "metadata": {
        "id": "a4a5315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "#whether we are training or loading saved\n",
        "train = True\n",
        "load_saved = 1\n",
        "\n",
        "#regularization parameters:\n",
        "## generalization seems great on cosmological data and so generally I run with these off\n",
        "L1weight = 0 #1e-8\n",
        "dropout_rate = 0\n",
        "\n",
        "# Specify the directory containing the .tar files\n",
        "if use_COLAB >= 1:\n",
        "    directory_path = './columbialensing/'\n",
        "else:\n",
        "        directory_path = '../weaklensing_ML/columbialensing/'\n",
        "number_batches = 10\n",
        "normalize_by_RMS = False #set to one if you want to renormalize by RMS\n",
        "\n",
        "# image_size\n",
        "image_size = 1024\n",
        "sub_image_size = 32 #needs to divide image into these units; must divide evenly image_size\n",
        "                    #division is using that it is unlikely there are learnable correlations\n",
        "                    #that allow one to compress the data on large scales in the images\n",
        "                    #dividing images gives more samples to learn correlations\n",
        "number_fits_files = 16 # just sto start\n",
        "\n",
        "\n",
        "number_subimages_across =image_size//sub_image_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extracts only if indicated (could make this more elegant by checking to see if they exist)\n",
        "extract_tarfiles = False  #if I need to extract tarfiles\n",
        "suffix = f\"_{image_size}\"\n",
        "run_suffix = rf\"im{image_size}\"\n",
        "if extract_tarfiles:\n",
        "    # Use a regular expression to match .tar files with the desired suffix\n",
        "    pattern = re.compile(rf\"{suffix}.tar$\")\n",
        "\n",
        "    # List all matching .tar files in the directory\n",
        "    all_tar_files = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "\n",
        "    # Extract the tar archive\n",
        "    for tar_file in all_tar_files:\n",
        "        #print(tar_file)\n",
        "        tar_file_path = os.path.join(directory_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as archive:\n",
        "            archive.extractall(path=directory_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0619681f",
      "metadata": {
        "id": "0619681f"
      },
      "source": [
        "# Read into memory the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "48a05090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a05090",
        "outputId": "de8579ef-f38a-414d-f362-c3a282901a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in Om0.268_si0.801\n",
            "RMS=0.018752897158265114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-26 09:19:53.858924: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
            "2024-10-26 09:19:53.858941: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
            "2024-10-26 09:19:53.858945: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
            "2024-10-26 09:19:53.858958: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-10-26 09:19:53.858968: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "#import wl_auxiliary\n",
        "\n",
        "def get_labels_for_file(dir_name):\n",
        "    \"\"\"\n",
        "    Extracts labels from the tar file name.\n",
        "    For the file \"Om0.183_si0.958_256.tar\", the labels will be [0.183, 0.958].\n",
        "\n",
        "    Args:\n",
        "    - tar_file_name (str): Name of the tar file.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the two labels extracted from the filename.\n",
        "    \"\"\"\n",
        "    # Split the filename on underscores\n",
        "    parts = dir_name.split('_')\n",
        "\n",
        "    # Extract the numeric values for 'Om' and 'si'\n",
        "    om_label = float(parts[0][2:])\n",
        "    si_label = float(parts[1][2:])\n",
        "\n",
        "    return [om_label, si_label]\n",
        "\n",
        "#now loop through all files in the\n",
        "pattern = re.compile(rf\"{suffix}$\")\n",
        "#all_directories = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "all_directories = [\"Om0.268_si0.801\"] # \"Om0.283_si0.805_256\"\n",
        "num_cosmologies = len(all_directories)\n",
        "\n",
        "random.shuffle(all_directories) #this makes it so that there is no particular order for the directories\n",
        "#print(all_directories)\n",
        "\n",
        "#tensor of labels; there are two labels for each\n",
        "numsubimages = number_subimages_across**2\n",
        "number_images = number_fits_files*numsubimages\n",
        "#cosmology_labels = np.empty((len(all_directories), number_images, 2), dtype=np.float16)\n",
        "\n",
        "RMS =0 #first time set to zero\n",
        "data_array = np.empty((num_cosmologies, number_images, sub_image_size, sub_image_size), dtype=np.float16)\n",
        "\n",
        "number_subimages_total = 0\n",
        "for idy, dir_name in enumerate(all_directories):\n",
        "\n",
        "\n",
        "    #if idy%10 ==0:\n",
        "    print(\"reading in\", dir_name)\n",
        "    dir_path = os.path.join(directory_path, dir_name)\n",
        "\n",
        "    all_files = os.listdir(dir_path)\n",
        "    fits_files = [f for f in all_files if f.endswith('.fits')]\n",
        "\n",
        "\n",
        "\n",
        "    for idx, file in enumerate(fits_files):\n",
        "        if idx >= number_fits_files:\n",
        "            break\n",
        "\n",
        "        with fits.open(os.path.join(dir_path, file)) as hdul:\n",
        "\n",
        "            original_data = hdul[0].data\n",
        "\n",
        "            if RMS == 0: #get RMS to divide by for first file to normalize everything\n",
        "                RMS = np.sqrt(np.var(hdul[0].data))\n",
        "                print(f\"RMS={RMS}\")\n",
        "\n",
        "            ##get rid of NANs, which affects a few files\n",
        "            #if np.isnan(original_data).any():\n",
        "            #    continue\n",
        "            #I've cleaned this out already\n",
        "            for i in range(number_subimages_across):\n",
        "                for j in range(number_subimages_across):\n",
        "                    data_array[idy][numsubimages*idx+ number_subimages_across*i+j] = original_data[sub_image_size*i:sub_image_size*(i+1),\\\n",
        "                                                                  sub_image_size*j:sub_image_size*(j+1)]/RMS\n",
        "                number_subimages_total +=1\n",
        "\n",
        "\n",
        "\n",
        "    #since all fits files in one directory have the same label\n",
        "    cosmology = get_labels_for_file(dir_name)\n",
        "    #cosmology_labels[idy] = np.array([cosmology for i in range(number_fits_files)])\n",
        "\n",
        "\n",
        "    #flatten data_array[idy][numsubimages*idx+ number_subimages_across*i+j]\n",
        "WL_tensor = tf.convert_to_tensor(data_array)\n",
        "\n",
        "WL_tensor = tf.reshape(WL_tensor, (-1, WL_tensor.shape[2], WL_tensor.shape[3]));\n",
        "\n",
        "WL_tensor = WL_tensor[..., np.newaxis]  # Add channel dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f790bd8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Number of images you want to display\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Extract the i-th image tensor\n",
        "    image_tensor = WL_tensor[i, :, :, 0]\n",
        "    # Use TensorFlow operations if needed (optional)\n",
        "    # Display the image\n",
        "    plt.imshow(image_tensor, cmap='gray')\n",
        "    plt.title(f\"Image {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f2885b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number_subimages_across = 32\n",
            "total number of images = 16384 512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([16384, 32, 32, 1])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"number_subimages_across =\", number_subimages_across )\n",
        "print(\"total number of images =\", number_subimages_across*number_subimages_across*number_fits_files *len(all_directories), number_subimages_total)\n",
        "np.shape(WL_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019e88b1",
      "metadata": {},
      "source": [
        "# Renormalize the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1363e4fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Implement Welford's algorithm for numerically stable mean and variance calculation\n",
        "def welford_algorithm(data):\n",
        "    n = 0\n",
        "    mean = 0.0\n",
        "    M2 = 0.0\n",
        "\n",
        "    for x in data:\n",
        "        n += 1\n",
        "        delta = x - mean\n",
        "        mean += delta / n\n",
        "        delta2 = x - mean\n",
        "        M2 += delta * delta2\n",
        "\n",
        "    variance = M2 / (n - 1) if n > 1 else float('nan')\n",
        "    return mean, variance\n",
        "\n",
        "calculate_statistics = False\n",
        "\n",
        "if calculate_statistics:\n",
        "    # Assuming WL_tensor is a TensorFlow tensor of floating-point numbers\n",
        "\n",
        "    # Convert the TensorFlow tensor to a NumPy array\n",
        "    WL_tensor_np = WL_tensor.numpy()\n",
        "    WL_tensor_np = WL_tensor_np[::1000, :, :, :]  # Downsample the tensor for faster computation\n",
        "\n",
        "    # Check for NaNs and Infinities\n",
        "    num_nans = np.isnan(WL_tensor_np).sum()\n",
        "    num_infs = np.isinf(WL_tensor_np).sum()\n",
        "    print(f\"Number of NaNs: {num_nans}\")\n",
        "    print(f\"Number of Infinities: {num_infs}\")\n",
        "\n",
        "\n",
        "    # Inspect the range of values\n",
        "    min_value = WL_tensor_np.min()\n",
        "    max_value = WL_tensor_np.max()\n",
        "    print(f\"Min value: {min_value}\")\n",
        "    print(f\"Max value: {max_value}\")\n",
        "\n",
        "    # Check the shape of the tensor\n",
        "    tensor_shape = WL_tensor_np.shape\n",
        "    print(f\"Tensor shape: {tensor_shape}\")\n",
        "\n",
        "    # Manually calculate the mean and variance\n",
        "    mean_value = np.mean(WL_tensor_np)\n",
        "    variance_value = np.var(WL_tensor_np)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Flatten the tensor to 1D for easier processing\n",
        "    WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate mean and variance using Welford's algorithm\n",
        "    mean_value, variance_value = welford_algorithm(WL_tensor_flat)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Calculate the standard deviation\n",
        "    std_dev = np.std(WL_tensor_np)\n",
        "    print(f\"Standard Deviation: {std_dev}\")\n",
        "\n",
        "    DO_KDE= True\n",
        "    if DO_KDE:\n",
        "        # Flatten the tensor to 1D for PDF calculation\n",
        "        WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
        "        kde = gaussian_kde(WL_tensor_flat)\n",
        "        x_values = np.linspace(WL_tensor_flat.min(), WL_tensor_flat.max(), 1000)\n",
        "        pdf_values = kde(x_values)\n",
        "\n",
        "        # Plot the PDF\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.semilogx(x_values, pdf_values, label='PDF')\n",
        "        plt.hist(WL_tensor_flat, bins=50, density=True, alpha=0.6, color='g', label='Histogram')\n",
        "        plt.title('Probability Density Function of WL_tensor')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0cc9a74",
      "metadata": {},
      "source": [
        "# now I'd like to bin all the pixels into logrithmic bins "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8e46b14b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min value: -1.816\n",
            "Max value: 28.12\n",
            "numbins = 127\n",
            "mean values =  [-1.786, -1.735, -1.688, -1.64, -1.59, -1.54, -1.49, -1.44, -1.392, -1.342, -1.292, -1.243, -1.193, -1.144, -1.094, -1.044, -0.994, -0.9443, -0.8945, -0.845, -0.7954, -0.7456, -0.696, -0.646, -0.596, -0.5464, -0.4968, -0.447, -0.3972, -0.3474, -0.2976, -0.2478, -0.198, -0.1482, -0.0983, -0.04852, 0.001258, 0.05103, 0.10077, 0.1505, 0.2003, 0.25, 0.2998, 0.3496, 0.3994, 0.4492, 0.499, 0.549, 0.5986, 0.6484, 0.698, 0.748, 0.798, 0.8477, 0.8975, 0.9473, 0.997, 1.047, 1.097, 1.146, 1.196, 1.246, 1.296, 1.346, 1.3955, 1.445, 1.495, 1.545, 1.595, 1.645, 1.694, 1.744, 1.794, 1.844, 1.894, 1.943, 1.993, 2.043, 2.092, 2.14, 2.191, 2.242, 2.291, 2.34, 2.39, 2.441, 2.49, 2.54, 2.59, 2.64, 2.69, 2.738, 2.79, 2.84, 2.889, 2.938, 2.988, 3.04, 3.088, 3.137, 3.188, 3.238, 3.287, 3.338, 3.387, 3.438, 3.486, 3.537, 3.586, 3.637, 3.686, 3.734, 3.787, 3.836, 3.885, 3.934, 3.979, 4.42, 5.484, 6.81, 8.43, 10.46, 12.98, 16.19, 20.06, 24.27, nan]\n",
            "binning_scheme =  [-1.8164e+00 -1.7666e+00 -1.7168e+00 -1.6670e+00 -1.6172e+00 -1.5674e+00\n",
            " -1.5176e+00 -1.4678e+00 -1.4180e+00 -1.3682e+00 -1.3184e+00 -1.2686e+00\n",
            " -1.2188e+00 -1.1689e+00 -1.1191e+00 -1.0693e+00 -1.0195e+00 -9.6973e-01\n",
            " -9.1992e-01 -8.7012e-01 -8.2031e-01 -7.7051e-01 -7.2070e-01 -6.7090e-01\n",
            " -6.2109e-01 -5.7129e-01 -5.2148e-01 -4.7168e-01 -4.2188e-01 -3.7207e-01\n",
            " -3.2227e-01 -2.7246e-01 -2.2266e-01 -1.7285e-01 -1.2305e-01 -7.3242e-02\n",
            " -2.3438e-02  2.6367e-02  7.6172e-02  1.2598e-01  1.7578e-01  2.2559e-01\n",
            "  2.7539e-01  3.2520e-01  3.7500e-01  4.2480e-01  4.7461e-01  5.2441e-01\n",
            "  5.7422e-01  6.2402e-01  6.7383e-01  7.2363e-01  7.7344e-01  8.2324e-01\n",
            "  8.7305e-01  9.2285e-01  9.7266e-01  1.0225e+00  1.0723e+00  1.1221e+00\n",
            "  1.1719e+00  1.2217e+00  1.2715e+00  1.3213e+00  1.3711e+00  1.4209e+00\n",
            "  1.4707e+00  1.5205e+00  1.5703e+00  1.6201e+00  1.6699e+00  1.7197e+00\n",
            "  1.7695e+00  1.8193e+00  1.8691e+00  1.9189e+00  1.9688e+00  2.0195e+00\n",
            "  2.0684e+00  2.1172e+00  2.1680e+00  2.2188e+00  2.2676e+00  2.3164e+00\n",
            "  2.3672e+00  2.4180e+00  2.4668e+00  2.5156e+00  2.5664e+00  2.6172e+00\n",
            "  2.6660e+00  2.7148e+00  2.7656e+00  2.8164e+00  2.8652e+00  2.9141e+00\n",
            "  2.9648e+00  3.0156e+00  3.0645e+00  3.1133e+00  3.1641e+00  3.2148e+00\n",
            "  3.2637e+00  3.3125e+00  3.3633e+00  3.4141e+00  3.4629e+00  3.5117e+00\n",
            "  3.5625e+00  3.6133e+00  3.6621e+00  3.7109e+00  3.7617e+00  3.8125e+00\n",
            "  3.8613e+00  3.9102e+00  3.9609e+00  4.0000e+00  4.9688e+00  6.1719e+00\n",
            "  7.6641e+00  9.5156e+00  1.1820e+01  1.4688e+01  1.8234e+01  2.2656e+01\n",
            "  2.8141e+01]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = arr.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "#number_pixels = 100\n",
        "#dlogval = np.log10(max_value - min_value)/number_pixels\n",
        "#print(dlogval)\n",
        "\n",
        "# Find the minimum value\n",
        "min_value = tf.reduce_min(WL_tensor)\n",
        "\n",
        "# Find the maximum value\n",
        "max_value = tf.reduce_max(WL_tensor)\n",
        "\n",
        "print(\"Min value:\", min_value.numpy())\n",
        "print(\"Max value:\", max_value.numpy())\n",
        "\n",
        "\n",
        "# Flatten the tensor to 1D for easier processing\n",
        "WL_tensor_flat = tf.reshape(WL_tensor, [-1])\n",
        " \n",
        "ddelta = 0.05 #value tracked in terms of RMS\n",
        "\n",
        "#binning_scheme = np.arange(min_value, -3, ddelta)\n",
        "binning_scheme = np.arange(min_value, 4, ddelta) #since there will be more rate pixels in other direction\n",
        "binning_scheme = np.append(binning_scheme, np.logspace(np.log10(4), np.log10(max_value), 10)) #ten bins for very high values\n",
        "\n",
        "# Convert binning_scheme to float16\n",
        "binning_scheme = binning_scheme.astype(np.float16)\n",
        "\n",
        "num_bins = len(binning_scheme)\n",
        "print(\"numbins =\", num_bins)\n",
        "\n",
        "WL_tensor_binned =  np.floor((WL_tensor_flat[WL_tensor_flat <4]-min_value)/ddelta)*ddelta - WL_tensor_flat[WL_tensor_flat <4] + min_value\n",
        "\n",
        "#print(np.std(WL_tensor_binned))\n",
        "\n",
        "\n",
        "# Digitize the tensor values according to the binning scheme\n",
        "bin_indices = tf.searchsorted(binning_scheme, WL_tensor_flat, side='right') - 1\n",
        "\n",
        "\n",
        "\n",
        "#let's calculate the mean we expect in each bin\n",
        "mean_values = [np.mean(WL_tensor_flat[bin_indices == index]) for index in range(len(binning_scheme))]\n",
        "\n",
        "print(\"mean values = \", mean_values)\n",
        "\n",
        "encoded_tensor = tf.gather(mean_values, bin_indices) #realization of quantized tensor\n",
        "\n",
        "print(\"binning_scheme = \",  binning_scheme)\n",
        "\n",
        "#reshape\n",
        "bin_indices = tf.reshape(bin_indices, [number_images,  sub_image_size*sub_image_size])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f8b7750",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[-0.696  -0.696  -0.7954 -0.845  -0.7456 -0.696  -0.4968 -0.447  -0.596\n",
            " -0.7456], shape=(10,), dtype=float16)\n",
            "tf.Tensor(\n",
            "[-0.685  -0.7046 -0.7974 -0.851  -0.763  -0.673  -0.4883 -0.4543 -0.608\n",
            " -0.762 ], shape=(10,), dtype=float16)\n",
            "now calculating std on dataset of size 16778:\n",
            "std_quantized = 0.03424\n"
          ]
        }
      ],
      "source": [
        "skip = 1000\n",
        "diff_tensor = encoded_tensor[::skip] - WL_tensor_flat[::skip]\n",
        "\n",
        "#just to check quantization works\n",
        "print(encoded_tensor[-10:])\n",
        "print(WL_tensor_flat[-10:])\n",
        "\n",
        "print(f\"now calculating std on dataset of size {len(diff_tensor)}:\")\n",
        "std_quantized = np.std(diff_tensor)\n",
        "print(\"std_quantized =\", std_quantized)\n",
        "\n",
        "####this is very slow\n",
        "#print(\"now calculating std using welford algorithm:\")\n",
        "#mean_value_diff, variance_value_diff = welford_algorithm(diff_tensor)\n",
        "#print(\"std_quantized =\", np.sqrt(variance_value_diff))\n",
        "\n",
        "#plot histogram of values ....very show\n",
        "plot_hist = False\n",
        "if plot_hist:\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Plot the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(diff_tensor, bins=30, edgecolor='black', alpha=0.7)\n",
        "    plt.title('Histogram of std_quantized')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5f23c29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices) =  (16384, 32, 32, 1) (16654944,) (16384, 1024) <dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "print(\"np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices) = \", np.shape(WL_tensor), np.shape(WL_tensor_binned), np.shape(bin_indices), bin_indices.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684c8c58",
      "metadata": {},
      "source": [
        "# Autoregressive image transformer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9dfa8755",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: tf.Tensor([16384  1024], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size*2  # Embedding dimension (d_model); sub_image_size*2 results in Nyquist sampling of the image\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0.01\n",
        "num_classes = num_bins  # Set num_classes to num_bins\n",
        "num_heads = 8 #number of attention heads\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(x)  # (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)  # Use width-1 to match seq_length\n",
        "\n",
        "    # Create causal mask\n",
        "    mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    mask = tf.cast(mask, dtype=tf.bool)\n",
        "\n",
        "    \n",
        "    for _ in range(n_layers):\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=d_model // num_heads\n",
        "        )(x, x, attention_mask=mask)\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(x)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Assume bin_indices is provided and has shape (num_samples, seq_length)\n",
        "# For demonstration, let's create a dummy bin_indices\n",
        "\n",
        "height = sub_image_size\n",
        "width = sub_image_size\n",
        "#seq_length = height * width\n",
        "\n",
        "\n",
        "print(\"Shape of bin_indices:\", tf.shape(bin_indices))\n",
        "\n",
        "# Adjust bin_indices to be zero-based indices\n",
        "#bin_indices_zero_based = bin_indices - 1  # Subtract 1 to make indices start from 0\n",
        "\n",
        "# Ensure that bin_indices_zero_based contains valid indices in the range [0, num_bins - 1]\n",
        "#assert bin_indices_zero_based.min() >= 0 and bin_indices_zero_based.max() < num_bins\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_old = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=number_channels,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_old.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d795f89c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_13       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_13         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">8,128</span>  input_layer_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 positional_encod \n",
              "\n",
              " dropout_43           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  layer_normalizat \n",
              "\n",
              " dropout_44           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n",
              "                                                     dropout_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "\n",
              " dropout_46           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n",
              "                                                     dropout_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span>  layer_normalizat \n",
              "\n",
              " dropout_47           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer_normalizat \n",
              "                                                     dropout_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,255</span>  layer_normalizat \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_13       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_13         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m8,128\u001b[0m  input_layer_13[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 positional_encod \n",
              "\n",
              " dropout_43           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_28 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_31 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m4,160\u001b[0m  layer_normalizat \n",
              "\n",
              " dropout_44           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_29 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  layer_normalizat \n",
              "                                                     dropout_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "\n",
              " dropout_46           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_30 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  layer_normalizat \n",
              "                                                     dropout_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_32 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m4,160\u001b[0m  layer_normalizat \n",
              "\n",
              " dropout_47           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_31 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  layer_normalizat \n",
              "                                                     dropout_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_33 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m127\u001b[0m)       \u001b[38;5;34m8,255\u001b[0m  layer_normalizat \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,495</span> (228.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,495\u001b[0m (228.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,495</span> (228.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,495\u001b[0m (228.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "autoregressive_transformer_old.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "9d6a013b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 522ms/step - accuracy: 0.0742 - loss: 5.8166 - val_accuracy: 0.1024 - val_loss: 2.9617\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m468s\u001b[0m 508ms/step - accuracy: 0.1026 - loss: 2.9563 - val_accuracy: 0.1063 - val_loss: 2.9269\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 513ms/step - accuracy: 0.1121 - loss: 2.8702 - val_accuracy: 0.1449 - val_loss: 2.5900\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2558s\u001b[0m 3s/step - accuracy: 0.1425 - loss: 2.6099 - val_accuracy: 0.1573 - val_loss: 2.4978\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 512ms/step - accuracy: 0.1521 - loss: 2.5343 - val_accuracy: 0.1625 - val_loss: 2.4635\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step\n",
            "Predictions shape: (1, 1023, 127)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 396ms/step - accuracy: 0.1632 - loss: 2.4644\n",
            "Validation Loss: 2.4635  bits per pixel : 3.5541\n",
            "Validation Accuracy: 0.1625\n"
          ]
        }
      ],
      "source": [
        "# Optionally split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_old.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1].numpy()  # Take the first sample\n",
        "predictions = autoregressive_transformer_old.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Optionally, you can get the most likely class for each position\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# You can also evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_old.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275e2ddb",
      "metadata": {},
      "source": [
        "# Visualize images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d07388d",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_val' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 169\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Run the generalized code\u001b[39;00m\n\u001b[1;32m    168\u001b[0m num_samples_to_visualize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Change this to the number of random samples you want to visualize\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m process_random_images(\u001b[43mX_val\u001b[49m, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Much slower code that I don't use anymore\n",
        "def autoregressive_predict(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model, one step at a time,\n",
        "    using the original sequence up to the current position as context.\n",
        "    \"\"\"\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for i in range(1, sequence_length+1):\n",
        "        # Prepare the context (all previous pixels)\n",
        "        context = original_sequence[:i]\n",
        "\n",
        "        # No padding required; model expects input of length seq_length\n",
        "        # For positions where context is shorter, we need to pad or adjust the input\n",
        "        # Since we're avoiding padding, we'll adjust the input sequence accordingly\n",
        "\n",
        "        # Create a context of length 'i'\n",
        "        model_input = np.array(context, dtype=np.int32)[np.newaxis, :]\n",
        "\n",
        "        # Since the model expects input of shape (batch_size, seq_length),\n",
        "        # we need to handle the varying lengths\n",
        "        # One way is to slice the model to accept variable input lengths\n",
        "\n",
        "        # Predict\n",
        "        next_pixel_probs = model.predict(model_input, verbose=0)\n",
        "        # Get the prediction for the current position\n",
        "        next_pixel = np.argmax(next_pixel_probs[0, -1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Step {i}: Context length: {len(context)}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "#should be much faster....need to check\n",
        "def autoregressive_predict_batched(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model by batching inputs,\n",
        "    which significantly speeds up the prediction process.\n",
        "    \"\"\"\n",
        "    # Prepare all contexts at once\n",
        "    contexts = [original_sequence[:i] for i in range(1, sequence_length + 1)]\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = sequence_length\n",
        "    padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        contexts, maxlen=max_len, padding='post', value=0\n",
        "    )\n",
        "\n",
        "    # Convert to array\n",
        "    model_inputs = np.array(padded_contexts, dtype=np.int32)\n",
        "\n",
        "    # Predict all at once\n",
        "    next_pixel_probs = model.predict(model_inputs, verbose=0)\n",
        "\n",
        "    # Extract the predictions\n",
        "    predicted_sequence = []\n",
        "    for i, context in enumerate(contexts):\n",
        "        # Get the prediction for the current position\n",
        "        seq_length = len(context)\n",
        "        next_pixel = np.argmax(next_pixel_probs[i, seq_length - 1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1}: Context length: {seq_length}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "def process_random_images(X_val, y_val, model, sub_image_size, num_samples=5):\n",
        "    \"\"\"\n",
        "    Process and visualize random images from the validation set.\n",
        "    \"\"\"\n",
        "    total_samples = len(X_val)\n",
        "    random_indices = np.random.choice(total_samples, num_samples, replace=False)\n",
        "    \n",
        "    \n",
        "    for i, idx in enumerate(random_indices):\n",
        "        val_sample = X_val[idx]\n",
        "        original_sequence = y_val[idx]\n",
        "        \n",
        "        # Determine the sequence length\n",
        "        sequence_length = len(original_sequence)\n",
        "        \n",
        "        # Predict using the original sequence as context\n",
        "        predicted_sequence = autoregressive_predict_batched(\n",
        "            model,\n",
        "            original_sequence,\n",
        "            sequence_length,\n",
        "        )\n",
        "        \n",
        "        print(\"val_sample and original_sequence shapes\", np.shape(val_sample), np.shape(original_sequence), np.shape(predicted_sequence))\n",
        "    \n",
        "        visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, i+1)\n",
        "        \n",
        "        mse = np.mean((original_sequence - predicted_sequence)**2)\n",
        "        mae = np.mean(np.abs(original_sequence - predicted_sequence))\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.4f}\")\n",
        "        print(f\"  Original sequence shape: {original_sequence.shape}\")\n",
        "        print(f\"  Predicted sequence shape: {predicted_sequence.shape}\")\n",
        "        print(f\" fraction of pixles correct\", np.sum(original_sequence - predicted_sequence == 0)/len(original_sequence))\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# The visualize_autoregressive_prediction function remains the same\n",
        "\n",
        "def visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, predicted, and difference images with equal sizes.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "    fig.suptitle(f'Sample {index}', fontsize=16)\n",
        "\n",
        "    # Prepare images\n",
        "    original_2d = sequence_to_image(original_sequence, sub_image_size, original=True)\n",
        "    predicted_2d = sequence_to_image(predicted_sequence, sub_image_size, original=False)\n",
        "    diff = original_2d - predicted_2d\n",
        "\n",
        "    # Set up common parameters for imshow\n",
        "    imshow_args = {'interpolation': 'nearest', 'aspect': 'equal'}\n",
        "\n",
        "    # Visualize original sequence\n",
        "    im1 = axes[0].imshow(original_2d, cmap='gray', **imshow_args)\n",
        "    axes[0].set_title('Original Image')\n",
        "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize predicted sequence\n",
        "    im2 = axes[1].imshow(predicted_2d, cmap='gray', **imshow_args)\n",
        "    axes[1].set_title('Predicted Image (Autoregressive)')\n",
        "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize difference\n",
        "    im3 = axes[2].imshow(diff, cmap='bwr', **imshow_args)\n",
        "    axes[2].set_title('Difference (Original - Predicted)')\n",
        "    plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sequence_to_image(sequence, sub_image_size, original=True):\n",
        "    \"\"\"\n",
        "    Convert a 1D sequence of length 1023 to a 2D image of size sub_image_size x sub_image_size,\n",
        "    by prepending the first_token to the sequence.\n",
        "    \"\"\"\n",
        "    # Concatenate the first token to the sequence\n",
        "    if original:\n",
        "        full_sequence = np.concatenate((sequence, [0]))\n",
        "    else:\n",
        "        full_sequence = np.concatenate(([0], sequence))\n",
        "    # Reshape to image\n",
        "    image = full_sequence.reshape(sub_image_size, sub_image_size)\n",
        "    return image\n",
        "\n",
        "# Run the generalized code\n",
        "num_samples_to_visualize = 3  # Change this to the number of random samples you want to visualize\n",
        "process_random_images(X_val, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349c2e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot predictions vs actual values\n",
        "def plot_predictions(model, sequences, targets, num_samples=10):\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(sequences[:num_samples])\n",
        "    predicted_pixels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Plot the actual vs predicted pixel values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(sequences[i].reshape(-1, 1), cmap='gray', aspect='auto')\n",
        "        plt.title(f\"Sequence {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(num_classes), predictions[i])\n",
        "        plt.axvline(x=targets[i], color='r', linestyle='--')\n",
        "        plt.axvline(x=predicted_pixels[i], color='g', linestyle='--')\n",
        "        plt.title(f\"True: {targets[i]}, Pred: {predicted_pixels[i]}\")\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions for some sequences\n",
        "plot_predictions(autoregressive_transformer, train_sequences, train_targets, num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3f181ef2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.16.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab491d",
      "metadata": {},
      "source": [
        "## Simple autoencoder with sines and cosines sampling to Nyquist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "98c92482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">8,128</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " not_equal            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)                                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n",
              "                                                     not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              "\n",
              " add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " dropout_10           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_11           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                     dropout_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">8,255</span>  add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m8,128\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda (\u001b[38;5;33mLambda\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " not_equal            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  input_layer_1[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mNotEqual\u001b[0m)                                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n",
              "                                                     not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " dropout_7 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              "\n",
              " add_4 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_4 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_8 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_5 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " dropout_10           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_6 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_5 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_11           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_7 (\u001b[38;5;33mAdd\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                     dropout_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_7 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m)       \u001b[38;5;34m8,255\u001b[0m  add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,351</span> (454.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m116,351\u001b[0m (454.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">116,351</span> (454.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m116,351\u001b[0m (454.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  #don't think need L1 and dropout  (I've tested and it performs best with this equal to zero)\n",
        "num_classes = num_bins  # Set num_classes to num_bins: this is the number of values that can be populated\n",
        "d_model = number_channels*2  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.d_model = d_model\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    #inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)  # Accept variable-length sequences\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(x)  # Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Create the causal mask once as a constant tensor\n",
        "    #causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    #causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    def create_causal_mask(x):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        return causal_mask\n",
        "\n",
        "    causal_mask = layers.Lambda(create_causal_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_length = sub_image_size * sub_image_size\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_deep = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers*2,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "\n",
        "autoregressive_transformer.summary()\n",
        "\n",
        "autoregressive_transformer_deep.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6de246",
      "metadata": {},
      "source": [
        "## Compile models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb4fc3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_deep.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1a6b2",
      "metadata": {},
      "source": [
        "### Train models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "650102df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "2024-10-26 09:20:18.755213: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 522ms/step - accuracy: 0.0862 - loss: 3.3005 - val_accuracy: 0.1213 - val_loss: 2.7849\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 549ms/step - accuracy: 0.1242 - loss: 2.7409 - val_accuracy: 0.1484 - val_loss: 2.5348\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 558ms/step - accuracy: 0.1467 - loss: 2.5444 - val_accuracy: 0.1628 - val_loss: 2.4392\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 550ms/step - accuracy: 0.1598 - loss: 2.4570 - val_accuracy: 0.1752 - val_loss: 2.3618\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 545ms/step - accuracy: 0.1688 - loss: 2.3998 - val_accuracy: 0.1790 - val_loss: 2.3372\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 418ms/step - accuracy: 0.1798 - loss: 2.3376\n",
            "Validation Loss: 2.3372  bits per pixel : 3.3719\n",
            "Validation Accuracy: 0.1790\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b344ac9",
      "metadata": {},
      "source": [
        "### Let's fit the deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "49bec05f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m982s\u001b[0m 1s/step - accuracy: 0.0871 - loss: 3.2729 - val_accuracy: 0.1239 - val_loss: 2.7536\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1021s\u001b[0m 1s/step - accuracy: 0.1277 - loss: 2.7058 - val_accuracy: 0.1580 - val_loss: 2.4666\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1073s\u001b[0m 1s/step - accuracy: 0.1560 - loss: 2.4744 - val_accuracy: 0.1778 - val_loss: 2.3402\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1109s\u001b[0m 1s/step - accuracy: 0.1720 - loss: 2.3700 - val_accuracy: 0.1864 - val_loss: 2.2889\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1164s\u001b[0m 1s/step - accuracy: 0.1809 - loss: 2.3185 - val_accuracy: 0.1920 - val_loss: 2.2581\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 471ms/step - accuracy: 0.1798 - loss: 2.3376\n",
            "Validation Loss: 2.3372  bits per pixel : 3.3719\n",
            "Validation Accuracy: 0.1790\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_deep.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489870fe",
      "metadata": {},
      "source": [
        "## This has a free positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6cb767fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matt/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'positional_embedding' (of type PositionalEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_3        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_3          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">32,736</span>  embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  positional_embed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "\n",
              " dropout_25           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_embed \n",
              "                                                     dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_26           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "\n",
              " dropout_28           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_29           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1023</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_3        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_3          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,096\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)       \u001b[38;5;34m32,736\u001b[0m  embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEmbeddi\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  positional_embed \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "\n",
              " dropout_25           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_16 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_embed \n",
              "                                                     dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_17 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_18 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_26           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_17 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "\n",
              " dropout_28           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_18 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_19 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_20 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_29           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_19 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1023\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,464</span> (259.62 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,464\u001b[0m (259.62 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  # zero is no regularizer (at least with set positional encoding performs better without regularization)\n",
        "num_classes = num_bins+1  # Number of quantization bins\n",
        "d_model = number_channels  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Embedding Layer with learnable parameters\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, seq_length, d_model, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.d_model = d_model\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            shape=(seq_length, d_model),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='position_embeddings'\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, seq_length, d_model)\n",
        "        return inputs + self.position_embeddings\n",
        "\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Adjusted for autoregressive prediction\n",
        "    inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(inputs)\n",
        "\n",
        "    # Apply learnable positional embeddings\n",
        "    x = PositionalEmbedding(seq_length, d_model)(x)\n",
        "\n",
        "    # Create the causal mask\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Adjusted sequence length for autoregressive prediction\n",
        "seq_length = sub_image_size * sub_image_size - 1\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_freeposemb = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_freeposemb.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# View the model summary\n",
        "autoregressive_transformer_freeposemb.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f974a0b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 473ms/step - accuracy: 0.0892 - loss: 3.2488 - val_accuracy: 0.1038 - val_loss: 2.9402\n",
            "Epoch 2/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 474ms/step - accuracy: 0.1047 - loss: 2.9234 - val_accuracy: 0.1215 - val_loss: 2.7774\n",
            "Epoch 3/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 494ms/step - accuracy: 0.1269 - loss: 2.7261 - val_accuracy: 0.1461 - val_loss: 2.5764\n",
            "Epoch 4/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 514ms/step - accuracy: 0.1474 - loss: 2.5605 - val_accuracy: 0.1595 - val_loss: 2.4724\n",
            "Epoch 5/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 500ms/step - accuracy: 0.1589 - loss: 2.4741 - val_accuracy: 0.1677 - val_loss: 2.4133\n",
            "Epoch 6/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 493ms/step - accuracy: 0.1668 - loss: 2.4227 - val_accuracy: 0.1720 - val_loss: 2.3865\n",
            "Epoch 7/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 499ms/step - accuracy: 0.1709 - loss: 2.3979 - val_accuracy: 0.1746 - val_loss: 2.3719\n",
            "Epoch 8/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 468ms/step - accuracy: 0.1735 - loss: 2.3823 - val_accuracy: 0.1765 - val_loss: 2.3580\n",
            "Epoch 9/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 461ms/step - accuracy: 0.1751 - loss: 2.3716 - val_accuracy: 0.1785 - val_loss: 2.3528\n",
            "Epoch 10/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 464ms/step - accuracy: 0.1768 - loss: 2.3613 - val_accuracy: 0.1794 - val_loss: 2.3425\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
            "Predictions shape: (1, 1023, 128)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 414ms/step - accuracy: 0.1799 - loss: 2.3426\n",
            "Validation Loss: 2.3425  bits per pixel : 3.3795\n",
            "Validation Accuracy: 0.1794\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_freeposemb.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1]  # Take the first sample\n",
        "predictions = autoregressive_transformer_freeposemb.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# Reconstruct and visualize the predicted image\n",
        "#first_token = sample_input.numpy()[0, 0]\n",
        "#reconstructed_sequence = np.concatenate(([first_token], predicted_classes[0]))\n",
        "#reconstructed_image = reconstructed_sequence.reshape(sub_image_size, sub_image_size)\n",
        "\n",
        "#plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.title('Predicted Image')\n",
        "#plt.axis('off')\n",
        "#plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_freeposemb.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c0608c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
