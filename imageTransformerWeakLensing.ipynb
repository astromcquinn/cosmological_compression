{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed531a95",
      "metadata": {
        "id": "ed531a95"
      },
      "source": [
        "# Implements encoder/decoder for weak lensing outputs or an image transformer\n",
        "\n",
        "The major idea is to see if I can compress the data in the snapshot files.\n",
        "The result is that the compression of many different algorithms based on CNNs (of different depths) is not so much different than averaging neighboring cells (as shown at the end).  This in retrospect is not so surprising as there are differences on the cell scale in the maps that make compression challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zQpe_fjWxGwO",
      "metadata": {
        "id": "zQpe_fjWxGwO"
      },
      "source": [
        "Set configurations for google COLAB if running there (to use their GPUs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "KEtElnI8fDj1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtElnI8fDj1",
        "outputId": "59dde345-ea8f-426f-a56a-69affd2aaf3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "use_COLAB = 0 #1 is for on colab, and 2 is for on local machine but using colab\n",
        "\n",
        "if use_COLAB >= 1:\n",
        "  if use_COLAB == 2: # for running in VS CODE\n",
        "      from colabcode import ColabCode\n",
        "      ColabCode(port=10000)\n",
        "  #mount drive\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  WORK_AREA = '/content/gdrive/My Drive/weaklensing_ML/' #columbialensing/\n",
        "  os.chdir(WORK_AREA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSxBOjESwy_q",
      "metadata": {
        "id": "MSxBOjESwy_q"
      },
      "source": [
        "\n",
        "### extract tarfiles if necessary and set specs for run\n",
        "\n",
        "(we are only using Weak lensing maps here)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a4a5315a",
      "metadata": {
        "id": "a4a5315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tarfile\n",
        "import os\n",
        "import shutil\n",
        "from astropy.io import fits\n",
        "import numpy as np\n",
        "from scipy.ndimage import zoom\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "#whether we are training or loading saved\n",
        "train = True\n",
        "load_saved = 1\n",
        "\n",
        "# Specify the directory containing the .tar files\n",
        "if use_COLAB >= 1:\n",
        "    directory_path = './columbialensing/'\n",
        "else:\n",
        "        directory_path = '../weaklensing_ML/columbialensing/'\n",
        "number_batches = 10\n",
        "#normalize_by_RMS = True #set to one if you want to renormalize by RMS\n",
        "\n",
        "# image_size\n",
        "image_size = 1024\n",
        "sub_image_size = 32 #needs to divide image into these units; must divide evenly image_size\n",
        "                    #division is using that it is unlikely there are learnable correlations\n",
        "                    #that allow one to compress the data on large scales in the images\n",
        "                    #dividing images gives more samples to learn correlations\n",
        "number_fits_files = 16 # just sto start\n",
        "\n",
        "\n",
        "number_subimages_across =image_size//sub_image_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extracts only if indicated (could make this more elegant by checking to see if they exist)\n",
        "extract_tarfiles = False  #if need to unpack tarfiles before reading in\n",
        "suffix = f\"_{image_size}\"\n",
        "run_suffix = rf\"im{image_size}\"\n",
        "if extract_tarfiles:\n",
        "    # Use a regular expression to match .tar files with the desired suffix\n",
        "    pattern = re.compile(rf\"{suffix}.tar$\")\n",
        "\n",
        "    # List all matching .tar files in the directory\n",
        "    all_tar_files = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "\n",
        "    # Extract the tar archive\n",
        "    for tar_file in all_tar_files:\n",
        "        #print(tar_file)\n",
        "        tar_file_path = os.path.join(directory_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as archive:\n",
        "            archive.extractall(path=directory_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0619681f",
      "metadata": {
        "id": "0619681f"
      },
      "source": [
        "# Read into memory the data\n",
        "\n",
        "two important parameters are set here\n",
        "\n",
        "\n",
        "bool normalize_by_RMS   -- normalizes to RMS of data (probably always want to do this as it makes errors and other things easier to interpret\n",
        "                       it doesn't impact compression as you can always pass this float as well\n",
        "bool LOGFIELD --   whether you want to log the field or not.  Try both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "48a05090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a05090",
        "outputId": "de8579ef-f38a-414d-f362-c3a282901a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading in Om0.268_si0.801\n",
            "LOGFIELD = True :  mean, std, min 0.00015788472 0.018293597 -0.034070354\n"
          ]
        }
      ],
      "source": [
        "#parameters we need\n",
        "normalize_by_RMS = True #normalizes to RMS of data (probably always want to do this as it makes errors and other things easier to interpret\n",
        "                       #it doesn't impact compression as you can always pass this float as well\n",
        "LOGFIELD = True  #whether you want to log the field or not.  Try both\n",
        "\n",
        "def get_labels_for_file(dir_name):\n",
        "    \"\"\"\n",
        "    Extracts labels from the tar file name.\n",
        "    For the file \"Om0.183_si0.958_256.tar\", the labels will be [0.183, 0.958].\n",
        "\n",
        "    Args:\n",
        "    - tar_file_name (str): Name of the tar file.\n",
        "\n",
        "    Returns:\n",
        "    - list: List containing the two labels extracted from the filename.\n",
        "    \"\"\"\n",
        "    # Split the filename on underscores\n",
        "    parts = dir_name.split('_')\n",
        "\n",
        "    # Extract the numeric values for 'Om' and 'si'\n",
        "    om_label = float(parts[0][2:])\n",
        "    si_label = float(parts[1][2:])\n",
        "\n",
        "    return [om_label, si_label]\n",
        "\n",
        "#now loop through all files in the\n",
        "pattern = re.compile(rf\"{suffix}$\")\n",
        "#all_directories = [f for f in os.listdir(directory_path) if pattern.search(f)]\n",
        "all_directories = [\"Om0.268_si0.801\"] # \"Om0.283_si0.805_256\"\n",
        "num_cosmologies = len(all_directories)\n",
        "\n",
        "random.shuffle(all_directories) #this makes it so that there is no particular order for the directories\n",
        "#print(all_directories)\n",
        "\n",
        "#tensor of labels; there are two labels for each\n",
        "numsubimages = number_subimages_across**2\n",
        "number_images = number_fits_files*numsubimages\n",
        "#cosmology_labels = np.empty((len(all_directories), number_images, 2), dtype=np.float16)\n",
        "\n",
        "RMS =0 #first time set to zero\n",
        "data_array = np.empty((num_cosmologies, number_images, sub_image_size, sub_image_size), dtype=np.float32)\n",
        "\n",
        "number_subimages_total = 0\n",
        "for idy, dir_name in enumerate(all_directories):\n",
        "\n",
        "\n",
        "    #if idy%10 ==0:\n",
        "    print(\"reading in\", dir_name)\n",
        "    dir_path = os.path.join(directory_path, dir_name)\n",
        "\n",
        "    all_files = os.listdir(dir_path)\n",
        "    fits_files = [f for f in all_files if f.endswith('.fits')]\n",
        "\n",
        "\n",
        "\n",
        "    for idx, file in enumerate(fits_files):\n",
        "        if idx >= number_fits_files:\n",
        "            break\n",
        "\n",
        "        with fits.open(os.path.join(dir_path, file)) as hdul:\n",
        "\n",
        "            original_data = hdul[0].data\n",
        "\n",
        "            #if RMS == 0: #get RMS to divide by for first file to normalize everything\n",
        "            #    RMS = np.sqrt(np.var(hdul[0].data))\n",
        "            #    print(f\"RMS={RMS}\")\n",
        "\n",
        "            ##get rid of NANs, which affects a few files\n",
        "            #if np.isnan(original_data).any():\n",
        "            #    continue\n",
        "            #I've cleaned this out already\n",
        "            for i in range(number_subimages_across):\n",
        "                for j in range(number_subimages_across):\n",
        "                    data_array[idy][numsubimages*idx+ number_subimages_across*i+j] = original_data[sub_image_size*i:sub_image_size*(i+1),\\\n",
        "                                                                  sub_image_size*j:sub_image_size*(j+1)]\n",
        "                number_subimages_total +=1\n",
        "\n",
        "\n",
        "\n",
        "    #since all fits files in one directory have the same label\n",
        "    cosmology = get_labels_for_file(dir_name)\n",
        "    #cosmology_labels[idy] = np.array([cosmology for i in range(number_fits_files)])\n",
        "\n",
        "\n",
        "    #flatten data_array[idy][numsubimages*idx+ number_subimages_across*i+j]\n",
        "\n",
        "if LOGFIELD:\n",
        "    meanf = np.mean(data_array)\n",
        "    stdf = np.std(data_array)\n",
        "    minf = np.min(data_array)\n",
        "    data_array = np.log((data_array-minf)/stdf + 1e-3)  #this was 1e-30 before 10/29/24, but 1e-3 makes sense to me\n",
        "    print(\"LOGFIELD = True :  mean, std, min\", meanf, stdf, minf)\n",
        "elif normalize_by_RMS:\n",
        "    stdf = np.std(data_array)\n",
        "    data_array = data_array/stdf\n",
        "    print(stdf)\n",
        "    print(\"normalize_by_RMS = True : std\", stdf)\n",
        "\n",
        "WL_tensor = tf.convert_to_tensor(data_array)\n",
        "\n",
        "WL_tensor = tf.reshape(WL_tensor, (-1, WL_tensor.shape[2], WL_tensor.shape[3]));\n",
        "\n",
        "WL_tensor = WL_tensor[..., np.newaxis]  # Add channel dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacc2ea6",
      "metadata": {},
      "source": [
        "#$ Let's make an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2f790bd8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbNElEQVR4nO3db2yddf3G8as9bU/bc9qe/lu7dWMbE6eocQqRSFRkSnAGiUHUqAmIfwIPJLjEB0ZiSCDZA0XikmkwzuADNjD4ADUaCQkzGlEiMSqRLCthMujawun/nj897en5PSB+whi670V22OT3fiUmUD/77D73fZ9eOxv3tZZGo9EQAACSWs/1AQAAzh+EAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKOC/89Kc/VUtLi5588slzfShNU6lU9OUvf1nvfOc71dfXp3w+r3e/+93av3+/VldXz/XhAZKktnN9AMD/F5VKRf/85z/18Y9/XNu2bVNra6sef/xx7d27V0888YQOHz58rg8RIBSAN8rAwID+/Oc/n/K1W265RX19fTpw4IDuuecejY6OnqOjA17Gbx/hvPXFL35R+XxeJ06c0DXXXKN8Pq+xsTH94Ac/kCQ99dRT2r17t3K5nLZu3Xrar7RnZ2f1jW98Q+9617uUz+fV29urPXv26O9///tpP9dzzz2na6+9VrlcThs2bNDevXv1yCOPqKWlRb/73e9OmX3iiSf0sY99TH19feru7tYVV1yhP/7xj6/7dW7btk2SND8//7p3AGcLnxRwXqvX69qzZ48+9KEP6Tvf+Y4OHTqkr33ta8rlcrr99tv1hS98Qdddd53uvfde3XDDDXr/+9+v7du3S5KeffZZPfzww/r0pz+t7du3a3p6Wj/60Y90xRVX6Omnn9amTZskSaVSSbt379bk5KRuu+02jY6O6vDhwzpy5Mhpx/PYY49pz549uuSSS3THHXeotbVV9913n3bv3q0//OEPet/73nfG11Sr1bS4uKhKpaInn3xSd999t7Zu3aq3vOUtZ/fkAa9HAzgP3HfffQ1Jjb/85S/xtRtvvLEhqbFv37742tzcXKOrq6vR0tLSePDBB+PrR48ebUhq3HHHHfG1arXaqNfrp/w8x48fb2Sz2cadd94ZX/ve977XkNR4+OGH42uVSqXxtre9rSGpceTIkUaj0Wisr683LrroosbVV1/dWF9fj9lyudzYvn1746qrrkp6rQ888EBDUvzv0ksvbfzjH/9I+rFAs/HbRzjvfeUrX4l/LhQK2rlzp3K5nD7zmc/E13fu3KlCoaBnn302vpbNZtXa+vItXq/XNTMzo3w+r507d+qvf/1rzP32t7/V2NiYrr322vhaZ2envvrVr55yHH/72980Pj6uz3/+85qZmVGxWFSxWFSpVNJHPvIR/f73v9f6+voZX8+VV16pRx99VA899JBuueUWtbe3q1Qq+ScGaAJ++wjntc7OTg0PD5/ytb6+Pm3evFktLS2nfX1ubi7+fX19Xfv379cPf/hDHT9+XPV6Pf6/wcHB+OfnnntOO3bsOG3fq387Z3x8XJJ04403/sfjXVhYUH9//399TSMjIxoZGZEkXX/99dq3b5+uuuoqjY+P8wfNOOcIBZzXMpmM9fXGK/522X379unb3/62vvSlL+muu+7SwMCAWltb9fWvfz3pV/Sv9u8f893vfle7du16zZl8Pm/vvf7663X77bfrF7/4hW6++Wb7xwNnE6GAN62f//znuvLKK/WTn/zklK/Pz89raGgo/n3r1q16+umn1Wg0Tvm08Mwzz5zy43bs2CFJ6u3t1Uc/+tGzdpyVSkXSy58ygHONP1PAm1Ymkznlk4MkPfTQQ5qYmDjla1dffbUmJib0y1/+Mr5WrVb14x//+JS5Sy65RDt27NDdd9+t5eXl036+l1566b8eT7FYPO14JOngwYOSpEsvvfS/vyDgDcAnBbxpXXPNNbrzzjt100036fLLL9dTTz2lQ4cO6cILLzxl7uabb9aBAwf0uc99Trfddps2btyoQ4cOqbOzU5Li00Nra6sOHjyoPXv26B3veIduuukmjY2NaWJiQkeOHFFvb69+9atf/cfjuf/++3Xvvffqk5/8pC688EItLS3pkUce0aOPPqpPfOIT2r17d/NOBpCIUMCb1re+9S2VSiUdPnxYP/vZz/Te975Xv/71r/XNb37zlLl8Pq/HHntMt956q/bv3698Pq8bbrhBl19+uT71qU9FOEjShz/8Yf3pT3/SXXfdpQMHDmh5eVmjo6O67LLLzvjnAR/4wAf0+OOP64EHHtD09LTa2tq0c+dO3XPPPbr11lubcg4AV0vjtT7PAtD3v/997d27Vy+88ILGxsbO9eEAbwhCAdDLf9jb1dUV/16tVvWe97xH9Xpdx44dO4dHBryx+O0jQNJ1112nCy64QLt27dLCwoLuv/9+HT16VIcOHTrXhwa8oQgFQC//F0gHDx7UoUOHVK/XdfHFF+vBBx/UZz/72XN9aMAbit8+AgAEnlMAAARCAQAQkv9M4YMf/KC1+JX/bfeZvLKoLEWtVkueddsnnd2vfgjqTF7rKdizMStJW7Zssea7u7uTZ9/61rc2bferS+jOpKOjI3nWva9czjV6PV1Lqf7dBJtqYGAgefaV/0VWitnZ2eTZVz9ZfiYzMzPWvPPed/+O7P/UvfVanHtWksrlcvLs2tqatfs3v/nNGWf4pAAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJDcfTQ1NWUtLhQKybNub8/8/Lw176hWq8mzxWLR2r24uJg863bluB0ozewQaubr7O/vT55ta/P+uhCnz0aS2tvbk2fd1zk0NJQ867zXJGl0dDR51j2H7vcJh3ssc3NzybMLCwvWbudvHHD/doJsNps8656TFHxSAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABCSn5Hetm2btbi1NT1v3IoGp17ArWhwjtut23BeZy6Xs3Y7j8ZL3qP3Tm2FJLW0tCTPrq6uWrsdfX191vzg4KA171SFdHV1WbudmotNmzZZuzdu3Jg8W6vVrN0Ot1rCrZVxvk808/ubew6d95v73kzBJwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITk7qORkRFrsdN/Mzc3Z+12Ok0ymYy1e3l5OXnW7WJxtLUlXxpJ0szMjDXvdKasrKxYuwuFQvKs23v1r3/9K3l2YGDA2u32TfX29ibPDg8PW7vHxsaaMitJ/f39ybNuv5fzfnOvvduT5fQTOddSknp6epJnS6WStbtarSbPuv1RKfikAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAkdym4j4E7j7C7j4GXy+XkWbeiwXk0vqOjw9rtvM5Go2Htrtfr1vz/KqcCYGlpydpdq9Wseeecd3V1Wbs7OzuTZ51KGcl7bzq1L5JXWeO+7516G8l7nU7ti8u5ZyXvuJ3vhan4pAAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJDcfeR28TjczpmZmZnk2UwmY+0eGBhInu3p6bF2O70w+Xze2l0oFKz59fX15FmnD0ry7hX3vlpdXU2edTtnisWiNe90Hzn3lSR1d3cnz7rn0On5efHFF63dzj3u3IOS3x+1sLCQPDs5OWntdnrP3G4q5/uh+75PwScFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAACG55sKtDHAe7V5bW7N2N1Mul0uedesFVlZWkmf7+/ut3dls1pp3qg6mpqas3bOzs8mzbtWBcw6dSgxJOnnypDXv3LduzYVTz+LUOUjefbu0tGTtdu4rZ1by7ivJO3Z3d19fX/Jsb2+vtXt+fj55thn1Q3xSAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBASO4+qtfr1uJarZY8297ebu1uZu/I4OBg8qzTTyNJnZ2dybMbNmywdre2evne1dWVPOt0sUjeveJ2H/X09CTPutfHPYdOH9j4+Li12+kOczmdTe5xlEql5Nnl5WVr99zcnDXvdB+594rTweW+f5zzQvcRAKCpCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBIrrmoVCre4rbk1VZthSQVCoXk2Xw+b+1uZr2A8zqdSgzJq62QvHqJ1dVVa3e5XE6edSoXJO++cne7lRvO63Svz+TkZPKs+9507i33uJ2aC6eGQvJrMRzZbNaad75PuPUpzvcsai4AAE1FKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIyUUyw8PD1mKnR8btHXG6XjKZjLW7o6Mjeba7u9va7XQIOf1Okt/Z5JwXt4fJ6QRaWFiwdjt9OYuLi9bu2dlZa965/u71cXp+3A6h9vb25Nmenh5rd61WS56dnp62dju9SpLXOeR2pDnf39xOLee9737vTMEnBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhOTuo6GhIWux0yHkmpqaSp51engkqV6vJ8/mcjlrt9N9ND8/b+12el4krz/K1Wg0mrbb6Rtyr73bw+RcT7efyO0ccji9Ss6s5J/DZnI6ntzvV2tra8mz7nutt7c3edbtSEvBJwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAIbnmwnmsW/Iev15fX7d2DwwMJM86tQiSVK1Wk2fdOodarZY8654Tt+bCkclkrPlsNps829nZae12rufWrVut3c71kaTFxcXk2VKpZO123j/uPe7UfzivUfLOoVud497jTl2EW3Ph1NA431MkaWVlJXm2GbUifFIAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBI7j5y+zuKxWLybE9Pj7V7cHAwebZer1u7T5w4kTy7vLxs7Z6dnU2edTuBXKurq8mzbveR01HT3t5u7Xa0tLQ0bbfk9d8sLS1Zu50Ooa6uLmu3c9xuH5TTB+Z2hw0PD1vzF198cfKs+/3t6NGjybPuPe58z3r++eet3Sn4pAAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgJNdclMtla/Hi4mLybKlUsnY79QVu1cGLL76YPNvMKoq2tuRLI8m/Pmtra8mzbgWAU//h3CeSVKlUrHmH+zrn5uaSZ937cGVlJXnWvVecWoxsNmvtXl9fT5517kFJ6u7utuad8+Jee6cmxq3zcK69M5uKTwoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAjJ5SATExPW4kwm05RZyescam9vt3aPjIwkz3Z0dFi7nY4nt8/G6WKRvP6ber1u7Xbmnf4gyes+auZuSZqenk6edTqBJO8+dN8/zr2yceNGa/fg4GDybKFQsHbXajVr/tixY8mzzrWUpIWFheRZ9/uE8/5x3sep+KQAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAAKhAAAIhAIAICR3KaysrFiLh4aGkmcbjYa12+E+Bt7d3Z08W61Wrd3Oo/SLi4vW7vn5eWt+dHQ0edZ9TN+pIXHOt+RVVzi1Iu5uya+ucCwtLSXPutent7c3edZ9/+Ryuabtds+3U0XhVpw4VSHOOZGkiy66KHnWrfFJwScFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAACE5O6jWq1mLa7X68mza2tr1u5MJpM863brFAqF5Nnl5WVrt3MOy+Wytbu11cv3trbkS6++vj5rt9Nl5fZHOZ1AzrWU/HPodOu49/jg4GDyrNut41x7t5csm80mz7rnu6WlpWnH4pwTyes+cvX39yfPOq8xFZ8UAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQkgs/VlZWrMVOd4/b3dLT05M86/alOF08i4uLTdvtdPxIfgeK0/Xi9ry4nVAO57jd7iO342nLli3Js+3t7dZu57513z/FYjF51u0nco7FPW6n80zyes/ca+90u7n9XnNzc9b82cYnBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAAAhuTMgn89bizs7O5Nn3d1OBYBb0eA8Yj47O2vtHh4eTp516wWcWhFJqtVqybPuOZyfn0+edSsxnPmOjg5r98DAgDW/a9eu5Fnn/SBJJ06cSJ51rqUkra2tJc+69Q/OOXerWdxz6FSLOPUpkleh8sILL1i7jx8/njzrnpMUfFIAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEBILvzo7++3Fq+vryfPVqtVa3epVEqerdfr1m6nt8fpSZK8Hhnn/L2e+cXFxeRZtxPI6ZxxzrckFYtFa97R1dVlzff09CTPuh1CzvVxz6HTl+N26zgdQk6HmeR3PK2srCTPLi0tWbsd3d3d1vzCwkLybDOOm08KAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCAQCgAAEL6M+mmtbW15Nnnn3/e2n3y5MnkWbe6wHn0vlwuW7tnZ2eTZzs6Oqzdbh2BU4vhHLfkPXrvPqbvVAC4tQiFQsGaf+aZZ5Jnh4aGrN3OeXHvQ6f+wZmVvON2qjwk/3pms9nkWbdyI5PJJM/mcjlrd2tr+q/V3XqbpJ//rG8EAPzPIhQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhOTuo7GxMWux05ly/Phxa/fy8nLyrNtp4qhWq9a80/Xi9vC4x+KcQ2dW8jpqSqWStXt1dTV51umnkaRKpWLNj4+PJ8+6HU89PT3Js41Gw9rt9GS599Xc3FzyrPve7Ovrs+ad7iP3/dbWll4bNzk5ae123hN0HwEAmopQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhORntTdv3mwtdh53n5qasnY7FRpuvYDziLlbAZDP55Nn3eqCcrlszReLxeRZty7CqWhwrqUktbe3J8+2tnq/5nHvla6uruTZXC5n7XbOufs6nfoH9/o4VS4jIyPW7v7+fmveqcVw7ivJq5dwr/38/HzyrPu+T8EnBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAARCAQAQCAUAQCAUAACBUAAAhOTuo7a25FFJUnd3d/Lspk2brN1OL4zTwSR5r3NgYMDa7fSluOr1ujXvHEsz+4k6Ozut3c6xDA0NWbvd67lx48bkWbe3x+mPct+bzjlcW1uzdjvH7fZ7OV1GkjQ6Opo863SeSV7HU0tLi7Xb6VRzv7+l4JMCACAQCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgJD8fLxbR+DMu7vdx8YdzmP6Tp2DJBWLxeTZ1dVVa7dbGVAoFJJn3evT2pr+aw33dTpVB261xNvf/nZr3jkvTu2L5B27ex8uLy8nz3Z0dFi7nQqNbDZr7Xbf9y+99FLy7MmTJ63djvn5eWt+cnIyedat50jBJwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITk7qNMJmMtdnph3O4WpxdmaWnJ2r24uJg8W6lUrN1Oj4x7vp3OJsnr4mlrS75NJEmzs7PJs875lrzuo5GREWu3ex86nE4gd97tBHI6h9zda2trybNuH1StVrPmnT4jt58on88nz87NzVm73WM52/ikAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCA0LTuI6f/plqtWru7urqSZzds2GDtXlhYSJ51e3sGBweTZ52eJEkqFArWvNNnVC6Xrd0TExPJs6VSydrtvk6Hez2dXiD3Hl9eXk6edTueNm/enDzrdJhJ3rWv1+vWbrf7KJfLJc+695XTTzQ2NmbtHhoaSp5176sUfFIAAARCAQAQCAUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEJK7DtxHzCuVSvKs+7i7U9HQ2urlXjabbcqsJLW3tyfP5vN5a3d3d7c17zweXywWrd1OLYZzTqTm1nO493h/f3/yrFtbsrKykjzrHrdzrzjVLO6xOFU4krS+vm7NO6/Tff806zgk773fjNoXPikAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAkF8lMT09bi50+I7fTZG1tzZp3XHDBBcmzTg+P5PXZtLS0WLvdc7K8vJw8Ozc3Z+12rr3bTeVwu6nc69nZ2Zk863bU5HK55Fm3J8t5v7n3lXMOnfMn+T1Zq6urybPu96De3t7k2b6+Pmt3T09P8izdRwCApiIUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAIfmZdOfRa8mraXArHRYWFpJn3eoC5/H1TCZj7XaqJdyKhlqtZs1XKpXkWbeKwrlX3BqFrq6u5Fm3RsGp55CkarWaPFsul63dzbwPZ2dnk2fd6zM0NJQ8OzAwYO12qyimpqaSZ51KDMm7x517VvLem83AJwUAQCAUAACBUAAABEIBABAIBQBAIBQAAIFQAAAEQgEAEAgFAEAgFAAAgVAAAITkYqDLLrvMWjw/P5886/b2HDt2LHm2VCpZu53ekeHhYWv36Oho8uzS0pK12znfktc35fYwdXd3J8+6vUpOn5HbCeT23zj3lnt9nHPY0dFh7Xaup9PvJHnnJJ/PW7vdHjNn3u1fa29vT551r4/zfaKvr8/anYJPCgCAQCgAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAACC99y4YX19vSmzklSv15Nn3eqChYWF5Nn+/n5rd6FQSJ51H413zc7OJs9OTk5au50ahYGBAWu3U43gVhesrKxY805Fh1tb4tRLuO8fp0KjXC5bu53X6VSWSH7NhVOf49Z5uNU8ji1btiTPulUhKfikAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAAACoQAACIQCACAQCgCA0NJoNBrn+iAAAOcHPikAAAKhAAAIhAIAIBAKAIBAKAAAAqEAAAiEAgAgEAoAgEAoAADC/wG+oHdINlkIkwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Number of images you want to display\n",
        "num_images_to_display = 5\n",
        "\n",
        "for i in range(num_images_to_display):\n",
        "    # Extract the i-th image tensor\n",
        "    image_tensor = WL_tensor[i, :, :, 0]\n",
        "    # Use TensorFlow operations if needed (optional)\n",
        "    # Display the image\n",
        "    plt.imshow(image_tensor, cmap='gray')\n",
        "    plt.title(f\"Image {i}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb0e17a",
      "metadata": {},
      "source": [
        "#### print out some dimensions of arrays "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f2885b4e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number_subimages_across = 32\n",
            "total number of images = 16384 512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([16384, 32, 32, 1])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"number_subimages_across =\", number_subimages_across )\n",
        "print(\"total number of images =\", number_subimages_across*number_subimages_across*number_fits_files *len(all_directories), number_subimages_total)\n",
        "np.shape(WL_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019e88b1",
      "metadata": {},
      "source": [
        "## functions to calculate statistics of distribution of pixel values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1363e4fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NaNs: 0\n",
            "Number of Infinities: 0\n",
            "Min value: -1.1941020488739014\n",
            "Max value: 2.4392032623291016\n",
            "Tensor shape: (17, 32, 32, 1)\n",
            "Mean value: 0.4890615940093994\n",
            "Variance value: 0.17789484560489655\n",
            "Mean value: 0.48906160283867894\n",
            "Variance value: 0.17790506583065654\n",
            "Standard Deviation: 0.42177581787109375\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Implement Welford's algorithm for numerically stable mean and variance calculation\n",
        "def welford_algorithm(data):\n",
        "    n = 0\n",
        "    mean = 0.0\n",
        "    M2 = 0.0\n",
        "\n",
        "    for x in data:\n",
        "        n += 1\n",
        "        delta = x - mean\n",
        "        mean += delta / n\n",
        "        delta2 = x - mean\n",
        "        M2 += delta * delta2\n",
        "\n",
        "    variance = M2 / (n - 1) if n > 1 else float('nan')\n",
        "    return mean, variance\n",
        "\n",
        "calculate_statistics = True\n",
        "\n",
        "if calculate_statistics:\n",
        "    # Assuming WL_tensor is a TensorFlow tensor of floating-point numbers\n",
        "\n",
        "    # Convert the TensorFlow tensor to a NumPy array\n",
        "    WL_tensor_np = WL_tensor.numpy()\n",
        "    WL_tensor_np = WL_tensor_np[::1000, :, :, :]  # Downsample the tensor for faster computation\n",
        "\n",
        "    # Check for NaNs and Infinities\n",
        "    num_nans = np.isnan(WL_tensor_np).sum()\n",
        "    num_infs = np.isinf(WL_tensor_np).sum()\n",
        "    print(f\"Number of NaNs: {num_nans}\")\n",
        "    print(f\"Number of Infinities: {num_infs}\")\n",
        "\n",
        "\n",
        "    # Inspect the range of values\n",
        "    min_value = WL_tensor_np.min()\n",
        "    max_value = WL_tensor_np.max()\n",
        "    print(f\"Min value: {min_value}\")\n",
        "    print(f\"Max value: {max_value}\")\n",
        "\n",
        "    # Check the shape of the tensor\n",
        "    tensor_shape = WL_tensor_np.shape\n",
        "    print(f\"Tensor shape: {tensor_shape}\")\n",
        "\n",
        "    # Manually calculate the mean and variance\n",
        "    mean_value = np.mean(WL_tensor_np)\n",
        "    variance_value = np.var(WL_tensor_np)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Flatten the tensor to 1D for easier processing\n",
        "    WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate mean and variance using Welford's algorithm\n",
        "    mean_value, variance_value = welford_algorithm(WL_tensor_flat)\n",
        "    print(f\"Mean value: {mean_value}\")\n",
        "    print(f\"Variance value: {variance_value}\")\n",
        "\n",
        "\n",
        "    # Calculate the standard deviation\n",
        "    std_dev = np.std(WL_tensor_np)\n",
        "    print(f\"Standard Deviation: {std_dev}\")\n",
        "\n",
        "    DO_KDE= True\n",
        "    if DO_KDE:\n",
        "        # Flatten the tensor to 1D for PDF calculation\n",
        "        WL_tensor_flat = WL_tensor_np.flatten()\n",
        "\n",
        "        # Calculate the PDF using Gaussian Kernel Density Estimation\n",
        "        kde = gaussian_kde(WL_tensor_flat)\n",
        "        x_values = np.linspace(WL_tensor_flat.min(), WL_tensor_flat.max(), 1000)\n",
        "        pdf_values = kde(x_values)\n",
        "\n",
        "        # Plot the PDF\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.semilogx(x_values, pdf_values, label='PDF')\n",
        "        plt.hist(WL_tensor_flat, bins=50, density=True, alpha=0.6, color='g', label='Histogram')\n",
        "        plt.title('Probability Density Function of WL_tensor')\n",
        "        plt.xlabel('Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a635a7",
      "metadata": {},
      "source": [
        "# Motivated binning method that partitions so that variance in each bin should be the same\n",
        "\n",
        "really we want to learn the bins and not choose them from the start (in LLMs this is the tokenization step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2439072c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Variance: 0.010000000000000002\n",
            "Target Variance per Bin: 0.00020000000000000004\n",
            "num bins= 15\n",
            "Target Variance: 0.010000000000000002\n",
            "Target Variance per Bin: 0.0006666666666666668\n",
            "num bins= 10\n",
            "Target Variance: 0.010000000000000002\n",
            "Target Variance per Bin: 0.0010000000000000002\n",
            "num bins= 9\n",
            "Target Variance: 0.010000000000000002\n",
            "Target Variance per Bin: 0.0011111111111111113\n",
            "num bins= 8\n",
            "Target Variance: 0.010000000000000002\n",
            "Target Variance per Bin: 0.0012500000000000002\n",
            "num bins= 8\n",
            "done with calculating number of bins\n",
            "mean values =  [-0.1482372, 0.23132344, 0.48518446, 0.7403438, 1.0323923, 1.428342, 2.0469747]\n",
            "bin_edges =  [-0.27657598  0.08495405  0.3580989   0.61281127  0.88676727  1.2324514\n",
            "  1.8156495   3.4240055 ]\n",
            "num_points =  [346527, 1262608, 2060749, 2340940, 2043048, 1330312, 551527]\n",
            "total_variance_bin = [0.0012500027011464942, 0.0012500002575710613, 0.0012500002435982258, 0.0012500014692704898, 0.001250001326376575, 0.0012500004766762308, 0.0012500107019120814]\n",
            "##########################################\n",
            "Standard Deviation of Quantization Error: 0.11677048 0.11676547 0.09354152648183137\n",
            "##########################################\n",
            "tf.Tensor(\n",
            "[ 0.23132344  0.23132344 -0.1482372  -0.1482372  -0.1482372   0.23132344\n",
            "  0.23132344  0.23132344  0.23132344 -0.1482372 ], shape=(10,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.14921579  0.13201605  0.04518863 -0.00887103  0.07779365  0.16029042\n",
            "  0.30966666  0.33476958  0.2154874   0.07893117], shape=(10,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[ 0.08210765  0.09930739 -0.19342583 -0.13936616 -0.22603086  0.07103302\n",
            " -0.07834323 -0.10344614  0.01583603 -0.22716837], shape=(10,), dtype=float32)\n",
            "[346527, 1262608, 2060749, 2340940, 2043048, 1330312, 551527] 9935711\n",
            "[0.0012500027011464942, 0.0012500002575710613, 0.0012500002435982258, 0.0012500014692704898, 0.001250001326376575, 0.0012500004766762308, 0.0012500107019120814] 0.008750017176551158\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Flatten the tensor to 1D for easier processing\n",
        "WL_tensor_flat =  tf.reshape(WL_tensor, [-1]) # WL_tensor_np.flatten()\n",
        "\n",
        "# Ensure WL_tensor_flat is a NumPy array\n",
        "if isinstance(WL_tensor_flat, tf.Tensor):\n",
        "    WL_tensor_flat_np = WL_tensor_flat.numpy()\n",
        "else:\n",
        "    WL_tensor_flat_np = WL_tensor_flat\n",
        "\n",
        "# Step 1: Sample a Subset of the Data\n",
        "sample_size = 10_000_000  # Adjust based on available memory and desired accuracy\n",
        "\n",
        "if sample_size  > len(WL_tensor_flat_np):\n",
        "    sampled_data = WL_tensor_flat_np\n",
        "else:\n",
        "    indices = np.random.choice(len(WL_tensor_flat_np), size=sample_size, replace=False)\n",
        "    sampled_data = WL_tensor_flat_np[indices]\n",
        "\n",
        "\n",
        "# Step 2: Sort the Sampled Data\n",
        "sorted_data = np.sort(sampled_data)\n",
        "n = len(sorted_data)\n",
        "\n",
        "\n",
        "#print(\"num zeros = \", np.sum(sampled_data == 0), np.sum(WL_tensor_flat_np == 0), np.sum(sorted_data ==0))\n",
        "\n",
        "#print(\"sorted_data =\", sorted_data)\n",
        "\n",
        "# Step 3: Compute Cumulative Sums and Cumulative Sum of Squares\n",
        "cum_sum = np.cumsum(sorted_data, dtype=float)\n",
        "cum_sum_sq = np.cumsum(sorted_data**2, dtype=float)\n",
        "\n",
        "# Step 4: Compute Total Variance and Target Variance per Bin\n",
        "target_std = .1\n",
        "target_variance = target_std**2\n",
        "num_bins= 50 # just a guess to start\n",
        "MIN_BIN_SIZE = 10  # Adjust based on your data size and acceptable error\n",
        "\n",
        "\n",
        "for iiter in range(5): # iterate five times to reach target std\n",
        "\n",
        "    target_variance_per_bin = target_variance/ num_bins\n",
        "\n",
        "    print(f\"Target Variance: {target_variance}\")\n",
        "    print(f\"Target Variance per Bin: {target_variance_per_bin}\")\n",
        "\n",
        "\n",
        "    # Step 5: Find Bin Edges\n",
        "    bin_edges = []\n",
        "    num_points_arr = []\n",
        "    total_variance_bin_arr = []\n",
        "    mean_bin_arr = []\n",
        "\n",
        "    start_index = 0\n",
        "    while start_index < n:\n",
        "        found = False\n",
        "        for end_index in range(start_index + MIN_BIN_SIZE, n):\n",
        "            num_points = end_index - start_index + 1\n",
        "            #if num_points <= 3:\n",
        "            #    continue\n",
        "            # Sum and sum of squares in the bin\n",
        "            sum_bin = cum_sum[end_index] - (cum_sum[start_index - 1] if start_index > 0 else 0)\n",
        "            sum_sq_bin = cum_sum_sq[end_index] - (cum_sum_sq[start_index - 1] if start_index > 0 else 0)\n",
        "            # Mean and variance in the bin\n",
        "            mean_bin = sum_bin / num_points\n",
        "            total_variance_bin = (sum_sq_bin - num_points*mean_bin**2)/(num_points-1)*num_points/n  #total contribute to variance\n",
        "            if total_variance_bin >= target_variance_per_bin:\n",
        "                bin_edges.append(sorted_data[end_index])\n",
        "                num_points_arr.append(num_points)\n",
        "                total_variance_bin_arr.append(total_variance_bin)\n",
        "                mean_bin_arr.append(mean_bin)\n",
        "                #print(\"sums = \",  sum_bin , sum_sq_bin, mean_bin, sum_sq_bin / num_points, num_points, \"total_variance_bin = \", total_variance_bin, target_variance_per_bin, num_points/n, ((sum_sq_bin / num_points) - mean_bin**2) )\n",
        "                #print(\"indexes =\", start_index, end_index, cum_sum[end_index], cum_sum_sq[start_index - 1]) \n",
        "                start_index = end_index + 1\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            # Include all remaining data in the last bin\n",
        "            bin_edges.append(tf.reduce_max(WL_tensor)) #       sorted_data[-1])\n",
        "            break\n",
        "\n",
        "    num_bins = len(bin_edges)\n",
        "    print(\"num bins=\", num_bins)\n",
        "\n",
        "print(\"done with calculating number of bins\")\n",
        "\n",
        "# Step 6: Digitize the Original Data Using Bin Edges\n",
        "# Convert bin_edges to numpy array for consistency\n",
        "bin_edges = np.array(bin_edges)\n",
        "\n",
        "# Use np.digitize to find bin indices for the entire dataset\n",
        "bin_indices = np.digitize(WL_tensor_flat_np, bin_edges, right=True)-1\n",
        "\n",
        "# Adjust bin_indices to ensure they are within the valid range\n",
        "# Since mean_values has length len(bin_edges) + 1, bin_indices should be in [0, len(bin_edges)]\n",
        "#may be necessary since our bins are not based on everything\n",
        "bin_indices = np.clip(bin_indices, 0, len(bin_edges))\n",
        "\n",
        "\n",
        "# Digitize the tensor values according to the binning scheme\n",
        "#bin_indices = tf.searchsorted(bin_edges, WL_tensor_flat, side='right') - 1\n",
        "\n",
        "\n",
        "#let's calculate the mean we expect in each bin\n",
        "#MIGHT NEED TO GAURD AGAINST ZEROS\n",
        "mean_values = [np.mean(WL_tensor_flat[bin_indices == index]) for index in range(len(bin_edges)-1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"mean values = \", mean_values)\n",
        "print(\"bin_edges = \", bin_edges)\n",
        "print(\"num_points = \", num_points_arr)\n",
        "print(\"total_variance_bin =\", total_variance_bin_arr)\n",
        "\n",
        "encoded_tensor = tf.gather(mean_values, bin_indices) #realization of quantized tensor\n",
        "\n",
        "\n",
        "#reshape\n",
        "bin_indices = tf.reshape(bin_indices, [number_images,  sub_image_size*sub_image_size])\n",
        "\n",
        "\n",
        "# Compute mean values\n",
        "# mean_values = np.zeros(len(bin_edges) + 1)\n",
        "# for i in range(len(mean_values)):\n",
        "#     indices_in_bin = bin_indices == i\n",
        "#     if np.any(indices_in_bin):\n",
        "#         mean_values[i] = np.mean(WL_tensor_flat_np[indices_in_bin])\n",
        "#     else:\n",
        "#         # Handle empty bins if necessary\n",
        "#         mean_values[i] = 0\n",
        "\n",
        "# # Map bin indices to mean values\n",
        "# encoded_tensor_np = mean_values[bin_indices]\n",
        "\n",
        "# # If you need to convert back to a TensorFlow tensor\n",
        "# encoded_tensor = tf.convert_to_tensor(encoded_tensor_np, dtype=WL_tensor_flat.dtype)\n",
        "\n",
        "# # Compute Quantization Error\n",
        "# \n",
        "# \n",
        "diff_tensor = encoded_tensor - WL_tensor_flat\n",
        "\n",
        "# Compute the standard deviation of the quantization error\n",
        "std_quantized = tf.math.reduce_std(diff_tensor).numpy()\n",
        "\n",
        "# Convert indices to a TensorFlow tensor\n",
        "indices_tf = tf.constant(indices, dtype=tf.int32)\n",
        "# Use tf.gather to index diff_tensor\n",
        "std_quantized_sampled = tf.math.reduce_std(tf.gather(diff_tensor, indices_tf)).numpy()\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Standard Deviation of Quantization Error:\", std_quantized, std_quantized_sampled, np.sum(total_variance_bin_arr)**.5)\n",
        "print(\"##########################################\")\n",
        "# If needed, reshape encoded_tensor back to the original tensor shape\n",
        "# encoded_tensor = tf.reshape(encoded_tensor, WL_tensor.shape)\n",
        "\n",
        "#just to check quantization works\n",
        "print(encoded_tensor[-10:])\n",
        "print(WL_tensor_flat[-10:])\n",
        "\n",
        "\n",
        "print(diff_tensor[-10:])\n",
        "\n",
        "\n",
        "print(num_points_arr, np.sum(num_points_arr))\n",
        "print(total_variance_bin_arr, np.sum(total_variance_bin_arr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "684c8c58",
      "metadata": {},
      "source": [
        "# Autoregressive image transformer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ab491d",
      "metadata": {},
      "source": [
        "## Autoregressive transformer with sinusoidal 2D positional encoding\n",
        "\n",
        "The model uses sinusoidal positional encodings (combining sines and cosines at different frequencies)\n",
        "to encode 2D spatial position information, sampled up to the Nyquist frequency.\n",
        "This variant compares performance across two model depths: a deeper and a shallower architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "98c92482",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_20           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_22           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_23           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span>  add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m512\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_2 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m,             \u001b[38;5;34m0\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_12 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_14 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_15 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_20           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_13 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_22           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_14 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_16 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_17 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_23           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_15 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_18 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)           \u001b[38;5;34m520\u001b[0m  add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,000</span> (394.53 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,000\u001b[0m (394.53 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,000</span> (394.53 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,000\u001b[0m (394.53 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_3        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_3          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_encodin  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  positional_encod \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_25           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_encod \n",
              "                                                     dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_26           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_28           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_29           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_31           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_32           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_34           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span>  layer_normalizat \n",
              "\n",
              " dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span>  dense_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_35           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span>  add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_3        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_3          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m512\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_encodin  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEncodin\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  positional_encod \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_3 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;45mNone\u001b[0m,             \u001b[38;5;34m0\u001b[0m  input_layer_3[\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_25           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_16 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_encod \n",
              "                                                     dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_19 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_20 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_26           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_17 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_28           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_18 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_22 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_29           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_19 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_31           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_20 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_23 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_24 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_32           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_21 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_34           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_22 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m128\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_25 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      \u001b[38;5;34m16,640\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_26 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)       \u001b[38;5;34m16,448\u001b[0m  dense_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_35           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_23 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_27 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)           \u001b[38;5;34m520\u001b[0m  add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,968</span> (785.03 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m200,968\u001b[0m (785.03 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">200,968</span> (785.03 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m200,968\u001b[0m (785.03 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  #don't think need L1 and dropout  (I've tested and it performs best with this equal to zero)\n",
        "\n",
        "num_classes = num_bins  # Set num_classes to num_bins: this is the number of values that can be populated\n",
        "d_model = number_channels*2  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "# Custom Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, height, width, d_model, **kwargs):\n",
        "        super(PositionalEncoding, self).__init__(**kwargs)\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.d_model = d_model\n",
        "        self.supports_masking = True  # Enable masking support\n",
        "        self.pos_encoding = self.positional_encoding(height, width, d_model)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEncoding, self).get_config()\n",
        "        config.update({\n",
        "            'height': self.height,\n",
        "            'width': self.width,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_angles(self, pos):\n",
        "        num_frequencies = self.d_model // 2\n",
        "        frequencies = tf.linspace(0.0, np.pi, num_frequencies)\n",
        "        frequencies = tf.cast(frequencies, tf.float32)\n",
        "        angle_rates = frequencies[tf.newaxis, :]  # Shape: (1, num_frequencies)\n",
        "        return pos * angle_rates  # pos: (positions, 1), angle_rates: (1, num_frequencies)\n",
        "\n",
        "    def positional_encoding(self, height, width, d_model):\n",
        "        position_x = tf.range(width, dtype=tf.float32)[:, tf.newaxis]  # Shape: (width, 1)\n",
        "        position_y = tf.range(height, dtype=tf.float32)[:, tf.newaxis]  # Shape: (height, 1)\n",
        "\n",
        "        angles_x = self.get_angles(position_x)  # Shape: (width, num_frequencies)\n",
        "        angles_y = self.get_angles(position_y)  # Shape: (height, num_frequencies)\n",
        "\n",
        "        sines_x = tf.math.sin(angles_x)\n",
        "        cosines_x = tf.math.cos(angles_x)\n",
        "        sines_y = tf.math.sin(angles_y)\n",
        "        cosines_y = tf.math.cos(angles_y)\n",
        "\n",
        "        pos_encoding_x = tf.concat([sines_x, cosines_x], axis=-1)  # Shape: (width, d_model)\n",
        "        pos_encoding_y = tf.concat([sines_y, cosines_y], axis=-1)  # Shape: (height, d_model)\n",
        "\n",
        "        pos_encoding_x = pos_encoding_x[tf.newaxis, :, :]  # Shape: (1, width, d_model)\n",
        "        pos_encoding_y = pos_encoding_y[:, tf.newaxis, :]  # Shape: (height, 1, d_model)\n",
        "\n",
        "        pos_encoding = pos_encoding_y + pos_encoding_x  # Shape: (height, width, d_model)\n",
        "\n",
        "        pos_encoding = tf.reshape(pos_encoding, [1, height * width, d_model])\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Ensure the positional encoding matches the input sequence length\n",
        "        return inputs + self.pos_encoding[:, :seq_length, :]\n",
        "\n",
        "'''\n",
        "# Function to create the Transformer model\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    seq_length = height * width - 1  # Subtract 1 for autoregressive prediction\n",
        "    #inputs = layers.Input(shape=(seq_length,))  # Input tokens are integers\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)  # Accept variable-length sequences\n",
        "    x = inputs\n",
        "\n",
        "    # Embed the input tokens (indices)\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model, mask_zero=True)(x)  # Shape: (batch_size, seq_length, d_model)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Create the causal mask once as a constant tensor\n",
        "    #causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    #causal_mask = tf.cast(causal_mask, dtype=tf.bool)\n",
        "\n",
        "    def create_causal_mask(x):\n",
        "        seq_length = tf.shape(x)[1]\n",
        "        causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "        return causal_mask\n",
        "\n",
        "    causal_mask = layers.Lambda(create_causal_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with causal masking\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=causal_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "'''\n",
        "\n",
        "# Function to create the combined attention mask\n",
        "def create_attention_mask(inputs):\n",
        "    # Create padding mask (1 for valid tokens, 0 for padding tokens)\n",
        "    padding_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)\n",
        "    padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]  # Shape: (batch_size, 1, 1, seq_length)\n",
        "\n",
        "    # Create causal mask (lower triangular matrix)\n",
        "    seq_length = tf.shape(inputs)[1]\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
        "    causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # Shape: (1, 1, seq_length, seq_length)\n",
        "\n",
        "    # Combine masks: only attend to previous tokens and non-padding tokens\n",
        "    attention_mask = tf.cast(causal_mask, tf.float32) * padding_mask\n",
        "\n",
        "    return attention_mask  # Shape: (batch_size, 1, seq_length, seq_length)\n",
        "\n",
        "# Function to create the Transformer model with manual attention mask\n",
        "def create_autoregressive_transformer(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Remove mask_zero to prevent automatic masking\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(inputs)\n",
        "\n",
        "    # Apply positional encoding\n",
        "    x = PositionalEncoding(height, width, d_model)(x)\n",
        "\n",
        "    # Manually create the attention mask\n",
        "    attention_mask = layers.Lambda(create_attention_mask)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with manual attention mask\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=attention_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "seq_length = sub_image_size * sub_image_size\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_deep = create_autoregressive_transformer(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers*2,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "\n",
        "autoregressive_transformer.summary()\n",
        "\n",
        "autoregressive_transformer_deep.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f6de246",
      "metadata": {},
      "source": [
        "### Compile transformer models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2cb4fc3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_deep.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bb1a6b2",
      "metadata": {},
      "source": [
        "### Train transformer models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "650102df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-05 10:11:24.649566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 532ms/step - accuracy: 0.6317 - loss: 0.9835 - val_accuracy: 0.6766 - val_loss: 0.7775\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 592ms/step - accuracy: 0.6836 - loss: 0.7515 - val_accuracy: 0.7158 - val_loss: 0.6605\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 621ms/step - accuracy: 0.7151 - loss: 0.6622 - val_accuracy: 0.7289 - val_loss: 0.6302\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 593ms/step - accuracy: 0.7250 - loss: 0.6393 - val_accuracy: 0.7329 - val_loss: 0.6215\n",
            "Epoch 5/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 662ms/step - accuracy: 0.7296 - loss: 0.6285 - val_accuracy: 0.7363 - val_loss: 0.6137\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 446ms/step - accuracy: 0.7363 - loss: 0.6135\n",
            "Validation Loss: 0.6137  bits per pixel : 0.8854\n",
            "Validation Accuracy: 0.7363\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b344ac9",
      "metadata": {},
      "source": [
        "### Let's fit the deep model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "49bec05f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1124s\u001b[0m 1s/step - accuracy: 0.0447 - loss: 3.9281 - val_accuracy: 0.0796 - val_loss: 3.1744\n",
            "Epoch 2/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1134s\u001b[0m 1s/step - accuracy: 0.0798 - loss: 3.1678 - val_accuracy: 0.0901 - val_loss: 3.0447\n",
            "Epoch 3/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1108s\u001b[0m 1s/step - accuracy: 0.0882 - loss: 3.0651 - val_accuracy: 0.0948 - val_loss: 2.9914\n",
            "Epoch 4/5\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1475s\u001b[0m 2s/step - accuracy: 0.0926 - loss: 3.0179 - val_accuracy: 0.0973 - val_loss: 2.9681\n",
            "Epoch 5/5\n",
            "\u001b[1m 87/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21:23\u001b[0m 2s/step - accuracy: 0.0942 - loss: 2.9992"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mautoregressive_transformer_deep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust epochs as needed\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m autoregressive_transformer\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_deep.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=5,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a3d9481",
      "metadata": {},
      "source": [
        "# Visualize reconstructed images after autoregressive compression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba17c87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "#Much slower code that I don't use anymore\n",
        "def autoregressive_predict(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model, one step at a time,\n",
        "    using the original sequence up to the current position as context.\n",
        "    \"\"\"\n",
        "    predicted_sequence = []\n",
        "\n",
        "    for i in range(1, sequence_length+1):\n",
        "        # Prepare the context (all previous pixels)\n",
        "        context = original_sequence[:i]\n",
        "\n",
        "        # No padding required; model expects input of length seq_length\n",
        "        # For positions where context is shorter, we need to pad or adjust the input\n",
        "        # Since we're avoiding padding, we'll adjust the input sequence accordingly\n",
        "\n",
        "        # Create a context of length 'i'\n",
        "        model_input = np.array(context, dtype=np.int32)[np.newaxis, :]\n",
        "\n",
        "        # Since the model expects input of shape (batch_size, seq_length),\n",
        "        # we need to handle the varying lengths\n",
        "        # One way is to slice the model to accept variable input lengths\n",
        "\n",
        "        # Predict\n",
        "        next_pixel_probs = model.predict(model_input, verbose=0)\n",
        "        # Get the prediction for the current position\n",
        "        next_pixel = np.argmax(next_pixel_probs[0, -1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Step {i}: Context length: {len(context)}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "#should be much faster....need to check\n",
        "def autoregressive_predict_batched(model, original_sequence, sequence_length):\n",
        "    \"\"\"\n",
        "    Generate predictions using the autoregressive model by batching inputs,\n",
        "    which significantly speeds up the prediction process.\n",
        "    \"\"\"\n",
        "    # Prepare all contexts at once\n",
        "    contexts = [original_sequence[:i] for i in range(1, sequence_length + 1)]\n",
        "\n",
        "    # Pad sequences to the same length\n",
        "    max_len = sequence_length\n",
        "    padded_contexts = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        contexts, maxlen=max_len, padding='post', value=0\n",
        "    )\n",
        "\n",
        "    # Convert to array\n",
        "    model_inputs = np.array(padded_contexts, dtype=np.int32)\n",
        "\n",
        "    # Predict all at once\n",
        "    next_pixel_probs = model.predict(model_inputs, verbose=0)\n",
        "\n",
        "    # Extract the predictions\n",
        "    predicted_sequence = []\n",
        "    for i, context in enumerate(contexts):\n",
        "        # Get the prediction for the current position\n",
        "        seq_length = len(context)\n",
        "        next_pixel = np.argmax(next_pixel_probs[i, seq_length - 1, :])\n",
        "        predicted_sequence.append(next_pixel)\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Step {i + 1}: Context length: {seq_length}, Predicted next pixel: {next_pixel}\")\n",
        "\n",
        "    return np.array(predicted_sequence)\n",
        "\n",
        "\n",
        "def process_random_images(X_val, y_val, model, sub_image_size, num_samples=5):\n",
        "    \"\"\"\n",
        "    Process and visualize random images from the validation set.\n",
        "    \"\"\"\n",
        "    total_samples = len(X_val)\n",
        "    random_indices = np.random.choice(total_samples, num_samples, replace=False)\n",
        "    \n",
        "    \n",
        "    for i, idx in enumerate(random_indices):\n",
        "        val_sample = X_val[idx]\n",
        "        original_sequence = y_val[idx]\n",
        "        \n",
        "        # Determine the sequence length\n",
        "        sequence_length = len(original_sequence)\n",
        "        \n",
        "        # Predict using the original sequence as context\n",
        "        predicted_sequence = autoregressive_predict_batched(\n",
        "            model,\n",
        "            original_sequence,\n",
        "            sequence_length,\n",
        "        )\n",
        "        \n",
        "        print(\"val_sample and original_sequence shapes\", np.shape(val_sample), np.shape(original_sequence), np.shape(predicted_sequence))\n",
        "    \n",
        "        visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, i+1)\n",
        "        \n",
        "        mse = np.mean((original_sequence - predicted_sequence)**2)\n",
        "        mae = np.mean(np.abs(original_sequence - predicted_sequence))\n",
        "        print(f\"Sample {i+1}:\")\n",
        "        print(f\"  Mean Squared Error: {mse:.4f}\")\n",
        "        print(f\"  Mean Absolute Error: {mae:.4f}\")\n",
        "        print(f\"  Original sequence shape: {original_sequence.shape}\")\n",
        "        print(f\"  Predicted sequence shape: {predicted_sequence.shape}\")\n",
        "        print(f\" fraction of pixles correct\", np.sum(original_sequence - predicted_sequence == 0)/len(original_sequence))\n",
        "        print(\"-----------------------------\")\n",
        "\n",
        "\n",
        "# The visualize_autoregressive_prediction function remains the same\n",
        "\n",
        "def visualize_autoregressive_prediction(original_sequence, predicted_sequence, sub_image_size, index):\n",
        "    \"\"\"\n",
        "    Visualize the original, predicted, and difference images with equal sizes.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
        "    fig.suptitle(f'Sample {index}', fontsize=16)\n",
        "\n",
        "    # Prepare images\n",
        "    original_2d = sequence_to_image(original_sequence, sub_image_size, original=True)\n",
        "    predicted_2d = sequence_to_image(predicted_sequence, sub_image_size, original=False)\n",
        "    diff = original_2d - predicted_2d\n",
        "\n",
        "    # Set up common parameters for imshow\n",
        "    imshow_args = {'interpolation': 'nearest', 'aspect': 'equal'}\n",
        "\n",
        "    # Visualize original sequence\n",
        "    im1 = axes[0].imshow(original_2d, cmap='gray', **imshow_args)\n",
        "    axes[0].set_title('Original Image')\n",
        "    plt.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize predicted sequence\n",
        "    im2 = axes[1].imshow(predicted_2d, cmap='gray', **imshow_args)\n",
        "    axes[1].set_title('Predicted Image (Autoregressive)')\n",
        "    plt.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Visualize difference\n",
        "    im3 = axes[2].imshow(diff, cmap='bwr', **imshow_args)\n",
        "    axes[2].set_title('Difference (Original - Predicted)')\n",
        "    plt.colorbar(im3, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Remove axis ticks for cleaner look\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def sequence_to_image(sequence, sub_image_size, original=True):\n",
        "    \"\"\"\n",
        "    Convert a 1D sequence of length 1023 to a 2D image of size sub_image_size x sub_image_size,\n",
        "    by prepending the first_token to the sequence.\n",
        "    \"\"\"\n",
        "    # Concatenate the first token to the sequence\n",
        "    if original:\n",
        "        full_sequence = np.concatenate((sequence, [0]))\n",
        "    else:\n",
        "        full_sequence = np.concatenate(([0], sequence))\n",
        "    # Reshape to image\n",
        "    image = full_sequence.reshape(sub_image_size, sub_image_size)\n",
        "    return image\n",
        "\n",
        "# Run the generalized code\n",
        "num_samples_to_visualize = 3  # Change this to the number of random samples you want to visualize\n",
        "process_random_images(X_val, y_val, autoregressive_transformer, sub_image_size, num_samples_to_visualize)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc300667",
      "metadata": {},
      "source": [
        "### Plot probability distribution of predicted pixel values\n",
        "\n",
        "Visualizes the model's predicted probability distribution for each pixel value, \n",
        "comparing predicted vs. actual values using bar charts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "48d23252",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'autoregressive_transformer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Plot predictions for some sequences\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m plot_predictions(\u001b[43mautoregressive_transformer\u001b[49m, train_sequences, train_targets, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'autoregressive_transformer' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot predictions vs actual values\n",
        "def plot_predictions(model, sequences, targets, num_samples=10):\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(sequences[:num_samples])\n",
        "    predicted_pixels = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    # Plot the actual vs predicted pixel values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(2, num_samples, i + 1)\n",
        "        plt.imshow(sequences[i].reshape(-1, 1), cmap='gray', aspect='auto')\n",
        "        plt.title(f\"Sequence {i+1}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(2, num_samples, num_samples + i + 1)\n",
        "        plt.bar(range(num_classes), predictions[i])\n",
        "        plt.axvline(x=targets[i], color='r', linestyle='--')\n",
        "        plt.axvline(x=predicted_pixels[i], color='g', linestyle='--')\n",
        "        plt.title(f\"True: {targets[i]}, Pred: {predicted_pixels[i]}\")\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Probability')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot predictions for some sequences\n",
        "plot_predictions(autoregressive_transformer, train_sequences, train_targets, num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489870fe",
      "metadata": {},
      "source": [
        "## This transformer uses a learned positional embedding\n",
        "\n",
        "The positional embedding adds learned position information to the input sequence. \n",
        "It's trainable and has shape (seq_length, d_model), allowing the model to learn \n",
        "position-specific patterns different from the standard sinusoidal positional encoding for the previous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6cb767fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of bin_indices: (16384, 1024)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " embedding_2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                                                           \n",
              "\n",
              " positional_embeddi  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">32,736</span>  embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi</span>                                                   \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  positional_embed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "                      <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                                            \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_19           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  positional_embed \n",
              "                                                     dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_20           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " multi_head_attenti  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio</span>                                 layer_normalizat \n",
              "                                                     lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_22           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  multi_head_atten \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " layer_normalizatio  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio</span>                                                   \n",
              "\n",
              " dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span>  layer_normalizat \n",
              "\n",
              " dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  dense_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dropout_23           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                                                             \n",
              "\n",
              " add_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  add_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     \n",
              "                                                     dropout_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  \n",
              "\n",
              " dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>  add_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " embedding_2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)          \u001b[38;5;34m288\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mEmbedding\u001b[0m)                                                           \n",
              "\n",
              " positional_embeddi  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)       \u001b[38;5;34m32,736\u001b[0m  embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mPositionalEmbeddi\u001b[0m                                                   \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  positional_embed \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " lambda_2 (\u001b[38;5;33mLambda\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,                \u001b[38;5;34m0\u001b[0m  input_layer_2[\u001b[38;5;34m0\u001b[0m] \n",
              "                      \u001b[38;5;45mNone\u001b[0m)                                            \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_19           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_12 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  positional_embed \n",
              "                                                     dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_14 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_15 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_20           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_13 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " multi_head_attenti  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              " (\u001b[38;5;33mMultiHeadAttentio\u001b[0m                                 layer_normalizat \n",
              "                                                     lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_22           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  multi_head_atten \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_14 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " layer_normalizatio  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)           \u001b[38;5;34m64\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mLayerNormalizatio\u001b[0m                                                   \n",
              "\n",
              " dense_16 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m4,224\u001b[0m  layer_normalizat \n",
              "\n",
              " dense_17 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        \u001b[38;5;34m4,128\u001b[0m  dense_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dropout_23           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  dense_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              " (\u001b[38;5;33mDropout\u001b[0m)                                                             \n",
              "\n",
              " add_15 (\u001b[38;5;33mAdd\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m0\u001b[0m  add_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     \n",
              "                                                     dropout_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  \n",
              "\n",
              " dense_18 (\u001b[38;5;33mDense\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)           \u001b[38;5;34m297\u001b[0m  add_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,729</span> (229.41 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,729\u001b[0m (229.41 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,729</span> (229.41 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,729\u001b[0m (229.41 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the network\n",
        "n_trans_layers = 2  # Number of Transformer layers\n",
        "number_channels = sub_image_size  # Embedding dimension (d_model)\n",
        "act_string = 'relu'\n",
        "dropout_rate = 0.1\n",
        "L1weight = 0  # zero is no regularizer (at least with set positional encoding performs better without regularization)\n",
        "num_classes = num_bins+1  # Number of quantization bins\n",
        "d_model = number_channels  # Embedding dimension\n",
        "d_ff = d_model * 4  # Feed-forward network dimension\n",
        "num_heads = 8\n",
        "\n",
        "# Conditionally add L1 regularizer if L1weight is greater than 0\n",
        "if L1weight > 0:\n",
        "    regularizer = regularizers.l1(L1weight)\n",
        "else:\n",
        "    regularizer = None\n",
        "\n",
        "class PositionalEmbedding_learned(tf.keras.layers.Layer):\n",
        "    def __init__(self, seq_length, d_model, **kwargs):\n",
        "        super(PositionalEmbedding_learned, self).__init__(**kwargs)\n",
        "        self.seq_length = seq_length\n",
        "        self.d_model = d_model\n",
        "        self.position_embeddings = self.add_weight(\n",
        "            shape=(seq_length, d_model),\n",
        "            initializer='random_uniform',\n",
        "            trainable=True,\n",
        "            name='position_embeddings'\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding_learned, self).get_config()\n",
        "        config.update({\n",
        "            'seq_length': self.seq_length,\n",
        "            'd_model': self.d_model,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "        # Slice the positional embeddings to match the input sequence length\n",
        "        position_embeddings = self.position_embeddings[:seq_len, :]\n",
        "        position_embeddings = tf.expand_dims(position_embeddings, axis=0)\n",
        "        return inputs + position_embeddings\n",
        "\n",
        "\n",
        "def create_attention_mask2(inputs):\n",
        "    # Create padding mask: 1 for valid tokens, 0 for padding tokens\n",
        "    padding_mask = tf.cast(tf.math.not_equal(inputs, 0), tf.float32)  # Shape: (batch_size, seq_length)\n",
        "\n",
        "    # Create causal mask: lower triangular matrix\n",
        "    seq_length = tf.shape(inputs)[1]\n",
        "    causal_mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)  # Shape: (seq_length, seq_length)\n",
        "\n",
        "    # Combine masks: only attend to previous tokens and non-padding tokens\n",
        "    attention_mask = tf.expand_dims(padding_mask, axis=1) * causal_mask  # Shape: (batch_size, seq_length, seq_length)\n",
        "\n",
        "    return attention_mask  # Shape: (batch_size, seq_length, seq_length)\n",
        "\n",
        "\n",
        "def create_autoregressive_transformer_freeposemb(height, width, n_layers, d_model, d_ff, dropout_rate, num_classes, act_string, regularizer):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Embed the input tokens (indices) without automatic masking\n",
        "    x = layers.Embedding(input_dim=num_classes, output_dim=d_model)(inputs)\n",
        "\n",
        "    # Apply learnable positional embeddings\n",
        "    x = PositionalEmbedding_learned(seq_length, d_model)(x)\n",
        "\n",
        "    # Manually create the attention mask\n",
        "    attention_mask = layers.Lambda(create_attention_mask2)(inputs)\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        # Pre-Norm Layer Normalization\n",
        "        attn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Multi-Head Attention with manual attention mask\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model // num_heads,\n",
        "        )(attn_input, attn_input, attention_mask=attention_mask)\n",
        "\n",
        "        attn_output = layers.Dropout(dropout_rate)(attn_output)\n",
        "        x = x + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-Forward Network with Pre-Norm\n",
        "        ffn_input = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_ff, activation=act_string, kernel_regularizer=regularizer\n",
        "        )(ffn_input)\n",
        "        ffn_output = layers.Dense(\n",
        "            d_model, kernel_regularizer=regularizer\n",
        "        )(ffn_output)\n",
        "        ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n",
        "        x = x + ffn_output  # Residual connection\n",
        "\n",
        "    # Output layer to predict the next bin index at each position\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Adjusted sequence length for autoregressive prediction\n",
        "seq_length = sub_image_size * sub_image_size - 1\n",
        "\n",
        "print(\"Shape of bin_indices:\", bin_indices.shape)\n",
        "\n",
        "# Prepare input and target sequences by shifting the data\n",
        "input_sequences = bin_indices[:, :-1]  # All indices except the last one\n",
        "target_sequences = bin_indices[:, 1:]  # All indices except the first one\n",
        "\n",
        "# Create the model\n",
        "autoregressive_transformer_freeposemb = create_autoregressive_transformer_freeposemb(\n",
        "    height=sub_image_size,\n",
        "    width=sub_image_size,\n",
        "    n_layers=n_trans_layers,\n",
        "    d_model=d_model,\n",
        "    d_ff=d_ff,\n",
        "    dropout_rate=dropout_rate,\n",
        "    num_classes=num_classes,\n",
        "    act_string=act_string,\n",
        "    regularizer=regularizer\n",
        ")\n",
        "\n",
        "# Compile the model with cross-entropy loss\n",
        "autoregressive_transformer_freeposemb.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# View the model summary\n",
        "autoregressive_transformer_freeposemb.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88025e8",
      "metadata": {},
      "source": [
        "### below trains the transformer with positional embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f974a0b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m450s\u001b[0m 483ms/step - accuracy: 0.0476 - loss: 3.8486 - val_accuracy: 0.0580 - val_loss: 3.5442\n",
            "Epoch 2/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 497ms/step - accuracy: 0.0580 - loss: 3.5309 - val_accuracy: 0.0659 - val_loss: 3.3921\n",
            "Epoch 3/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 533ms/step - accuracy: 0.0698 - loss: 3.3314 - val_accuracy: 0.0808 - val_loss: 3.1690\n",
            "Epoch 4/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 505ms/step - accuracy: 0.0810 - loss: 3.1638 - val_accuracy: 0.0859 - val_loss: 3.1049\n",
            "Epoch 5/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 502ms/step - accuracy: 0.0853 - loss: 3.1139 - val_accuracy: 0.0878 - val_loss: 3.0774\n",
            "Epoch 6/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 508ms/step - accuracy: 0.0873 - loss: 3.0876 - val_accuracy: 0.0895 - val_loss: 3.0581\n",
            "Epoch 7/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 477ms/step - accuracy: 0.0893 - loss: 3.0654 - val_accuracy: 0.0916 - val_loss: 3.0344\n",
            "Epoch 8/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 565ms/step - accuracy: 0.0916 - loss: 3.0405 - val_accuracy: 0.0931 - val_loss: 3.0154\n",
            "Epoch 9/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 527ms/step - accuracy: 0.0933 - loss: 3.0206 - val_accuracy: 0.0962 - val_loss: 2.9894\n",
            "Epoch 10/10\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 528ms/step - accuracy: 0.0951 - loss: 3.0011 - val_accuracy: 0.0969 - val_loss: 2.9747\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predictions shape: (1, 1023, 186)\n",
            "Predicted classes shape: (1, 1023)\n",
            "\u001b[1m52/52\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 457ms/step - accuracy: 0.0966 - loss: 2.9750\n",
            "Validation Loss: 2.9747  bits per pixel : 4.2916\n",
            "Validation Accuracy: 0.0969\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optionally split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    input_sequences.numpy(), target_sequences.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = autoregressive_transformer_freeposemb.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=16,\n",
        "    epochs=10,  # Adjust epochs as needed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test the model with a sample input\n",
        "sample_input = input_sequences[0:1]  # Take the first sample\n",
        "predictions = autoregressive_transformer_freeposemb.predict(sample_input)\n",
        "print(\"Predictions shape:\", predictions.shape)  # Should be (1, seq_length, num_classes)\n",
        "\n",
        "# Get the predicted classes\n",
        "predicted_classes = np.argmax(predictions, axis=-1)\n",
        "print(\"Predicted classes shape:\", predicted_classes.shape)  # Should be (1, seq_length)\n",
        "\n",
        "# Reconstruct and visualize the predicted image\n",
        "#first_token = sample_input.numpy()[0, 0]\n",
        "#reconstructed_sequence = np.concatenate(([first_token], predicted_classes[0]))\n",
        "#reconstructed_image = reconstructed_sequence.reshape(sub_image_size, sub_image_size)\n",
        "\n",
        "#plt.imshow(reconstructed_image, cmap='gray')\n",
        "#plt.title('Predicted Image')\n",
        "#plt.axis('off')\n",
        "#plt.show()\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = autoregressive_transformer_freeposemb.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\", f\" bits per pixel : {val_loss/np.log(2):.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceafbd2c",
      "metadata": {},
      "source": [
        "# The following try to learn the quantization bins rather than assume some motivated quantizations\n",
        "\n",
        "\n",
        "Hurum, we really want to do this if we do use transformer method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8606e28e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the LearnedScalarQuantizer to use float32 instead of float16\n",
        "class LearnedScalarQuantizer(layers.Layer):\n",
        "    \"\"\"\n",
        "    Quantizes a flattened float sequence x \\in R^{L} into integer tokens.\n",
        "    - tokens in [1..K], reserving 0 for <START>/<PAD>\n",
        "    - learnable : global (shape=[]) or per-position (shape=[L])\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, num_bins, per_position=True, init_delta=0.05, name=\"lsq\"):\n",
        "        super().__init__(name=name)\n",
        "        self.seq_len = seq_len\n",
        "        self.num_bins = int(num_bins)             # K (excluding START=0)\n",
        "        self.per_position = per_position\n",
        "        shape = [seq_len] if per_position else []\n",
        "        # log-parameterize for positivity; init near provided value\n",
        "        # Changed dtype from tf.float16 to tf.float32\n",
        "        self.log_delta = self.add_weight(\n",
        "            name=\"log_delta\", shape=shape, initializer=tf.keras.initializers.Constant(tf.math.log(init_delta)),\n",
        "            trainable=True, dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def delta(self):\n",
        "        #  = softplus(exp(log)) for extra positivity margin (optional)\n",
        "        return tf.exp(self.log_delta) + 1e-6\n",
        "\n",
        "    def call(self, x_float, training=True):\n",
        "        \"\"\"\n",
        "        x_float: [B, L] float32 standardized input\n",
        "        Returns:\n",
        "          tokens: [B, L] int32 in [1..K]\n",
        "          q_int : [B, L] int32 quantized indices centered at 0, in [-K/2..K/2-1]\n",
        "          x_hat : [B, L] float32 reconstruction  * q_int\n",
        "          delta : [L] or [] current steps\n",
        "        \"\"\"\n",
        "         = self.delta  # [L] or []\n",
        "        y = x_float /                         # [B, L]\n",
        "        # STE: forward uses round(y); backward uses identity\n",
        "        if training:\n",
        "             q = tf.stop_gradient(tf.round(y) - y) + y\n",
        "        else:\n",
        "             q = tf.round(y)\n",
        "        q_int = tf.cast(tf.round(y), tf.int32)          # integers centered at 0\n",
        "\n",
        "        K = self.num_bins\n",
        "        # clamp to a finite vocabulary range\n",
        "        q_int = tf.clip_by_value(q_int, -K//2, K//2 - 1)\n",
        "        tokens = q_int + (K // 2) + 1                   # shift to [1..K], keeping 0 for <START>\n",
        "\n",
        "        x_hat = tf.cast(q_int, x_float.dtype) *        # reconstruction\n",
        "        return tokens, q_int, x_hat, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "bd9d8089",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_bins = 20\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Variable._variable_call() got multiple values for argument 'trainable'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m D_star \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(DISTORTION, tf\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# your target MSE on standardized units\u001b[39;00m\n\u001b[1;32m     75\u001b[0m rho \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0.1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mfloat32)      \u001b[38;5;66;03m# step for dual update\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m lambda_dual \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step_dual\u001b[39m(x_float_batch):\n\u001b[1;32m     80\u001b[0m     B \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(x_float_batch)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/ops/variables.py:197\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    196\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_variable_call\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_call):\n\u001b[0;32m--> 197\u001b[0m     variable_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m variable_call\n",
            "\u001b[0;31mTypeError\u001b[0m: Variable._variable_call() got multiple values for argument 'trainable'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#constants to set\n",
        "######################################################\n",
        "num_bins = 20 #adjust as needed\n",
        "lambda_rd_param = 1e-2 #determines how important the distortion is relative to rate\n",
        "fix_distortion = True #set to true to fix distortion and update lambda\n",
        "DISTORTION = 0.01 #target distortion (MSE) to achieve\n",
        "\n",
        "\n",
        "# ---- Assumptions from your notebook ----\n",
        "# sub_image_size, num_bins, autoregressive_transformer exist\n",
        "SEQ_LEN = sub_image_size * sub_image_size\n",
        "\n",
        "######################################################\n",
        "\n",
        "\n",
        "print(\"num_bins =\", num_bins)\n",
        "\n",
        "K = num_bins            # number of data bins (tokens 1..K); token 0 is <START>\n",
        "START_TOKEN = 0\n",
        "\n",
        "# Make / choose the AR transformer that maps [B,L] int tokens -> logits [B,L,V]\n",
        "ar_model = autoregressive_transformer  # or your *_freeposemb variant\n",
        "V = K + 1  # include START=0\n",
        "\n",
        "# Learned quantizer\n",
        "quant = LearnedScalarQuantizer(seq_len=SEQ_LEN, num_bins=K, per_position=True, init_delta=0.05)\n",
        "\n",
        "# Simple optimizer for both AR model and quantizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "lambda_rd = tf.Variable(lambda_rd_param, dtype=tf.float32, trainable=False)  # adjust to sweep RD curve\n",
        "\n",
        "#######################################################################################\n",
        "#Fixed lambda version of training step (so no control of distortion)\n",
        "#######################################################################################\n",
        "@tf.function\n",
        "def train_step(x_float_batch):\n",
        "    \"\"\"\n",
        "    x_float_batch: [B, H, W] floats (standardized); flattened inside\n",
        "    \"\"\"\n",
        "    B = tf.shape(x_float_batch)[0]\n",
        "    x_seq = tf.reshape(x_float_batch, [B, -1])  # [B, L]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Quantize -> tokens (labels) and reconstruction\n",
        "        tokens, q_int, x_hat,  = quant(x_seq, training=True)  # ints in [1..K]\n",
        "\n",
        "        # Build teacher-forcing inputs: prepend START, drop last token\n",
        "        inp = tf.concat([tf.fill([B, 1], START_TOKEN), tokens[:, :-1]], axis=1)  # [B,L]\n",
        "\n",
        "        # Forward AR transformer -> logits over {0..K}\n",
        "        logits = ar_model(inp, training=True)  # expected shape [B, L, V]\n",
        "\n",
        "        # Rate (bits/symbol): CE is in nats by default; divide by ln 2\n",
        "        ce_nats = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tokens, logits=logits)\n",
        "        R_bits = tf.reduce_mean(ce_nats) / tf.math.log(2.0)\n",
        "\n",
        "        # Distortion (default MSE). Replace with science metric if desired.\n",
        "        D = tf.reduce_mean(tf.square(x_seq - x_hat))\n",
        "\n",
        "        loss = R_bits + lambda_rd * D\n",
        "\n",
        "    vars_all = ar_model.trainable_variables + quant.trainable_variables\n",
        "    grads = tape.gradient(loss, vars_all)\n",
        "    optimizer.apply_gradients(zip(grads, vars_all))\n",
        "    return loss, R_bits, D, tf.reduce_mean()\n",
        "\n",
        "\n",
        "####################################################################\n",
        "#updates the distortion to achieve \n",
        "#####################################################################\n",
        "D_star = tf.constant(DISTORTION, tf.float32)  # your target MSE on standardized units\n",
        "rho = tf.constant(0.1, tf.float32)      # step for dual update\n",
        "lambda_dual = tf.Variable(0.0, tf.float32, trainable=False)\n",
        "\n",
        "@tf.function\n",
        "def train_step_dual(x_float_batch):\n",
        "    B = tf.shape(x_float_batch)[0]\n",
        "    x_seq = tf.reshape(x_float_batch, [B, -1])\n",
        "    with tf.GradientTape() as tape:\n",
        "        tokens, q_int, x_hat,  = quant(x_seq, training=True)\n",
        "        inp = tf.concat([tf.fill([B, 1], START_TOKEN), tokens[:, :-1]], axis=1)\n",
        "        logits = ar_model(inp, training=True)\n",
        "        ce_nats = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tokens, logits=logits)\n",
        "        R_bits = tf.reduce_mean(ce_nats) / tf.math.log(2.0)\n",
        "        D = tf.reduce_mean(tf.square(x_seq - x_hat))\n",
        "        # augmented Lagrangian\n",
        "        loss = R_bits + lambda_dual*(D - D_star) + 0.5*rho*tf.square(D - D_star)\n",
        "    vars_all = ar_model.trainable_variables + quant.trainable_variables\n",
        "    grads = tape.gradient(loss, vars_all)\n",
        "    optimizer.apply_gradients(zip(grads, vars_all))\n",
        "    return R_bits, D\n",
        "\n",
        "# dual update (e.g., every epoch)\n",
        "lambda_dual.assign(tf.maximum(0.0, lambda_dual + rho*(Dv_mean - D_star)))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def eval_step(x_float_batch):\n",
        "    B = tf.shape(x_float_batch)[0]\n",
        "    x_seq = tf.reshape(x_float_batch, [B, -1])\n",
        "    tokens, q_int, x_hat,  = quant(x_seq, training=False)\n",
        "    inp = tf.concat([tf.fill([B, 1], START_TOKEN), tokens[:, :-1]], axis=1)\n",
        "    logits = ar_model(inp, training=False)\n",
        "    ce_nats = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tokens, logits=logits)\n",
        "    R_bits = tf.reduce_mean(ce_nats) / tf.math.log(2.0)\n",
        "    D = tf.reduce_mean(tf.square(x_seq - x_hat))\n",
        "    return R_bits, D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d20c8d59",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set - mean: 0.516844, std: 0.456314\n",
            "Normalized training mean: 0.000000, std: 1.000000\n",
            "Normalized validation mean: 0.021434, std: 1.002346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-08 14:20:31.351676: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2025-09-08 14:20:50.463468: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 001 | train R=0.982bpp D=3.8110e-01 =0.0607 | val R=0.961bpp D=3.7844e-01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-08 14:28:04.936441: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2025-09-08 14:28:23.634629: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 002 | train R=0.980bpp D=3.6411e-01 =0.0634 | val R=0.959bpp D=3.6140e-01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-08 14:35:44.080310: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2025-09-08 14:36:05.092514: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 003 | train R=0.978bpp D=3.4686e-01 =0.0662 | val R=0.957bpp D=3.4441e-01\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb \u001b[38;5;129;01min\u001b[39;00m train_ds:\n\u001b[0;32m---> 53\u001b[0m         loss, R_bits, D, mean_delta \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mappend((R_bits\u001b[38;5;241m.\u001b[39mnumpy(), D\u001b[38;5;241m.\u001b[39mnumpy(), mean_delta\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# For the learned quantization approach, you need to work with the original float data\n",
        "# Use WL_tensor (the float data) instead of X_train (which seems to be integer indices)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH = 32 \n",
        "log_field = False  # Set to True if you want to use log scaling\n",
        "\n",
        "\n",
        "# Convert your WL_tensor to the right shape for the learned quantizer\n",
        "WL_float = tf.squeeze(WL_tensor, axis=-1)  # Remove channel dimension: (16384, 32, 32)\n",
        "\n",
        "# Split into train/val using the float data\n",
        "X_train_float, X_val_float = train_test_split(\n",
        "    WL_float.numpy(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "if log_field:\n",
        "    # Apply log scaling if needed\n",
        "    X_train_float = np.log(X_train_float) \n",
        "    X_val_float = np.log(X_val_float)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate normalization statistics from training set only\n",
        "train_mean = np.mean(X_train_float)\n",
        "train_std = np.std(X_train_float)\n",
        "\n",
        "# Apply normalization to both sets using training statistics\n",
        "X_train_float = (X_train_float - train_mean) / train_std\n",
        "X_val_float = (X_val_float - train_mean) / train_std\n",
        "\n",
        "print(f\"Training set - mean: {train_mean:.6f}, std: {train_std:.6f}\")\n",
        "print(f\"Normalized training mean: {np.mean(X_train_float):.6f}, std: {np.std(X_train_float):.6f}\")\n",
        "print(f\"Normalized validation mean: {np.mean(X_val_float):.6f}, std: {np.std(X_val_float):.6f}\")\n",
        "\n",
        "\n",
        "# Create datasets with float data\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices(X_train_float).shuffle(4096).batch(BATCH).prefetch(2)\n",
        "#val_ds = tf.data.Dataset.from_tensor_slices(X_val_float).batch(BATCH).prefetch(2)\n",
        "\n",
        "# Option 1: Make your dataset more robust by dropping remainder\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(X_train_float).shuffle(4096).batch(BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(X_val_float).batch(BATCH, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "# Option 2: Handle the exception explicitly in your training loop\n",
        "# for epoch in range(1, 5):\n",
        "#     metrics = []\n",
        "#     try:\n",
        "#         for xb in train_ds:\n",
        "#             loss, R_bits, D, mean_delta = train_step(xb)\n",
        "#             metrics.append((R_bits.numpy(), D.numpy(), mean_delta.numpy()))\n",
        "#     except tf.errors.OutOfRangeError:\n",
        "#         print(f\"End of dataset reached for epoch {epoch}\")\n",
        "    \n",
        "#     if metrics:  # Only compute if we have metrics\n",
        "#         Rm, Dm, Deltam = np.mean(metrics, axis=0)\n",
        "    \n",
        "#     Rv, Dv = [], []\n",
        "#     try:\n",
        "#         for xb in val_ds:\n",
        "#             Rb, Db = eval_step(xb)\n",
        "#             Rv.append(Rb.numpy()); Dv.append(Db.numpy())\n",
        "#     except tf.errors.OutOfRangeError:\n",
        "#         print(f\"End of validation dataset reached for epoch {epoch}\")\n",
        "    \n",
        "#     if Rv:  # Only print if we have validation metrics\n",
        "#         print(f\"epoch {epoch:03d} | train R={Rm:.3f} bpp D={Dm:.4e} ={Deltam:.4f} | val R={np.mean(Rv):.3f} bpp D={np.mean(Dv):.4e}\")\n",
        "\n",
        "\n",
        "# Replace your training loop with this:\n",
        "\n",
        "for epoch in range(1, 5):\n",
        "    print(f\"Starting epoch {epoch}\")\n",
        "    \n",
        "    # Training\n",
        "    train_metrics = []\n",
        "    train_batches = 0\n",
        "    for xb in train_ds:\n",
        "        if fix_distortion:\n",
        "            loss, R_bits, D, mean_delta = train_step_dual(xb)\n",
        "        else:\n",
        "            loss, R_bits, D, mean_delta = train_step(xb)\n",
        "        train_metrics.append((R_bits.numpy(), D.numpy(), mean_delta.numpy()))\n",
        "        train_batches += 1\n",
        "    \n",
        "    print(f\"Processed {train_batches} training batches\")\n",
        "    \n",
        "    if train_metrics:\n",
        "        Rm, Dm, Deltam = np.mean(train_metrics, axis=0)\n",
        "    \n",
        "    # Validation\n",
        "    val_metrics = []\n",
        "    val_batches = 0\n",
        "    for xb in val_ds:\n",
        "        Rb, Db = eval_step(xb)\n",
        "        val_metrics.append((Rb.numpy(), Db.numpy()))\n",
        "        val_batches += 1\n",
        "    \n",
        "    print(f\"Processed {val_batches} validation batches\")\n",
        "    \n",
        "    if val_metrics:\n",
        "        Rv_mean, Dv_mean = np.mean(val_metrics, axis=0)\n",
        "        print(f\"epoch {epoch:03d} | train R={Rm:.3f} bpp D={Dm:.4e} ={Deltam:.4f} |     val R={Rv_mean:.3f} bpp D={Dv_mean:.4e}\")        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ac64ef",
      "metadata": {},
      "source": [
        "## V-Net convolutional autoencoder\n",
        "\n",
        "A CNN-based compression model with symmetric encoder-decoder structure.\n",
        "The encoder progressively downsamples the image to a learned latent representation,\n",
        "while the decoder upsamples back to image space. The bottleneck at the center\n",
        "enables compression by forcing the network to learn a compact representation.\n",
        "\n",
        "\n",
        "This is the main way people compress images --- it's what's in tensorflow-compression and pytorch-compression.  Note that I didn't get this to work and produce small enough distortions below.  I think it may be hard to get this approach to produce very small distortions.\n",
        "\n",
        "I also tried to do this using the tensorflow-compression library which could make this easier, but I could never get it to run on GPUs, which was very important for this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8a0e9714",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: N=16384, H=32, W=32, C=1, dtype=float32\n",
            "Data: N=16384, H=32, W=32, C=1\n"
          ]
        }
      ],
      "source": [
        "# ========= VAE for fields: compile & train =========\n",
        "import os, math, numpy as np, tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Ensure we always end up with a NumPy float32 array\n",
        "if isinstance(WL_tensor, tf.Tensor):\n",
        "    WL_tensor = WL_tensor.numpy()  # convert TF tensor -> NumPy\n",
        "\n",
        "WL_tensor = WL_tensor.astype(np.float32)\n",
        "\n",
        "# Add channel dimension if needed\n",
        "if WL_tensor.ndim == 3:   # (N,H,W)\n",
        "    WL_tensor = WL_tensor[..., None]   # (N,H,W,1)\n",
        "\n",
        "N, H, W, C = WL_tensor.shape\n",
        "print(f\"Data shape: N={N}, H={H}, W={W}, C={C}, dtype={WL_tensor.dtype}\")\n",
        "\n",
        "\n",
        "\n",
        "N, H, W, C = WL_tensor.shape\n",
        "print(f\"Data: N={N}, H={H}, W={W}, C={C}\")\n",
        "\n",
        "# Standardize using train split only\n",
        "X_train, X_val = train_test_split(WL_tensor, test_size=0.1, random_state=42)\n",
        "mu, std = X_train.mean(), X_train.std() + 1e-8\n",
        "X_train = (X_train - mu) / std\n",
        "X_val   = (X_val   - mu) / std\n",
        "\n",
        "BATCH = 64\n",
        "\n",
        "debug_samples = BATCH*30  # Instead of 16384 images\n",
        "X_train_debug = X_train[:debug_samples]\n",
        "X_val_debug = X_val[:debug_samples//10]\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(X_train_debug).shuffle(4096).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds   = tf.data.Dataset.from_tensor_slices(X_val_debug).batch(BATCH).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d4b524",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def quantize_or_add_noise(y, training, added_noise=True):\n",
        "    if training:\n",
        "        # Additive uniform noise to relax rounding during training\n",
        "        if not added_noise:\n",
        "            return y\n",
        "        else:\n",
        "            noise = tf.random.uniform(tf.shape(y), -0.5, 0.5, dtype=y.dtype)\n",
        "            return y + noise\n",
        "    else:\n",
        "        # Real quantization at inference / validation time\n",
        "        return tf.round(y)\n",
        "\n",
        "@tf.function\n",
        "def discretized_logistic_cdf(z, mean, log_scale, eps=1e-9):\n",
        "    # z, mean, log_scale can be broadcast\n",
        "    inv_scale = tf.exp(-log_scale)\n",
        "    return tf.sigmoid((z - mean) * inv_scale)\n",
        "\n",
        "@tf.function\n",
        "def log_prob_convolved_logistic(y_tilde, mean, log_scale):\n",
        "    # P = CDF(y+0.5) - CDF(y-0.5) for logistic; clamp for numerical stability\n",
        "    cdf_plus  = discretized_logistic_cdf(y_tilde + 0.5, mean, log_scale)\n",
        "    cdf_minus = discretized_logistic_cdf(y_tilde - 0.5, mean, log_scale)\n",
        "    p = tf.clip_by_value(cdf_plus - cdf_minus, 1e-12, 1.0)\n",
        "    return tf.math.log(p)  # natural log\n",
        "\n",
        "def rate_bits(y_prequant, mean, log_scale, training):\n",
        "    \"\"\"\n",
        "    y_prequant: latents before quantization (B, L) or (B,H,W,C)\n",
        "    mean, log_scale: same shape (broadcast ok). If you only predict scale, set mean=0.\n",
        "    Returns: bits per element and total bits per pixel (bpp) if you know image size.\n",
        "    \"\"\"\n",
        "    y_tilde = quantize_or_add_noise(y_prequant, training=training)\n",
        "    logp = log_prob_convolved_logistic(y_tilde, mean, log_scale)  # ln p\n",
        "    bits = -logp / tf.math.log(tf.constant(2., y_tilde.dtype))   # convert to log2\n",
        "    # mean over all latent elements gives expected bits per latent symbol\n",
        "    bits_per_symbol = tf.reduce_mean(bits)\n",
        "    return bits, bits_per_symbol\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a793d3",
      "metadata": {},
      "source": [
        "## ecnoder and decoder blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa6b8ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_encoder\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"simple_encoder\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_layer_32       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
              "\n",
              " conv2d_84 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>  input_layer_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " leaky_re_lu_36       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_84[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
              "\n",
              " conv2d_85 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span>  leaky_re_lu_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " leaky_re_lu_37       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d_85[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)                                                           \n",
              "\n",
              " entropy_log_scale_  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span>  leaky_re_lu_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                              \n",
              "\n",
              " latent_y (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span>  leaky_re_lu_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " entropy_mu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">4,672</span>  leaky_re_lu_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " entropy_log_scale    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  entropy_log_scal \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                                                              \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_32       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)           \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
              "\n",
              " conv2d_84 (\u001b[38;5;33mConv2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)          \u001b[38;5;34m40\u001b[0m  input_layer_32[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " leaky_re_lu_36       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m4\u001b[0m)           \u001b[38;5;34m0\u001b[0m  conv2d_84[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
              "\n",
              " conv2d_85 (\u001b[38;5;33mConv2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)           \u001b[38;5;34m296\u001b[0m  leaky_re_lu_36[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " leaky_re_lu_37       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv2d_85[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n",
              " (\u001b[38;5;33mLeakyReLU\u001b[0m)                                                           \n",
              "\n",
              " entropy_log_scale_  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m4,672\u001b[0m  leaky_re_lu_37[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                              \n",
              "\n",
              " latent_y (\u001b[38;5;33mConv2D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m4,672\u001b[0m  leaky_re_lu_37[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " entropy_mu (\u001b[38;5;33mConv2D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)        \u001b[38;5;34m4,672\u001b[0m  leaky_re_lu_37[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " entropy_log_scale    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            \u001b[38;5;34m0\u001b[0m  entropy_log_scal \n",
              " (\u001b[38;5;33mLambda\u001b[0m)                                                              \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,352</span> (56.06 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,352\u001b[0m (56.06 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,352</span> (56.06 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,352\u001b[0m (56.06 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Decoder summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"simple_decoder\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"simple_decoder\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " input_layer_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " up_sampling2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_86 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,616</span> \n",
              "\n",
              " leaky_re_lu_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " up_sampling2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">292</span> \n",
              "\n",
              " leaky_re_lu_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " reconstruction (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_34 (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " up_sampling2d_18 (\u001b[38;5;33mUpSampling2D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_86 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m4,616\u001b[0m \n",
              "\n",
              " leaky_re_lu_38 (\u001b[38;5;33mLeakyReLU\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " up_sampling2d_19 (\u001b[38;5;33mUpSampling2D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_87 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 \u001b[38;5;34m292\u001b[0m \n",
              "\n",
              " leaky_re_lu_39 (\u001b[38;5;33mLeakyReLU\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " reconstruction (\u001b[38;5;33mConv2D\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                  \u001b[38;5;34m37\u001b[0m \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,945</span> (19.32 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,945\u001b[0m (19.32 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,945</span> (19.32 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,945\u001b[0m (19.32 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distortion (MSE): 1.312371\n",
            "Rate (bits/symbol): 2.0796\n",
            "Rate (bits/pixel): 8.318384\n"
          ]
        }
      ],
      "source": [
        "def make_simple_encoder(latent_dim=64, network_size=\"tiny\"):\n",
        "    \"\"\"\n",
        "    Simple encoder for image compression:\n",
        "    - Takes input images (H, W, C) \n",
        "    - Outputs latent representation y and entropy parameters\n",
        "    \"\"\"\n",
        "    x_in = layers.Input(shape=(H, W, C))\n",
        "    x = x_in\n",
        "    \n",
        "    # Downsample progressively: 32x32 -> 16x16 -> 8x8 -> 4x4 -> 2x2\n",
        "    # Different network sizes\n",
        "    if network_size == \"large\":\n",
        "        filters = [32, 64, 128, 192]\n",
        "    elif network_size == \"medium\":\n",
        "        filters = [16, 32, 64, 96]\n",
        "    elif network_size == \"small\":\n",
        "        filters = [16, 32, 48, 64]\n",
        "    elif network_size == \"tiny\":\n",
        "        filters = [4, 8]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid network size. Choose from 'large', 'medium', 'small', 'tiny'.\")\n",
        "        \n",
        "    for i, f in enumerate(filters):\n",
        "        x = layers.Conv2D(f, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    # Output heads\n",
        "    # y: latent representation (what gets quantized)\n",
        "    y = layers.Conv2D(latent_dim, kernel_size=3, strides=1, padding=\"same\", \n",
        "                      name=\"latent_y\")(x)\n",
        "    \n",
        "    # Entropy model parameters\n",
        "    mu = layers.Conv2D(latent_dim, kernel_size=3, strides=1, padding=\"same\",\n",
        "                       name=\"entropy_mu\")(x)\n",
        "    \n",
        "    # Log scale (clamped for stability)\n",
        "    log_scale_raw = layers.Conv2D(latent_dim, kernel_size=3, strides=1, padding=\"same\",\n",
        "                                  name=\"entropy_log_scale_raw\")(x)\n",
        "    log_scale = layers.Lambda(lambda t: tf.clip_by_value(t, -5.0, 3.0),\n",
        "                              name=\"entropy_log_scale\")(log_scale_raw)\n",
        "    \n",
        "    return models.Model(x_in, [y, mu, log_scale], name=\"simple_encoder\")\n",
        "\n",
        "\n",
        "def make_simple_decoder(latent_dim=64, network_size=\"tiny\"):\n",
        "    \"\"\"\n",
        "    Simple decoder for image compression:\n",
        "    - Takes quantized latents y_q\n",
        "    - Reconstructs original image\n",
        "    \"\"\"\n",
        "    y_in = layers.Input(shape=(H // 16, W // 16, latent_dim))  # Assuming 4 downsampling layers\n",
        "    x = y_in\n",
        "    \n",
        "    # Fix: Calculate the correct input shape based on network_size\n",
        "    if network_size == \"large\":\n",
        "        # 4 downsampling layers: 32->16->8->4->2\n",
        "        input_shape = (H // 16, W // 16, latent_dim)  # (2, 2, 64)\n",
        "        filters = [128, 96, 64, 32]\n",
        "    elif network_size == \"medium\":\n",
        "        # 4 downsampling layers: 32->16->8->4->2  \n",
        "        input_shape = (H // 16, W // 16, latent_dim)  # (2, 2, 64)\n",
        "        filters = [64, 48, 32, 16]\n",
        "    elif network_size == \"small\":\n",
        "        # 4 downsampling layers: 32->16->8->4->2\n",
        "        input_shape = (H // 16, W // 16, latent_dim)  # (2, 2, 64)\n",
        "        filters = [48, 32, 24, 16]\n",
        "    elif network_size == \"tiny\":\n",
        "        # Only 2 downsampling layers: 32->16->8\n",
        "        input_shape = (H // 4, W // 4, latent_dim)  # (8, 8, 64)\n",
        "        filters = [8, 4]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid network size. Choose from 'large', 'medium', 'small', 'tiny'.\")\n",
        "\n",
        "\n",
        "    y_in = layers.Input(shape=input_shape)\n",
        "    x = y_in\n",
        "\n",
        "    # Upsample progressively to match the encoder's downsampling\n",
        "    for i, f in enumerate(filters):\n",
        "        x = layers.UpSampling2D(size=2, interpolation=\"nearest\")(x)\n",
        "        x = layers.Conv2D(f, kernel_size=3, strides=1, padding=\"same\")(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    # Final output layer\n",
        "    x_hat = layers.Conv2D(C, kernel_size=3, strides=1, padding=\"same\",\n",
        "                          name=\"reconstruction\")(x)\n",
        "    \n",
        "    return models.Model(y_in, x_hat, name=\"simple_decoder\")\n",
        "\n",
        "\n",
        "\n",
        "# Create the encoder/decoder pair\n",
        "latent_dim = 64  # Adjust based on your compression needs\n",
        "encoder = make_simple_encoder(latent_dim)\n",
        "decoder = make_simple_decoder(latent_dim)\n",
        "\n",
        "# Test with your existing compression functions\n",
        "@tf.function\n",
        "def simple_compression_forward(x, training=True, addquantnoise=True):\n",
        "    \"\"\"\n",
        "    Complete forward pass using your existing compression functions\n",
        "    \"\"\"\n",
        "    # Get latents and entropy parameters\n",
        "    y, mu, log_scale = encoder(x, training=training)\n",
        "    \n",
        "    # Calculate rate using your existing function\n",
        "    bits_map, bits_per_symbol = rate_bits(y, mean=mu, log_scale=log_scale, training=training)\n",
        "    \n",
        "    # Quantize latents\n",
        "    y_q = quantize_or_add_noise(y, training=training, addquantnoise=addquantnoise)\n",
        "    \n",
        "    # Decode\n",
        "    x_hat = decoder(y_q, training=training)\n",
        "    \n",
        "    # Calculate distortion\n",
        "    D = tf.reduce_mean(tf.square(x - x_hat))\n",
        "    \n",
        "    # Calculate bits per pixel\n",
        "    latent_size = tf.cast(tf.reduce_prod(tf.shape(y)[1:]), tf.float32)\n",
        "    image_size = tf.cast(H * W, tf.float32)\n",
        "    bpp = bits_per_symbol * (latent_size / image_size)\n",
        "    \n",
        "    return D, bits_per_symbol, bpp, x_hat\n",
        "\n",
        "# Example usage with your data\n",
        "print(\"Encoder summary:\")\n",
        "encoder.summary()\n",
        "print(\"\\nDecoder summary:\")\n",
        "decoder.summary()\n",
        "\n",
        "# Test on a batch\n",
        "sample_batch = X_train[:4]  # Take 4 samples\n",
        "D, rate, bpp, reconstructed = simple_compression_forward(sample_batch, training=True)\n",
        "\n",
        "print(f\"Distortion (MSE): {D:.6f}\")\n",
        "print(f\"Rate (bits/symbol): {rate:.4f}\")\n",
        "print(f\"Rate (bits/pixel): {bpp:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "74222b4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Training compression network for 5 epochs...\n",
            "Lambda (rate-distortion tradeoff): 1\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"/var/folders/wm/r259x5453lgbh9fmt19f54940000gn/T/ipykernel_85223/2642697282.py\", line 23, in train_step  *\n        D, bits_per_symbol, bpp, x_hat = simple_compression_forward(x_batch, training=True, addquantnoise=addquantnoise)\n\n    TypeError: Binding inputs to tf.function failed due to `got an unexpected keyword argument 'addquantnoise'`. Received args: (<tf.Tensor 'x_batch:0' shape=(64, 32, 32, 1) dtype=float32>,) and kwargs: {'training': True, 'addquantnoise': False} for signature: (x, training=<captured_default_value>).\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Train the network\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_compression_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_rd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAMBDA_RD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Plot training curves\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_training_curves\u001b[39m(history):\n",
            "Cell \u001b[0;32mIn[32], line 75\u001b[0m, in \u001b[0;36mtrain_compression_network\u001b[0;34m(train_ds, val_ds, epochs, lambda_rd)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     73\u001b[0m     addquantnoise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m loss, D, bpp, bits_per_symbol \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_rd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddquantnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddquantnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     78\u001b[0m epoch_distortion\u001b[38;5;241m.\u001b[39mappend(D\u001b[38;5;241m.\u001b[39mnumpy())\n",
            "File \u001b[0;32m~/miniforge3/envs/tf_metal_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/var/folders/wm/r259x5453lgbh9fmt19f54940000gn/T/__autograph_generated_file830k6ekj.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(x_batch, lambda_rd, addquantnoise)\u001b[0m\n\u001b[1;32m     12\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 14\u001b[0m     (D, bits_per_symbol, bpp, x_hat) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_compression_forward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddquantnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddquantnoise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(lambda_rd) \u001b[38;5;241m*\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(D) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(bpp)\n\u001b[1;32m     16\u001b[0m     reg_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mld(encoder)\u001b[38;5;241m.\u001b[39mlosses,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_sum, (ag__\u001b[38;5;241m.\u001b[39mld(decoder)\u001b[38;5;241m.\u001b[39mlosses,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/var/folders/wm/r259x5453lgbh9fmt19f54940000gn/T/ipykernel_85223/2642697282.py\", line 23, in train_step  *\n        D, bits_per_symbol, bpp, x_hat = simple_compression_forward(x_batch, training=True, addquantnoise=addquantnoise)\n\n    TypeError: Binding inputs to tf.function failed due to `got an unexpected keyword argument 'addquantnoise'`. Received args: (<tf.Tensor 'x_batch:0' shape=(64, 32, 32, 1) dtype=float32>,) and kwargs: {'training': True, 'addquantnoise': False} for signature: (x, training=<captured_default_value>).\n"
          ]
        }
      ],
      "source": [
        "# Training setup for CNN compression network\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-4\n",
        "LAMBDA_RD = 1  # Rate-distortion tradeoff parameter\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Training step function\n",
        "@tf.function\n",
        "def train_step(x_batch, lambda_rd, addquantnoise=True):\n",
        "    \"\"\"\n",
        "    Single training step for rate-distortion optimization\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        D, bits_per_symbol, bpp, x_hat = simple_compression_forward(x_batch, training=True, addquantnoise=addquantnoise)\n",
        "        \n",
        "        # Rate-distortion loss\n",
        "        loss = lambda_rd * D + bpp\n",
        "        \n",
        "        # Add regularization if needed\n",
        "        reg_loss = tf.reduce_sum(encoder.losses) + tf.reduce_sum(decoder.losses)\n",
        "        total_loss = loss + reg_loss\n",
        "    \n",
        "    # Get trainable variables from both encoder and decoder\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    \n",
        "    # Compute gradients and apply\n",
        "    gradients = tape.gradient(total_loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    \n",
        "    return total_loss, D, bpp, bits_per_symbol\n",
        "\n",
        "@tf.function\n",
        "def val_step(x_batch):\n",
        "    \"\"\"\n",
        "    Validation step (no training)\n",
        "    \"\"\"\n",
        "    D, bits_per_symbol, bpp, x_hat = simple_compression_forward(x_batch, training=False)\n",
        "    return D, bpp, bits_per_symbol, x_hat\n",
        "\n",
        "# Training loop\n",
        "def train_compression_network(train_ds, val_ds, epochs=EPOCHS, lambda_rd=LAMBDA_RD):\n",
        "    \"\"\"\n",
        "    Train the compression network\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    train_distortions = []\n",
        "    train_rates = []\n",
        "    val_distortions = []\n",
        "    val_rates = []\n",
        "    \n",
        "    print(f\"Training compression network for {epochs} epochs...\")\n",
        "    print(f\"Lambda (rate-distortion tradeoff): {lambda_rd}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        epoch_loss = []\n",
        "        epoch_distortion = []\n",
        "        epoch_rate = []\n",
        "        \n",
        "        for batch_idx, x_batch in enumerate(train_ds):\n",
        "            addquantnoise = True\n",
        "            \n",
        "            if epoch <2:\n",
        "                addquantnoise = False\n",
        "                \n",
        "            loss, D, bpp, bits_per_symbol = train_step(x_batch, lambda_rd, addquantnoise=addquantnoise)\n",
        "            \n",
        "            epoch_loss.append(loss.numpy())\n",
        "            epoch_distortion.append(D.numpy())\n",
        "            epoch_rate.append(bpp.numpy())\n",
        "            \n",
        "            if batch_idx % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}: \"\n",
        "                      f\"Loss={loss:.6f}, D={D:.6f}, BPP={bpp:.6f}\")\n",
        "        \n",
        "        # Average training metrics\n",
        "        avg_train_loss = np.mean(epoch_loss)\n",
        "        avg_train_D = np.mean(epoch_distortion)\n",
        "        avg_train_bpp = np.mean(epoch_rate)\n",
        "        \n",
        "        train_losses.append(avg_train_loss)\n",
        "        train_distortions.append(avg_train_D)\n",
        "        train_rates.append(avg_train_bpp)\n",
        "        \n",
        "        # Validation\n",
        "        val_D_list = []\n",
        "        val_bpp_list = []\n",
        "        \n",
        "        for x_batch in val_ds:\n",
        "            D, bpp, bits_per_symbol, x_hat = val_step(x_batch)\n",
        "            val_D_list.append(D.numpy())\n",
        "            val_bpp_list.append(bpp.numpy())\n",
        "        \n",
        "        avg_val_D = np.mean(val_D_list)\n",
        "        avg_val_bpp = np.mean(val_bpp_list)\n",
        "        \n",
        "        val_distortions.append(avg_val_D)\n",
        "        val_rates.append(avg_val_bpp)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{epochs} Summary:\")\n",
        "        print(f\"  Train - Loss: {avg_train_loss:.6f}, D: {avg_train_D:.6f}, BPP: {avg_train_bpp:.6f}\")\n",
        "        print(f\"  Val   - D: {avg_val_D:.6f}, BPP: {avg_val_bpp:.6f}\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        # Save best model (optional)\n",
        "        #if epoch == 0 or avg_val_bpp < min(val_rates[:-1]):\n",
        "        #    encoder.save_weights(f'best_encoder_lambda_{lambda_rd}.weights.h5')\n",
        "        #    decoder.save_weights(f'best_decoder_lambda_{lambda_rd}.weights.h5')\n",
        "        #    print(f\"  -> Saved best model at epoch {epoch+1}\")\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'train_distortions': train_distortions, \n",
        "        'train_rates': train_rates,\n",
        "        'val_distortions': val_distortions,\n",
        "        'val_rates': val_rates\n",
        "    }\n",
        "\n",
        "# Train the network\n",
        "print(\"Starting training...\")\n",
        "history = train_compression_network(train_ds, val_ds, epochs=EPOCHS, lambda_rd=LAMBDA_RD)\n",
        "\n",
        "# Plot training curves\n",
        "def plot_training_curves(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation curves\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0,0].plot(history['train_losses'])\n",
        "    axes[0,0].set_title('Training Loss')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].grid(True)\n",
        "    \n",
        "    # Distortion\n",
        "    axes[0,1].plot(history['train_distortions'], label='Train')\n",
        "    axes[0,1].plot(history['val_distortions'], label='Validation')\n",
        "    axes[0,1].set_title('Distortion (MSE)')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('MSE')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True)\n",
        "    \n",
        "    # Rate\n",
        "    axes[1,0].plot(history['train_rates'], label='Train')\n",
        "    axes[1,0].plot(history['val_rates'], label='Validation')\n",
        "    axes[1,0].set_title('Rate (Bits per Pixel)')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('BPP')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True)\n",
        "    \n",
        "    # Rate-Distortion curve\n",
        "    axes[1,1].scatter(history['val_rates'], history['val_distortions'], \n",
        "                      c=range(len(history['val_rates'])), cmap='viridis')\n",
        "    axes[1,1].set_title('Rate-Distortion Curve (Validation)')\n",
        "    axes[1,1].set_xlabel('Rate (BPP)')\n",
        "    axes[1,1].set_ylabel('Distortion (MSE)')\n",
        "    axes[1,1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_curves(history)\n",
        "\n",
        "# Evaluate final performance\n",
        "print(\"\\nFinal Performance:\")\n",
        "print(f\"Best validation rate: {min(history['val_rates']):.6f} BPP\")\n",
        "print(f\"Final validation distortion: {history['val_distortions'][-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7924bac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Debug dataset sizes: train=1024, val=256\n",
            "Steps per epoch: 16, validation steps: 4\n",
            "Ultra-tiny encoder parameters: 160\n",
            "Ultra-tiny decoder parameters: 297\n",
            "Starting ultra-fast debug training...\n",
            "Ultra-fast training mode...\n",
            "Epoch 1/20\n",
            "  Step 0: Loss=9.2370, D=0.8260\n",
            "  Step 2: Loss=8.6426, D=0.7691\n",
            "  Step 4: Loss=9.4988, D=0.8480\n",
            "  Epoch summary: Train D=0.8570, Val D=0.8179, Rate=0.2606\n",
            "--------------------------------------------------\n",
            "Epoch 2/20\n",
            "  Step 0: Loss=9.3369, D=0.8337\n",
            "  Step 2: Loss=8.9973, D=0.8023\n",
            "  Step 4: Loss=9.2062, D=0.8201\n",
            "  Epoch summary: Train D=0.8119, Val D=0.8711, Rate=0.2636\n",
            "--------------------------------------------------\n",
            "Epoch 3/20\n",
            "  Step 0: Loss=8.7611, D=0.7793\n",
            "  Step 2: Loss=9.3073, D=0.8301\n",
            "  Step 4: Loss=8.5731, D=0.7605\n",
            "  Epoch summary: Train D=0.7940, Val D=0.7855, Rate=0.2585\n",
            "--------------------------------------------------\n",
            "Epoch 4/20\n",
            "  Step 0: Loss=8.8180, D=0.7835\n",
            "  Step 2: Loss=8.6594, D=0.7685\n",
            "  Step 4: Loss=9.5449, D=0.8507\n",
            "  Epoch summary: Train D=0.7887, Val D=0.8411, Rate=0.2617\n",
            "--------------------------------------------------\n",
            "Epoch 5/20\n",
            "  Step 0: Loss=8.7974, D=0.7808\n",
            "  Step 2: Loss=8.6855, D=0.7705\n",
            "  Step 4: Loss=8.7298, D=0.7734\n",
            "  Epoch summary: Train D=0.7752, Val D=0.7543, Rate=0.2568\n",
            "--------------------------------------------------\n",
            "Epoch 6/20\n",
            "  Step 0: Loss=8.8923, D=0.7884\n",
            "  Step 2: Loss=8.8703, D=0.7871\n",
            "  Step 4: Loss=7.8586, D=0.6903\n",
            "  Epoch summary: Train D=0.7331, Val D=0.8134, Rate=0.2604\n",
            "--------------------------------------------------\n",
            "Epoch 7/20\n",
            "  Step 0: Loss=8.3178, D=0.7336\n",
            "  Step 2: Loss=7.9447, D=0.6993\n",
            "  Step 4: Loss=9.1008, D=0.8062\n",
            "  Epoch summary: Train D=0.7530, Val D=0.7263, Rate=0.2560\n",
            "--------------------------------------------------\n",
            "Epoch 8/20\n",
            "  Step 0: Loss=9.0075, D=0.7987\n",
            "  Step 2: Loss=8.9838, D=0.7950\n",
            "  Step 4: Loss=8.1803, D=0.7186\n",
            "  Epoch summary: Train D=0.7238, Val D=0.7868, Rate=0.2602\n",
            "--------------------------------------------------\n",
            "Epoch 9/20\n",
            "  Step 0: Loss=7.5062, D=0.6564\n",
            "  Step 2: Loss=7.7037, D=0.6736\n",
            "  Step 4: Loss=7.7393, D=0.6771\n",
            "  Epoch summary: Train D=0.7129, Val D=0.6996, Rate=0.2563\n",
            "--------------------------------------------------\n",
            "Epoch 10/20\n",
            "  Step 0: Loss=7.4143, D=0.6466\n",
            "  Step 2: Loss=7.2517, D=0.6312\n",
            "  Step 4: Loss=7.7365, D=0.6752\n",
            "  Epoch summary: Train D=0.6568, Val D=0.7613, Rate=0.2611\n",
            "--------------------------------------------------\n",
            "Epoch 11/20\n",
            "  Step 0: Loss=7.6824, D=0.6702\n",
            "  Step 2: Loss=9.7494, D=0.8634\n",
            "  Step 4: Loss=7.9277, D=0.6921\n",
            "  Epoch summary: Train D=0.7077, Val D=0.6745, Rate=0.2576\n",
            "--------------------------------------------------\n",
            "Epoch 12/20\n",
            "  Step 0: Loss=7.2804, D=0.6315\n",
            "  Step 2: Loss=6.7625, D=0.5838\n",
            "  Step 4: Loss=7.3095, D=0.6333\n",
            "  Epoch summary: Train D=0.6241, Val D=0.7366, Rate=0.2631\n",
            "--------------------------------------------------\n",
            "Epoch 13/20\n",
            "  Step 0: Loss=7.0784, D=0.6113\n",
            "  Step 2: Loss=7.8755, D=0.6852\n",
            "  Step 4: Loss=7.4487, D=0.6448\n",
            "  Epoch summary: Train D=0.6556, Val D=0.6492, Rate=0.2599\n",
            "--------------------------------------------------\n",
            "Epoch 14/20\n",
            "  Step 0: Loss=7.3324, D=0.6337\n",
            "  Step 2: Loss=8.3120, D=0.7239\n",
            "  Step 4: Loss=7.0515, D=0.6061\n",
            "  Epoch summary: Train D=0.6542, Val D=0.7117, Rate=0.2660\n",
            "--------------------------------------------------\n",
            "Epoch 15/20\n",
            "  Step 0: Loss=7.8336, D=0.6780\n",
            "  Step 2: Loss=6.8054, D=0.5821\n",
            "  Step 4: Loss=6.3803, D=0.5429\n",
            "  Epoch summary: Train D=0.6223, Val D=0.6237, Rate=0.2631\n",
            "--------------------------------------------------\n",
            "Epoch 16/20\n",
            "  Step 0: Loss=6.5730, D=0.5605\n",
            "  Step 2: Loss=6.6250, D=0.5650\n",
            "  Step 4: Loss=6.7798, D=0.5787\n",
            "  Epoch summary: Train D=0.5772, Val D=0.6861, Rate=0.2696\n",
            "--------------------------------------------------\n",
            "Epoch 17/20\n",
            "  Step 0: Loss=6.8995, D=0.5889\n",
            "  Step 2: Loss=6.9155, D=0.5903\n",
            "  Step 4: Loss=6.6020, D=0.5597\n",
            "  Epoch summary: Train D=0.5907, Val D=0.5987, Rate=0.2667\n",
            "--------------------------------------------------\n",
            "Epoch 18/20\n",
            "  Step 0: Loss=6.3685, D=0.5386\n",
            "  Step 2: Loss=7.0774, D=0.6026\n",
            "  Step 4: Loss=6.8982, D=0.5863\n",
            "  Epoch summary: Train D=0.5645, Val D=0.6633, Rate=0.2736\n",
            "--------------------------------------------------\n",
            "Epoch 19/20\n",
            "  Step 0: Loss=6.6429, D=0.5618\n",
            "  Step 2: Loss=7.0189, D=0.5963\n",
            "  Step 4: Loss=6.3054, D=0.5295\n",
            "  Epoch summary: Train D=0.5748, Val D=0.5744, Rate=0.2708\n",
            "--------------------------------------------------\n",
            "Epoch 20/20\n",
            "  Step 0: Loss=6.9560, D=0.5882\n",
            "  Step 2: Loss=6.3310, D=0.5315\n",
            "  Step 4: Loss=6.5058, D=0.5471\n",
            "  Epoch summary: Train D=0.5470, Val D=0.6390, Rate=0.2780\n",
            "--------------------------------------------------\n",
            "Testing reconstruction...\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultra-fast debug training completed!\n"
          ]
        }
      ],
      "source": [
        "# SPEED UP TRAINING - Apply these modifications\n",
        "\n",
        "# 1. MUCH SMALLER NETWORKS FOR DEBUGGING\n",
        "def make_ultra_tiny_encoder(latent_dim=264):  # Reduced from 64 to 8\n",
        "    \"\"\"Ultra-tiny encoder for fast debugging\"\"\"\n",
        "    x_in = layers.Input(shape=(H, W, C))\n",
        "    x = x_in\n",
        "    \n",
        "    # Only 1 downsampling layer: 32->16\n",
        "    x = layers.Conv2D(4, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    # Output heads with 1x1 convs for speed\n",
        "    y = layers.Conv2D(latent_dim, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "    mu = layers.Conv2D(latent_dim, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "    log_scale_raw = layers.Conv2D(latent_dim, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "    log_scale = layers.Lambda(lambda t: tf.clip_by_value(t, -3.0, 3.0))(log_scale_raw)\n",
        "    \n",
        "    return models.Model(x_in, [y, mu, log_scale], name=\"ultra_tiny_encoder\")\n",
        "\n",
        "def make_ultra_tiny_decoder(latent_dim=64):\n",
        "    \"\"\"Ultra-tiny decoder for fast debugging\"\"\"\n",
        "    input_shape = (H // 2, W // 2, latent_dim)  # Only 1 downsampling\n",
        "    y_in = layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Single upsampling layer\n",
        "    x = layers.UpSampling2D(size=2)(y_in)\n",
        "    x = layers.Conv2D(4, kernel_size=3, strides=1, padding=\"same\")(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    \n",
        "    # Output\n",
        "    x_hat = layers.Conv2D(C, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "    return models.Model(y_in, x_hat, name=\"ultra_tiny_decoder\")\n",
        "\n",
        "# 2. MUCH SMALLER DATASET AND BATCH SIZE\n",
        "BATCH_SIZE = 64   # Much smaller batches\n",
        "EPOCHS = 20       # Fewer epochs\n",
        "debug_samples = 1024  # Much smaller dataset\n",
        "LAMBDA_RD = 10\n",
        "\n",
        "X_train_debug = X_train[:debug_samples]\n",
        "X_val_debug = X_val[:debug_samples//4]\n",
        "\n",
        "# 3. FIX DATA PIPELINE TO AVOID END OF SEQUENCE WARNINGS\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices(X_train_debug)\n",
        "           .shuffle(buffer_size=debug_samples)\n",
        "           .batch(BATCH_SIZE, drop_remainder=True)  # Drop remainder to avoid size issues\n",
        "           .prefetch(tf.data.AUTOTUNE)\n",
        "           .repeat())  # Repeat to avoid end of sequence\n",
        "\n",
        "val_ds = (tf.data.Dataset.from_tensor_slices(X_val_debug)\n",
        "         .batch(BATCH_SIZE, drop_remainder=True)\n",
        "         .prefetch(tf.data.AUTOTUNE)\n",
        "         .repeat())\n",
        "\n",
        "# 4. CALCULATE STEPS PER EPOCH\n",
        "steps_per_epoch = len(X_train_debug) // BATCH_SIZE\n",
        "validation_steps = len(X_val_debug) // BATCH_SIZE\n",
        "\n",
        "print(f\"Debug dataset sizes: train={len(X_train_debug)}, val={len(X_val_debug)}\")\n",
        "print(f\"Steps per epoch: {steps_per_epoch}, validation steps: {validation_steps}\")\n",
        "\n",
        "# 5. CREATE ULTRA-TINY MODELS\n",
        "latent_dim = 8  # Much smaller latent space\n",
        "encoder = make_ultra_tiny_encoder(latent_dim)\n",
        "decoder = make_ultra_tiny_decoder(latent_dim)\n",
        "\n",
        "print(\"Ultra-tiny encoder parameters:\", encoder.count_params())\n",
        "print(\"Ultra-tiny decoder parameters:\", decoder.count_params())\n",
        "\n",
        "# 6. CREATE NEW OPTIMIZER FOR NEW MODELS (FIX FOR THE ERROR)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "\n",
        "# 7. SIMPLIFIED TRAINING STEP WITH REDUCED CALCULATIONS\n",
        "@tf.function\n",
        "def ultra_fast_train_step(x_batch, lambda_rd):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Simplified forward pass\n",
        "        y, mu, log_scale = encoder(x_batch, training=True)\n",
        "\n",
        "        y_q = y + tf.random.uniform(tf.shape(y), -0.5, 0.5)  # Simple noise\n",
        "\n",
        "        # Simplified rate estimation (skip complex entropy modeling)\n",
        "        rate_estimate = tf.reduce_mean(tf.abs(y))  # Simple proxy for rate\n",
        "\n",
        "\n",
        "        # Simple but more accurate rate estimate\n",
        "        #y_var = tf.math.reduce_variance(y_q)\n",
        "        #rate_estimate = 0.5 * tf.math.log(2.0 * np.pi * np.e * y_var) / tf.math.log(2.0)\n",
        "        #rate_estimate = tf.maximum(rate_estimate, 0.1)  # Minimum rate\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "        x_hat = decoder(y_q, training=True)\n",
        "        \n",
        "        D = tf.reduce_mean(tf.square(x_batch - x_hat))\n",
        "        loss = lambda_rd * D + rate_estimate \n",
        "    \n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    \n",
        "    return loss, D, rate_estimate\n",
        "\n",
        "@tf.function  \n",
        "def ultra_fast_val_step(x_batch):\n",
        "    y, mu, log_scale = encoder(x_batch, training=False)\n",
        "    rate_estimate = tf.reduce_mean(tf.abs(y))\n",
        "    y_q = tf.round(y)  # Simple quantization\n",
        "    x_hat = decoder(y_q, training=False)\n",
        "    D = tf.reduce_mean(tf.square(x_batch - x_hat))\n",
        "    return D, rate_estimate, x_hat\n",
        "\n",
        "# 8. ULTRA-FAST TRAINING LOOP WITH STEP LIMITS\n",
        "def ultra_fast_training(epochs=EPOCHS):\n",
        "    print(\"Ultra-fast training mode...\")\n",
        "    \n",
        "    train_iter = iter(train_ds)\n",
        "    val_iter = iter(val_ds)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        # Training - limited steps\n",
        "        epoch_losses = []\n",
        "        epoch_distortions = []\n",
        "        \n",
        "        for step in range(min(steps_per_epoch, 5)):  # Max 5 steps per epoch\n",
        "            x_batch = next(train_iter)\n",
        "            loss, D, rate = ultra_fast_train_step(x_batch, LAMBDA_RD)\n",
        "            \n",
        "            epoch_losses.append(loss.numpy())\n",
        "            epoch_distortions.append(D.numpy())\n",
        "            \n",
        "            if step % 2 == 0:\n",
        "                print(f\"  Step {step}: Loss={loss:.4f}, D={D:.4f}\")\n",
        "        \n",
        "        # Validation - limited steps\n",
        "        val_distortions = []\n",
        "        val_rates = []\n",
        "        \n",
        "        for step in range(min(validation_steps, 2)):  # Max 2 validation steps\n",
        "            x_batch = next(val_iter)\n",
        "            D, rate, x_hat = ultra_fast_val_step(x_batch)\n",
        "            val_distortions.append(D.numpy())\n",
        "            val_rates.append(rate.numpy())\n",
        "        \n",
        "        avg_train_loss = np.mean(epoch_losses)\n",
        "        avg_train_D = np.mean(epoch_distortions)\n",
        "        avg_val_D = np.mean(val_distortions)\n",
        "        avg_val_rate = np.mean(val_rates)\n",
        "        \n",
        "        print(f\"  Epoch summary: Train D={avg_train_D:.4f}, Val D={avg_val_D:.4f}, Rate={avg_val_rate:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# 9. RUN ULTRA-FAST TRAINING\n",
        "print(\"Starting ultra-fast debug training...\")\n",
        "ultra_fast_training(epochs=EPOCHS)\n",
        "\n",
        "# 10. QUICK VISUALIZATION TEST\n",
        "print(\"Testing reconstruction...\")\n",
        "val_iter = iter(val_ds)\n",
        "x_batch = next(val_iter)\n",
        "sample_batch = x_batch[:4]\n",
        "D, rate, x_reconstructed = ultra_fast_val_step(sample_batch)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].imshow(sample_batch[0,:,:,0], cmap='gray')\n",
        "axes[0].set_title('Original')\n",
        "axes[1].imshow(x_reconstructed[0,:,:,0], cmap='gray')\n",
        "axes[1].set_title(f'Reconstructed (D={D:.4f})')\n",
        "plt.show()\n",
        "\n",
        "print(\"Ultra-fast debug training completed!\")\n",
        "\n",
        "# Expected speedup: 50-100x faster than original\n",
        "# - 64 samples vs 1000+ \n",
        "# - 8 latent dims vs 64\n",
        "# - 1 conv layer vs 2-4\n",
        "# - 2 epochs vs 5\n",
        "# - Simplified calculations\n",
        "# - Limited steps per epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c37a8c7",
      "metadata": {},
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9ecf3028",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABhMAAAMUCAYAAAC2CNaqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAC1+UlEQVR4nOzdd5hV5b3+/3tP29P7DGWAGZoIig1jbIhYE8WK3Sgox/JVMSaWaBJjj1GTSBqG5CCaoAlYQFKVGGM/HiWJ2AswShum9z6zfn/4mzmMA4vPB9lBh/frurgu3XPPs5+91tPWembvHQmCIBAAAAAAAAAAAMBWxO3sCgAAAAAAAAAAgM83NhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAMAX1s0336xIJLJdv/vAAw8oEomotLR0x1ZqM6WlpYpEInrggQdi9hzYtXjb1OLFi5Wbm6vGxsbYVgzAf0RVVZXS0tL05z//eWdXBQAA7ILYTAAAAP9xb731lr72ta+pqKhI0WhUQ4cO1bnnnqu33nprZ1dtQOrZOOn5l5CQoKKiIs2cOVPr16/f2dXb4ebOnbvTN3A+D3Xo6urSTTfdpNmzZys9Pb338ZKSkt62EBcXp+zsbE2cOFEXX3yxXnnllR1ej/nz52v8+PFKTk7W2LFj9bOf/cz0ezNnzuzTbj/9b/O2+9RTT2nWrFnac889FR8fr5KSkq2Wu3HjRl188cUaOXKkUlJSNHr0aH3zm99UVVVVn1zYcx999NG9uXfffVfXXXed9tlnH2VkZGjIkCE6/vjj9dprr/V77iVLlujYY4/V0KFDFY1GNWzYMJ122ml68803TcdkSz7dvyORiAoLCzV16lT95S9/6ZffPBcXF6ehQ4fqmGOO0T/+8Y8+uc3bSU+ZkydP1pIlS7a7rp/W3t6u73//+9p9992VnJysQYMG6fjjj9e6detc5bzwwgu99aysrOz389///vfab7/9lJycrIKCAs2aNWuLua2d7x/84Ad9co8//rjOPPNMjRo1SqmpqRo3bpyuvvpq1dbW9iuztbVVd955pyZMmKDU1FQVFRXp9NNP3+J8t2LFCk2bNk2DBw9Wenq69tprL/30pz9VV1dXbyYvL0//9V//pRtvvNF1jAAAAHaESBAEwc6uBAAA2HU8/vjjOvvss5Wbm6tZs2Zp5MiRKi0t1fz581VVVaXf//73OuWUU0xldXZ2qrOzU8nJye56dHV1qaOjQ9FodLvf3bAtpaWlGjlypBYsWKCZM2fG5DksHnjgAV1wwQW69dZbNXLkSLW2tup//ud/9MADD6ikpERvvvnmdh3Dz6s999xT+fn5/W6ODoQ6eNrU0qVLdeqpp2rt2rUqKirqfbykpEQ5OTm6+uqrJUkNDQ1655139Mgjj6isrEzf+MY39OMf/3iH1HfevHm69NJLNX36dB177LF6/vnn9dvf/lY/+MEP9K1vfSv0d19++WWtWrWqz2NBEOjSSy9VSUlJn5uxM2fO1KJFi7Tffvvp448/Vnx8/BbfddTY2Kg999xTTU1NuuyyyzR8+HC9/vrrmjdvnvbYYw+tWLFCcXGf/L3VwoUL+/3+a6+9pp/85Ce6++67de2110qSrrnmGs2fP1/Tp0/XAQccoLq6Os2bN0+lpaX661//qqOOOqr392+99Va9/fbb2nfffZWfn6+ysjLdf//92rhxo15++WXtvffe5mPb49P9OwgCbdq0SQ888IDeeust/eEPf9C0adN68z2bIeeff76CINCaNWs0d+5clZeX609/+pO++tWvSurfTjZs2KB58+Zp9erVuu+++3TppZe667q5jo4OHXfccXrppZd00UUXaa+99lJNTY1eeeUV3XTTTdpjjz1M5XR3d2vSpEn64IMP1NTUpIqKCuXn5/f+/L777tNll12mI488UqeeeqrWrVunn/zkJxozZoxeeeWVPmPf5sdmc/vuu2+f+uTn52vo0KE6+eSTNWLECL3xxhv65S9/qVGjRumf//ynUlJSerPTp0/XsmXLdNFFF2m//fbThg0b9Itf/EItLS164403VFxcLOmTjYSDDz5YY8eO1axZs5Samqq//OUveuKJJ3TllVfqJz/5SW+Z77zzjiZMmKCnn35aRxxxhO/AAwAAfBYBAADAf8iHH34YpKamBrvvvntQXl7e52cVFRXB7rvvHqSlpQWrVq0KLaexsTGW1dxh1qxZE0gKFixYsFPrsWDBgkBS8Oqrr/Z5/Fvf+lYgKVi0aNFOqlls7LHHHsGUKVNM2Vi1JU8dPDxt6sQTTwwOPfTQfo8XFxcHxx9/fL/Hm5ubg5NPPjmQFMydO/cz17W5uTnIy8vr91znnntukJaWFlRXV7vLfP755wNJwR133NHn8fXr1wft7e1BEATB8ccfHxQXF2/x9x966KFAUvDHP/6xz+Pf+973AknBP//5z9DnnzVrVhCJRIK1a9f2Pvbaa68FDQ0NfXKVlZVBQUFBcMghh2zzNZWVlQUJCQnBJZdcss3slmytf1dXVweJiYnBOeec0+dxScHll1/e57GVK1cGkoJjjjmm97EttZONGzcGaWlpwW677bZddd3cXXfdFSQmJgavvPLKZyrnvvvuC/Ly8oKvf/3rgaSgoqKi92dtbW1BdnZ2cNhhhwXd3d29j//hD38IJAU//elP+5S1pWOzJc8880y/xx588MFAUvDrX/+697F169YFkoJrrrmmT/bvf/97ICn48Y9/3PvYRRddFCQlJQVVVVV9socddliQmZnZ7/n23HPP4LzzzttmXQEAAHYkPuYIAAD8x9xzzz1qbm7Wr371KxUUFPT5WX5+vubNm6empibdfffdvY/3fC/C22+/rXPOOUc5OTk69NBD+/xscy0tLbryyiuVn5+vjIwMnXjiiVq/fr0ikYhuvvnm3tyWvjOhpKRE06ZN0wsvvKADDjhAycnJGjVqlH7zm9/0eY7q6mpdc801mjhxotLT05WZmamvfvWrev31193H5LXXXlMkEtGDDz7Y72dPPvmkIpGI/vjHP0r65C/Ir7rqKpWUlCgajaqwsFBHH320/vnPf7qfV5ImT54sSf3++vvdd9/VaaedptzcXCUnJ2v//ffXsmXL+v1+bW2tvvGNb/TWZ9iwYTr//PP7fHxIeXm5Zs2apUGDBik5OVl77713v9fa8z0AP/zhD/WrX/1Ko0ePVjQa1Ze+9CW9+uqrfbJlZWW64IILNGzYMEWjUQ0ZMkQnnXRS73ns+Yv1Z599tvcjSg4//HBJ/3fOn332WV122WUqLCzUsGHDJH3yl+1b+micrX0vx8KFC3XAAQcoNTVVOTk5Ouyww/TUU09tsw49x+2qq67S8OHDFY1GNWbMGN11113q7u7ud3xnzpyprKwsZWdna8aMGVv8GJUtaW1t7fdX8duSkpKi3/72t8rNzdUdd9yh4DO+gfmZZ55RVVWVLrvssj6PX3755WpqatKf/vQnd5kPP/ywIpGIzjnnnD6PDx06VImJidv8/fr6eknSoEGD+jw+ZMgQSerzF+Wf1tbWpscee0xTpkzpbTeSNGnSpD4fIyV98lE0kydP1jvvvLPNOhUWFio1NdV8bq2ys7OVkpKihISEbWYnTpyo/Px8rVmzJjQ3ePBgjR8/fpu5benu7tZPfvITnXLKKTrggAPU2dmp5uZmdznV1dX67ne/q1tvvVXZ2dn9fv7mm2+qtrZWZ555Zp9+PG3aNKWnp+v3v//9FsttaWlRa2vrVp938/7co+cddZuf84aGBkm29lZfX6/k5OR+r2PIkCFbbJdHH320/vCHP3zmfgoAAOCx7ZUlAADADvKHP/xBJSUlvTexP+2www5TSUnJFm8ynn766Ro7dqy+//3vh948mTlzphYvXqzzzjtPBx54oJ599lkdf/zx5jp++OGHOu200zRr1izNmDFD999/v2bOnKlJkyb1fszF6tWrtXTpUp1++ukaOXKkNm3apHnz5mnKlCl6++23NXToUPPz7b///ho1apQWL16sGTNm9PnZokWLlJOTo2OPPVaSdOmll+rRRx/VFVdcoQkTJqiqqkovvPCC3nnnHe23337m5+zRcwM+Jyen97G33npLhxxyiIqKinT99dcrLS1Nixcv1sknn6zHHnus94ZZY2Nj783SCy+8UPvtt58qKyu1bNkyrVu3Tvn5+WppadHhhx+uDz/8UFdccYVGjhypRx55RDNnzlRtba2+/vWv96nPww8/rIaGBl1yySWKRCK6++67deqpp2r16tW9N4qnT5+ut956S7Nnz1ZJSYnKy8u1fPlyffzxxyopKdGcOXN6vyPgO9/5jqT+N/Iuu+wyFRQU6Hvf+56amprcx+2WW27RzTffrIMPPli33nqrkpKS9Morr+jvf/+7jjnmmNA6NDc3a8qUKVq/fr0uueQSjRgxQi+99JJuuOEGbdy4UXPmzJH0ycf5nHTSSXrhhRd06aWXavz48VqyZEm/NrI1K1asUHt7u7tdpKen65RTTtH8+fP19ttv97b5mpqaPp/bvjWpqalKTU2VJP3rX/+S9Ekb39ykSZMUFxenf/3rX/ra175mrltHR4cWL16sgw8+OPQ7EcIcdthhiouL09e//nX96Ec/0rBhw7Ry5UrdcccdOvnkk7X77rtv9Xf//Oc/q7a2Vueee67pucrKyvp83M7mamtr1dHRobKyMs2ZM0f19fU68sgjt+s19airq1NlZaWCIFB5ebl+9rOfqbGx0XSMa2pqVFNTozFjxoTmOjo6tHbtWuXl5fU+1tjYGHrjvUdiYqKysrIkSW+//bY2bNigvfbaSxdffLEefPBBtbe3a+LEifrJT36iqVOnbrM8Sbrxxhs1ePBgXXLJJbrtttv6/bytrU3SljeJUlJS9K9//Uvd3d29H20lfbLpOHfuXAVBoPHjx+u73/1uv82rLSkrK5OkPud89OjRGjZsmH70ox9p3Lhx2nfffbVhwwZdd911GjlypM4666ze7OGHH65Fixbpkksu0Te/+c3ejzl6/PHHdc899/R7vkmTJunee+/VW2+9pT333HOb9QMAANghdubbIgAAwK6jtrY2kBScdNJJobkTTzwxkBTU19cHQRAEN910UyApOPvss/tle37WY8WKFYGk4KqrruqTmzlzZiApuOmmm3of6/lokDVr1vQ+VlxcHEgKnnvuud7HysvLg2g0Glx99dW9j7W2tgZdXV19nmPNmjVBNBoNbr311j6PyfCRNDfccEOQmJjY52Nfej6e48ILL+x9LCsry/QRHJ/W81r/9re/BRUVFcHatWuDRx99NCgoKAii0Wifj2w58sgjg4kTJwatra29j3V3dwcHH3xwMHbs2N7Hej4W5vHHH+/3fD0fJzJnzpxAUrBw4cLen7W3twcHHXRQkJ6e3nuOe45TXl5en2PwxBNPBJKCP/zhD0EQBEFNTU0gKbjnnntCX+/WPmKo5zgceuihQWdnZ5+fzZgxY4sfjfPpNvbBBx8EcXFxwSmnnNKvDWz+MSpbq8Ntt90WpKWlBe+//36fx6+//vogPj4++Pjjj4MgCIKlS5cGkoK77767N9PZ2RlMnjzZ1Kb++7//O5AUvPHGG/1+trWPOepx7733BpKCJ554os/vSNrmv8372OWXXx7Ex8dv8TkKCgqCs846K/Q1fFrPR9Ns6yOYwj7mKAg+OTbZ2dl96j1jxoygo6MjtNzp06cH0Wg0qKmp2WZdn3vuuSASiQQ33njjFn8+bty43udOT08Pvvvd7/ZrT1Y97frT/6LRaPDAAw/0y0sKZs2aFVRUVATl5eXBK6+8Ehx55JGBpOBHP/pRb664uDg45phjgoqKiqCioiJ4/fXXg7POOiuQFMyePbs3N2PGDFPb2Lw/PP744719fuzYscGCBQuCBQsWBGPHjg2SkpKC119/fZuv+/XXXw/i4+ODJ598MgiC/+urm3/MUUVFRRCJRIJZs2b1+d133323t16VlZW9jx988MHBnDlzgieeeCK47777gj333NP8sV+zZs0K4uPj+/XtV155JRg9enSfYzFp0qRg48aNfXKdnZ3BFVdcESQmJvbm4uPjg/vuu2+Lz/fSSy8NyI+pAwAAn2+8MwEAAPxH9HzcQ0ZGRmiu5+f19fV9spYv+/zrX/8qSf0+VmX27Nl64IEHTPWcMGFCn3dOFBQUaNy4cVq9enXvY9FotPe/u7q6VFtbq/T0dI0bN267PnLozDPP1J133qnHH39cs2bNkiQ99dRTvR/P0SM7O1uvvPKKNmzY4Hr3Q49Pf+RNSUmJFi5c2PuRLdXV1fr73/+uW2+9VQ0NDb3nTJKOPfZY3XTTTVq/fr2Kior02GOPae+9997il2X3fJzIn//8Zw0ePFhnn312788SExN15ZVX6uyzz9azzz7b54thzzzzzD7vkug5Dz3HPiUlRUlJSfrHP/6hWbNm9cl6XHTRRYqPj9+u3126dKm6u7v1ve99r89fM0syfZH3I488osmTJysnJ6fPx0EdddRR+sEPfqDnnntO5557rv785z8rISFB/+///b/eTHx8vGbPnq3nn39+m89TVVUlSdt1jHo+smfz8//QQw+ppaVlm787atSo3v9uaWlRUlLSFnPJycmm8jb38MMPKzExUWeccYbr9z6tqKhIBxxwgI477jgVFxfr+eef109/+lPl5+frhz/84RZ/p76+Xn/605903HHHbfHjdDZXXl6uc845RyNHjtR11123xcyCBQtUX1+v1atXa8GCBWppaVFXV1e/NuXxi1/8QrvttpskadOmTVq4cKH+67/+SxkZGTr11FP7ZOfPn6/58+f3/n9ycrK++c1v6qqrruqTe+qpp/p8JF18fLzOO+883XXXXb2PXXfddaZ3P2zeFhsbGyV90sb+9a9/afjw4ZKkI444QmPGjNHdd9+9xS/A3tyVV16pr371qzrmmGO2msnPz9cZZ5yhBx98UOPHj9cpp5yi9evXa/bs2UpMTFRHR0efdvjiiy/2+f0LL7xQkyZN0re//W3NnDlzqx+D9fDDD2v+/Pm67rrrNHbs2H6ve5999tHpp5+uAw88UB9++KHuvPNOnX766Vq+fHnvF0DHx8dr9OjROvbYY3X66acrOTlZv/vd7zR79mwNHjxYJ5988haP5+bjCAAAQKyxmQAAAP4jejYGNr9BuSVb23QYOXLkNp/jo48+UlxcXL/stj66Y3MjRozo91hOTo5qamp6/7/n877nzp2rNWvW9Pn4l80//sNq77331u67765Fixb1biYsWrRI+fn5OuKII3pzd999t2bMmKHhw4dr0qRJOu6443T++ef3uYEbpudmY11dne6//34999xzfTZGPvzwQwVBoBtvvFE33njjFssoLy9XUVGRVq1apenTp4c+30cffaSxY8f2u0E6fvz43p9v7tPHvudmWc+xj0ajuuuuu3T11Vdr0KBBOvDAAzVt2jSdf/75Gjx4sOEIfMLSlrZm1apViouL04QJE7br9z/44AOtXLmy33eG9CgvL5f0ybEZMmRIv8/iHzdunOv5gu34PPWeG72b98FDDjnEXU5KSora29u3+LPW1tbQ7yfYUp2eeOIJHXvssdvVx3q8+OKLmjZtmv7nf/6n9+OXTj75ZGVmZuqWW27RhRdeuMVz+9hjj6m1tXWbH3HU1NSkadOmqaGhQS+88EK/89fjoIMO6v3vs846q7dPbG0zw+KAAw7o85FSZ599tvbdd19dccUVmjZtWp+NnZNOOklXXHGFIpGIMjIytMceeygtLa1fmV/+8pd1++23KxKJKDU1VePHj++3mTJhwgR3f+g594ccckjvRoL0yRhw6KGH6qWXXgr9/UWLFumll17Sm2++uc3nmjdvnlpaWnTNNdfommuukSR97Wtf0+jRo/X4449v9RxJUlJSkq644gpdeumlWrFiRe/39Wzu+eef16xZs3Tsscfqjjvu6POzuro6TZ48Wddee62uvvrq3sf3339/HX744VqwYEHvhuEPfvAD/eQnP9EHH3zQW6czzjhDU6dO1eWXX65p06b1+f6Lnr5t2cQEAADYUdhMAAAA/xFZWVkaMmSIVq5cGZpbuXKlioqKlJmZ2edxz43Hz2Jrf7G++U3Z73//+7rxxht14YUX6rbbblNubq7i4uJ01VVX9fsSXaszzzxTd9xxhyorK5WRkaFly5bp7LPP7nPz6IwzztDkyZO1ZMkSPfXUU7rnnnt011136fHHH9dXv/rVbT7H5jcbTz75ZB166KE655xz9N577yk9Pb237tdcc03v9zR8mmdjxsty7K+66iqdcMIJWrp0qZ588kndeOONuvPOO/X3v/9d++67r+l5ttSWtnZDzvI9AR7d3d06+uijt/oX6z1/Wf5Z9dxwr6mp6fNlwRY9N2g3P9cVFRWmY5Gent57I3TIkCHq6upSeXm5CgsLezPt7e2qqqpyvbtm6dKlam5uNn9fwdbMmzdPgwYN6vc9DieeeKJuvvlmvfTSS1u8Mf7QQw8pKyurzztpPq29vV2nnnqqVq5cqSeffNL8OfY5OTk64ogj9NBDD32mzYRPi4uL09SpU3tvUPd8/4UkDRs2zPTl3Pn5+dvM1dXVmd5lkpSUpNzcXEnqPfef/j4T6ZMvpO75vo2tufbaa3X66acrKSmp97tfer7Aeu3atWpvb+99jqysLD3xxBP6+OOPVVpaquLiYhUXF+vggw9WQUHBNt9p0rPZUV1d3e9nr7/+uk488UTtueeeevTRR/t92fVjjz2mTZs26cQTT+zz+JQpU5SZmakXX3yxdzNh7ty5OuKII/ptbpx44on65je/qdLS0j59smeTdWvfywEAABALbCYAAID/mGnTpunXv/61Xnjhha3+hWdpaakuueSS7Sq/uLhY3d3dWrNmTZ+Pmvjwww+3u85b8uijj2rq1Kl9PiZE+uRm1vbe2DnzzDN1yy236LHHHtOgQYNUX1/f58s5ewwZMkSXXXaZLrvsMpWXl2u//fbTHXfcYdpM2Fx8fLzuvPNOTZ06VT//+c91/fXX977DITExcZs3EEePHr3NvwouLi7WypUr+33B6bvvvtv78+0xevRoXX311br66qv1wQcfaJ999tGPfvSj3o9F2Z6/1M3Jyem9Gbm5T797YvTo0eru7tbbb7+tffbZZ6vlba0Oo0ePVmNj4zaPb3FxsZ5++mk1Njb2ubn43nvvhf5ej54vEl6zZo0mTpxo+h3pk3cALFmyRMOHD+/9a3lJ+tKXvtTvWGzJTTfdpJtvvlmSeo/Pa6+9puOOO64389prr6m7uzv0+H3aQw89pPT09H43Zb02bdq0xU2Rjo4OSVJnZ2e/n23cuFHPPPOMZs6c2eedPJvr7u7W+eefr6efflqLFy/WlClTXPVqaWlRXV2d63csel5Pz7tNYuHrX/+6HnzwwW3mpkyZon/84x+SpIkTJyoxMVHr16/vl9uwYcNW37nTY+3atXr44Yf18MMP9/vZfvvtp7333lv//ve/+zw+YsSI3nc/1dbWasWKFdt8d5X0fx+z9uk6rVq1Sl/5yldUWFioP//5z1t8h8OmTZsk9d+UDIJAXV1dfdqbt22uWbNGkvr0UwAAgFjb/g/lBAAAcLr22muVkpKiSy65pPcz3XtUV1fr0ksvVWpqqq699trtKr/nr+nnzp3b5/Gf/exn21fhrYiPj+/38TGPPPLIFm+MWY0fP14TJ07UokWLtGjRIg0ZMkSHHXZY78+7urr63WwsLCzU0KFD1dbWtl3Pefjhh+uAAw7QnDlz1NraqsLCQh1++OGaN2+eNm7c2C9fUVHR+9/Tp0/X66+/riVLlvTL9Ryb4447TmVlZVq0aFHvzzo7O/Wzn/1M6enp7huuzc3Nam1t7fPY6NGjlZGR0ecYpKWlbXFjIMzo0aNVV1fX550zGzdu7Pf6Tj75ZMXFxenWW2/t9y6UzdvE1upwxhln6OWXX9aTTz7Z72e1tbW9NwyPO+44dXZ26r777uv9eVdXl7ktT5o0SUlJSXrttddMeemTG9rnnXeeqqur9Z3vfKfPhshDDz2k5cuXb/Pf+eef3/s7RxxxhHJzc/u8Bkm67777lJqaquOPP773scrKSr377rtqbm7uV6+Kigr97W9/0ymnnKLU1FTz69mS3XbbTZs2beq9qd3jd7/7nSRt8d0tv//979Xd3R36rojZs2dr0aJFmjt3br/vJ9hcz8dYba60tFRPP/10v3dLfFYdHR166qmnlJSUFNMbztddd52pbfzoRz/q/Z2MjAwdd9xxeumll3o3FyXpnXfe0UsvvaSjjz6697Hm5ma9++67fb4bYMmSJf3+9Xy/zG9+8xvde++9oXW+4YYb1NnZqW984xu9j20+vvVoaGjQnDlzlJ+fr0mTJvU+XlZWpmOOOUZxcXF68sknt7r50fNOo9///vd9Hl+2bJmampr6tLfddttNy5cv7zM3dnV1afHixcrIyNDo0aP7lLFixQplZWX1eccJAABArPHOBAAA8B8zduxYPfjggzr33HM1ceJEzZo1SyNHjlRpaanmz5+vyspK/e53v+t308Rq0qRJmj59uubMmaOqqiodeOCBevbZZ/X+++9L2nGfLT1t2jTdeuutuuCCC3TwwQfrjTfe0EMPPWT+7oKtOfPMM/W9731PycnJmjVrVp+/5m9oaNCwYcN02mmnae+991Z6err+9re/6dVXX+1zk86r5+NCHnjgAV166aX6xS9+oUMPPVQTJ07URRddpFGjRmnTpk16+eWXtW7dOr3++uu9v/foo4/q9NNP7/2S0urqai1btky//OUvtffee+viiy/WvHnzNHPmTK1YsUIlJSV69NFH9eKLL2rOnDnb/DLuT3v//fd15JFH6owzztCECROUkJCgJUuWaNOmTX3exTFp0iTdd999uv322zVmzBgVFhb2+e6JLTnrrLP0rW99S6eccoquvPJKNTc367777tNuu+3W50u1x4wZo+985zu67bbbNHnyZJ166qmKRqN69dVXNXToUN15552hdbj22mu1bNkyTZs2TTNnztSkSZPU1NSkN954Q48++qhKS0uVn5+vE044QYcccoiuv/56lZaWasKECXr88cfNf72enJysY445Rn/7299066239vv5+vXre9/J0djYqLfffluPPPKIysrKdPXVV/d7d9D2fmfCbbfdpssvv1ynn366jj32WD3//PNauHCh7rjjjt6PvJGkn//857rlllv0zDPP6PDDD+9TzqJFi9TZ2Rl6M3/lypVatmyZpE/eiVRXV6fbb79d0iffSXLCCSdIkq644gotWLBAJ5xwgmbPnq3i4mI9++yz+t3vfqejjz5aX/7yl/uV/dBDD2no0KH96tVjzpw5mjt3rg466CClpqb2++LgU045pff7CCZOnKgjjzxS++yzj3JycvTBBx9o/vz56ujo0A9+8IM+vzdz5kw9+OCDWrNmjUpKSrb62nv85S9/6b0xX15erocfflgffPCBrr/++n4fG7cjbc93JkiffFzc008/rSOOOEJXXnmlJOmnP/2pcnNz9e1vf7s397//+7+aOnVqn3e9fPrLiCX1vhPhq1/9ap93iP3gBz/Qm2++qS9/+ctKSEjQ0qVL9dRTT+n222/Xl770pd7cL37xCy1dulQnnHCCRowYoY0bN+r+++/Xxx9/rN/+9rd9vnPiK1/5ilavXq3rrrtOL7zwgl544YXenw0aNKh3M+SEE07QHnvsoVtvvVUfffRR7xcw//znP9eQIUN6vyNHkq6//np97Wtf05e//GVdfPHFSklJ0e9+9zutWLFCt99+uxITE/u83uXLl+uEE07gOxMAAMB/VgAAAPAftnLlyuDss88OhgwZEiQmJgaDBw8Ozj777OCNN97ol73pppsCSUFFRcVWf7a5pqam4PLLLw9yc3OD9PT04OSTTw7ee++9QFLwgx/8oDe3YMGCQFKwZs2a3seKi4uD448/vt/zTJkyJZgyZUrv/7e2tgZXX311MGTIkCAlJSU45JBDgpdffrlfbs2aNYGkYMGCBabj8sEHHwSSAknBCy+80OdnbW1twbXXXhvsvffeQUZGRpCWlhbsvffewdy5c7dZbs9rffXVV/v9rKurKxg9enQwevTooLOzMwiCIFi1alVw/vnnB4MHDw4SExODoqKiYNq0acGjjz7a53erqqqCK664IigqKgqSkpKCYcOGBTNmzAgqKyt7M5s2bQouuOCCID8/P0hKSgomTpzY73j0HKd77rmnX/0kBTfddFMQBEFQWVkZXH755cHuu+8epKWlBVlZWcGXv/zlYPHixX1+p6ysLDj++OODjIyMQFLvOQk7DkEQBE899VSw5557BklJScG4ceOChQsXbrGNBUEQ3H///cG+++4bRKPRICcnJ5gyZUqwfPnybdYhCIKgoaEhuOGGG4IxY8YESUlJQX5+fnDwwQcHP/zhD4P29vY+x/e8884LMjMzg6ysrOC8884L/vWvf5nb1OOPPx5EIpHg448/7vN4cXFxbzuLRCJBZmZmsMceewQXXXRR8Morr2yzXK9f/epXwbhx44KkpKRg9OjRwb333ht0d3f3yfQc52eeeabf7x944IFBYWFhb/vckp5zu6V/M2bM6JN99913g9NOOy0YPnx4kJiYGBQXFwfXXHNN0NTU1K/cd999N5AUfPOb39zqc8+YMWOrz/3pMeamm24K9t9//yAnJydISEgIhg4dGpx11lnBypUr+5U7ffr0ICUlJaipqdnqc2/ttScnJwf77LNPcN999/U71pKCyy+/PLTMINj6eLgjrVixIjjqqKOCtLS0ICMjIzjppJOC999/v0/mmWee6TMObM3W5oo//vGPwQEHHBBkZGQEqampwYEHHthvzAiCT/r/0Ucf3TvuZWdnB8ccc0zw9NNP98uGne/N+3oQBEF1dXXwjW98I9htt92CaDQa5OfnB2eddVawevXqfuX+9a9/DaZMmdJnvPzlL3/ZL/fOO+8EkoK//e1voccEAABgR4sEwafeow8AADDA/Pvf/9a+++6rhQsXfuYvcAW+KLq6ujRhwgSdccYZuu2223Z2deA0aNAgnX/++brnnnt2dlXwOXPVVVfpueee04oVK3hnAgAA+I/iOxMAAMCA0tLS0u+xOXPmKC4urs93EAADXXx8vG699Vb94he/iOkX8GLHe+utt9TS0qJvfetbO7sq+JypqqrSf//3f+v2229nIwEAAPzH8c4EAAAwoNxyyy1asWKFpk6dqoSEBP3lL3/RX/7yl97P7wcAAAAAAH5sJgAAgAFl+fLluuWWW/T222+rsbFRI0aM0HnnnafvfOc7SkhI2NnVAwAAAADgC4nNBAAAAAAAAAAAEIrvTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTMDnys0336xIJLJdv/vAAw8oEomotLR0x1ZqM6WlpYpEInrggQdi9hwAgG1jvgAAWDBfAACsmDOAbWMzATvEW2+9pa997WsqKipSNBrV0KFDde655+qtt97a2VXbae644w6deOKJGjRokCKRiG6++eadXSUA2OmYL/p69913dd1112mfffZRRkaGhgwZouOPP16vvfbazq4aAOxUzBd9bdiwQV/72tc0btw4ZWRkKDs7WwcccIAefPBBBUGws6sHADsVc0a4hx56SJFIROnp6Tu7KhgAIgErD3xGjz/+uM4++2zl5uZq1qxZGjlypEpLSzV//nxVVVXp97//vU455RRTWZ2dners7FRycrK7Hl1dXero6FA0Gt3uneRtKS0t1ciRI7VgwQLNnDkzNBuJRDR48GDtvffeevLJJ3XTTTexoQBgl8Z80d8111yj+fPna/r06TrggANUV1enefPmqbS0VH/961911FFHxaR+APB5xnzR38qVK3XllVfqkEMO0YgRI9TR0aHly5dr2bJluuGGG/T9738/JvUDgM875oxwjY2NGjdunOrq6nr/H/gs2EzAZ7Jq1SrttddeGjFihJ577jkVFBT0/qyyslKTJ0/W2rVrtXLlSo0aNWqr5TQ1NSktLe0/UeXPxDNwl5aWqqSkRJWVlSooKGAzAcAujfliy1asWKFx48b1+SuhqqoqjR8/XrvttpteeOGF/0BtAeDzg/nC54QTTtAzzzyjuro6xcfH7/gKAsDnGHPGtl1//fVaunSp9t9/fy1dupTNBHxmfMwRPpN77rlHzc3N+tWvftVn0Jak/Px8zZs3T01NTbr77rt7H+/5DLq3335b55xzjnJycnTooYf2+dnmWlpadOWVVyo/P18ZGRk68cQTtX79+n4fHbSlz6crKSnRtGnT9MILL+iAAw5QcnKyRo0apd/85jd9nqO6ulrXXHONJk6cqPT0dGVmZuqrX/2qXn/99e0+NiUlJdv9uwAw0DBfbNmkSZP6vd04Ly9PkydP1jvvvLNdZQLAFxnzhU9JSYmam5vV3t6+Q8sFgC8C5oxwH3zwge699179+Mc/VkJCwmcqC+hBS8Jn8oc//EElJSWaPHnyFn9+2GGHqaSkRH/605/6/ez000/X2LFj9f3vfz/0cz5nzpypxYsX67zzztOBBx6oZ599Vscff7y5jh9++KFOO+00zZo1SzNmzND999+vmTNnatKkSdpjjz0kSatXr9bSpUt1+umna+TIkdq0aZPmzZunKVOm6O2339bQoUPNzwcA6I/5wqesrEz5+fk7pCwA+CJhvgjX0tKipqYmNTY26tlnn9WCBQt00EEHKSUlZbvKA4AvMuaMcFdddZWmTp2q4447TosXL96uMoBPYzMB262urk4bNmzQSSedFJrba6+9tGzZMjU0NCgjI6P38b333lsPP/xw6O/+85//1OLFi3XVVVfp3nvvlSRddtlluuCCC8w7tO+9956ee+653snljDPO0PDhw7VgwQL98Ic/lCRNnDhR77//vuLi/u/NOuedd5523313zZ8/XzfeeKPpuQAA/TFf+Dz//PN6+eWX9d3vfvczlwUAXyTMF9v2k5/8RDfccEPv/x955JFasGDBdpUFAF9kzBnh/vSnP+mpp57a4e+IA/iYI2y3hoYGSeozGG9Jz8/r6+v7PH7ppZdu8zn++te/SvpksN7c7NmzzfWcMGFCn13qgoICjRs3TqtXr+59LBqN9g7aXV1dqqqqUnp6usaNG6d//vOf5ucCAPTHfGFXXl6uc845RyNHjtR11133mcsDgC8S5ottO/vss7V8+XI9/PDDOueccyR98m4FANjVMGdsXXt7u77xjW/o0ksv1YQJE9y/D4RhMwHbrWdA7hnAt2ZrA/zIkSO3+RwfffSR4uLi+mXHjBljrueIESP6PZaTk6Oampre/+/u7ta9996rsWPHKhqNKj8/XwUFBVq5cmXvN94DALYP84VNU1OTpk2bpoaGBj3xxBP9vksBAAY65ottKy4u1lFHHaWzzz5bDz30kEaNGqWjjjqKDQUAuxzmjK279957VVlZqVtuucX9u8C2sJmA7ZaVlaUhQ4Zo5cqVobmVK1eqqKhImZmZfR7/T32uZ3x8/BYf3/wz8b7//e/rm9/8pg477DAtXLhQTz75pJYvX6499thD3d3d/5F6AsBAxXyxbe3t7Tr11FO1cuVKPfHEE9pzzz23uywA+KJivvA77bTTtHbtWj333HM7rEwA+CJgztiyuro63X777broootUX1+v0tJSlZaWqrGxUUEQqLS0VOXl5Z/pNWHXxncm4DOZNm2afv3rX+uFF17QoYce2u/nzz//vEpLS3XJJZdsV/nFxcXq7u7WmjVrNHbs2N7HP/zww+2u85Y8+uijmjp1qubPn9/n8draWr4AEwB2AOaLrevu7tb555+vp59+WosXL9aUKVN2RFUB4AuJ+cKn5x0JvJsawK6IOaO/mpoaNTY26u6779bdd9/d7+cjR47USSedpKVLl36WKmMXxjsT8Jlce+21SklJ0SWXXKKqqqo+P6uurtall16q1NRUXXvttdtV/rHHHitJmjt3bp/Hf/azn21fhbciPj6+z66wJD3yyCNav379Dn0eANhVMV9s3ezZs7Vo0SLNnTtXp5566metIgB8oTFfbFlFRcUWH58/f74ikYj222+/7SoXAL7ImDP6Kyws1JIlS/r9mzp1qpKTk7VkyRLdcMMNO6rq2AXxzgR8JmPHjtWDDz6oc889VxMnTtSsWbM0cuRIlZaWav78+aqsrNTvfvc7jR49ervKnzRpkqZPn645c+aoqqpKBx54oJ599lm9//77kqRIJLJDXse0adN066236oILLtDBBx+sN954o/czSLfXb3/7W3300Udqbm6WJD333HO6/fbbJUnnnXeeiouLd0jdAeCLgPliy+bMmaO5c+fqoIMOUmpqqhYuXNjn56eccorS0tJ2RNUB4AuB+WLL7rjjDr344ov6yle+ohEjRqi6ulqPPfaYXn31Vc2ePdv1+d0AMFAwZ/SXmpqqk08+ud/jS5cu1f/+7/9u8WeAB5sJ+MxOP/107b777rrzzjt7B+u8vDxNnTpV3/72tz/z5z7/5je/0eDBg/W73/1OS5Ys0VFHHaVFixZp3LhxSk5O3iGv4dvf/raampr08MMPa9GiRdpvv/30pz/9Sddff/12lzl//nw9++yzvf//zDPP6JlnnpEkHXrooWwmANjlMF/09+9//1uS9PLLL+vll1/u9/M1a9awmQBgl8N80d/xxx+vVatW6f7771dFRYWSk5O11157acGCBZoxY8YOqTMAfBExZwD/WZHg0++jAb4A/v3vf2vffffVwoULde655+7s6gAAPqeYLwAAFswXAAAr5gzsyvjOBHzu9Xyp2ObmzJmjuLg4HXbYYTuhRgCAzyPmCwCABfMFAMCKOQPoi485wufe3XffrRUrVmjq1KlKSEjQX/7yF/3lL3/RxRdfrOHDh+/s6gEAPieYLwAAFswXAAAr5gygLz7mCJ97y5cv1y233KK3335bjY2NGjFihM477zx95zvfUUIC+2EAgE8wXwAALJgvAABWzBlAX2wmAAAAAAAAAACAUHxnAgAAAAAAAAAACMVmAgAAAAAAAAAACGX+cK9vfvOb5kLHjBljztbX15ty8fHx5jILCgrM2fz8fHM2PT09Jtnk5GRzdkvfIr8lVVVV5jI3bNhgzv7jH/8wZ9esWWPOdnZ2mrPW41VdXW0u09oOJV9bzMnJMWeTkpLM2VGjRpmz1s/wS0tLM5fpyRYWFpqzKSkp5mxqaqo5a9XR0WHOdnV1mbMbN240Z2+55RZz9vPq6aefNmc948/7779vylVWVprL9LQ5T7uPRCLmbGZmpjnr+UxOa3uORqPmMrOzs83ZYcOGmbOeedBzfktLS0259evXm8tsamoyZz1zW3Nzsznr6TdtbW3mrHWN4ek31jIl33zhmQM8bSYuzvZ3Np6+4OljnmN7//33m7OfZwsWLDBnPWOgdQ2WkZFhLnPw4MHmbGJiojnrGVc8a0tP/6+pqTHlVq5caS7zlVdeMWeXLl1qznquc6x9WrKfs7y8PHOZu+22mznruRbwfFJxa2urOVteXr7Dy/U8f25urjnrmV88128eX/7yl025kSNHmsv0tFnP2uGuu+4yZz+vfvvb35qz69atM2crKipMOc96orGx0Zz19Lva2lpz1sMzZ+25556mnGdN4zle7e3t5qzny5E9c7w167km9Kxxuru7zVnPGsNzX8wzF1vvh3iuyTzXj55rDE+5nvnNes487cDzujzXIzfddNM2M7wzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhEqwBpOSksyFdnV12SuQYKtCe3u7ucyamhpzNjc315zNzs42ZwsLC83Z5uZmc7alpcWU85yv1NTUmGSTk5PNWU+bycjI2OHPn5eXZ842NDSYswUFBeZsfHy8OZuTk2POWs+Z59ympaWZs9bzJfmOQUpKijlr7Q+ecaajo8Oc9dR1IPAcxyAIzFlru2tsbDSX6amrZ0zxvK76+npzNjEx0Zzt7u425Tx1raioMGfb2trMWc9Y6Tle1qx1bpWk9evXm7OeY+CZB5uamsxZT7u1Zq1rNyl2r8vDcx6s40w0Gt3hZUr2fjuQ1NXVmbOedYJ1bIvV+fFcj3jmrczMTHPW89qs86HnGMTF2f9uzbMO9fTp/Px8c9Z6PTB+/HhzmZ7XVVZWZs565m7PdaHHhg0bdvjzFxUVmbOeuciz3vOMSda+6zm3nvklPT3dnB0IPOfcc2w6OztNOc/4F6t+57kG9YyVHlVVVaac516b556Y59zG4p6Bh2cO8Fznee6xeF6X5zxY5wDJPq552rdnXI/VsfVcZ7W2tppynjnAU1fPmsxU3g4tDQAAAAAAAAAADDhsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFBsJgAAAAAAAAAAgFAJ1uC6devMhcbF2fcootGoKdfW1mYuMxKJmLMeSUlJ5mxmZqY5Gx8fb862trbu0JwkBUFgznZ2dpqz7e3t5mx3d7c5m5iYaMoNGjTIXKanru+995456zm22dnZ5uwee+xhzo4bN86US01NNZfp6Y+e9u3pu9axQ7K3W087WL9+vTlbV1dnzg4EnvmiqqrKnG1qajLlEhLMU5uam5vN2YqKCnO2vr7enPXMLfn5+eZsVlaWKeeZszs6OszZTZs2mbOeMaWystKctZ6HxsZGc5me/uw5Xta5TfLN8Z5sQ0ODKedZC3R1dZmzGzZsMGeHDh1qzqalpZmz1jnL028982us1rCfZ55x2DO+FxQUmHLWsVLyncvi4mJz1nMMPGsVz9hmnbs9c6FnvE5OTjZnx44da8561tfW/h+rayfPWOUpNyUlxZwdNWqUOTty5EhTzrMm8rRvz9rB03c959da3/LycnOZGRkZ5mx6ero5OxBY1ymSb76w9hHPOtxzP8jTRz19xDNOeK4HrOOqZ23rmYs9a8C8vDxzNicnx5wdPny4KecZ1z1rAc/9M0/78mQ97cs6D3jWGJ6+4GmLnmPgaTPW8xurfus5Xqbn3qGlAQAAAAAAAACAAYfNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAECrBGqyrqzMXWl1dbc4OHjzYlBs6dKi5zNzcXHM2Ls6+n9Ld3W3OeiQmJpqz2dnZO/z5m5qazFlPXVNTU83ZlpaWHV6u51jV1NSYs0lJSTHJ5uTkmLOtra3m7JAhQ0y5tLQ0c5lBEJizHp7X1dnZac6Wl5ebcs3NzeYyN2zYYM5+8MEH5uxAUFtbG5NyMzMzTbmUlBRzmZ4xrbKy0pyNj483Zz1tubGx0Zzt6uraoTlJamtrM2c9/dmT9axHrHNLfX29uUzPuiEhwbzMco0/nj7mGa+t5Xr6WKzGdQ/PMbCuHYqKisxleuZ3TzsYKCKRiDnrGYOsYjUGetbBnmuMiooKc9YzVqxfv96Uq6qqMpfpucbwXL95jq0nax0rrOtKScrIyDBn09PTzVkPzzVRVlaWORuNRk05z7y5bt06c9ZzHjzXOZ41nHUd6ZmHPGuH9vZ2c3Yg8KwBPefRytOXPOfR0/c946pnfeu5JrL2/eTkZHOZnrWlZ1z1nAdPHfLz8005z7n9+OOPzVnP/SvP2tJzvDxt0SoW9zsl3xjsuYfnaePWOsTq3sCOXkPzzgQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABCKzQQAAAAAAAAAABAqwRpsaGgwF9rc3GzOxsXZ9jOysrLMZWZnZ5uzVVVV5mx8fLw5m5BgPrSKRqM7vA7JycnmMlNTU81ZT10zMzPNWY8gCEy5yspKc5mdnZ3mbF5enjmbm5trzhYWFpqznv6QmJhoyqWnp5vL9PSxjo4Oc/bjjz82Z9va2sxZa1uoqakxl7lmzZqYZAeCjRs3mrOetmzNjho1ylymp++vXbvWnG1vbzdna2trzVnrnClJLS0tppynrp75wjOmeMYJT32bmprMWSvP3Nba2mrORiIRczYpKcmc9Yxr1jbjmQOsc5Dkq2t3d3dM6mBti/n5+eYyi4uLzVnPenugSElJMWc962vrufRcC3j6aV1dnTnrGavee+89c7a0tNScXb16tSnnmeM9x8AzZ2RkZJiznv5vvcbwzNueOcs6Bku+Y+C5LrQeA8l+XZiWlmYu0zMeVFdXm7OrVq0yZz31tY7vnrHDw7MeGAg2bNhgznZ1dZmz1nVNrO6xeOa2wYMHm7Oetjxo0KAdXq5nTeMZezzt3jMHxOI6x3Nv1HO8PO3bM2d5ro0958zKsxbwHINY1FXyXetZxw/Pfa76+npz1nO9bcE7EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQCg2EwAAAAAAAAAAQKgEa7CpqclcaFycfY8iEomYcp2dneYy6+vrzdlVq1aZs++88445+95775mzRUVF5mx+fr4pl5KSYi6zvb3dnPWUG6s6dHV1mXK1tbXmMj1tNiMjw5zNzs42Z9PS0szZhARz11VjY+MOL9PabyWpra3NnPWcM0+5SUlJplxlZaW5TE/W2mYHinXr1pmzFRUV5uygQYN2aE6SkpOTzdnc3FxzNlb9ydqWJXsfSUxMNJcZqzmgurranK2pqTFno9GoKecZ12O1HqmrqzNnPWsyz2uzKigoiEnWs3ZKTU01Zz1jsHVM+uijj8xlBkFgznrWDQOFZ/0zbNgwc9Y6Dnv6nmf8WblypTnrGQM96w/P+tra9j3zm2c+9uju7jZnPefXOl7ut99+5jI9xysW19CSbx6wXjdIUmtrqynnWWulp6ebs541kYenDtZsQ0ODucyWlhZz1rPOGAjKysrMWc9ayXqPxdrmJSkvL8+cjY+PN2c7OjrMWc+awnNPyrrG94xpzc3N5qxnTLFeC0i+OXPNmjWmnOd1edarnusszzrU07489bXOWZ424+k3nrnN074885C1LXrWOJ61gCdrwTsTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAqARrsL6+3lyoJ1tVVWXKRaNRc5lBEJiz69evN2dbWlrM2aamJnM2EomYs83NzaZcenq6ucyOjg5zNiHB3GSUlJRkzqakpJiz1vp66pqTk2POFhUVmbOe8xAXZ9/ba2trM2erq6tNudbWVnOZZWVl5mxnZ2dMsp7jZeUZOzIzM83ZCRMmbE91vrBSU1PN2fLycnPW2vcLCgrMZXqynrnNcwySk5PN2ViM7Z5+5xl7PDz9OT4+3pzNyMjYnuqEso6pklRXV2fOVlZWmrOetjho0CBz1jquedZDnuPlWTt51g3Z2dnmrPUY5ObmmsvMy8szZwsLC83ZgSIxMdGcta6DJam9vd2Uq62tNZdZU1Njznr6tGcN5un/njW+dd7y9CfPsfWswTzr9rS0NHPWuibwXAt45jdPm9m0aZM529XVZc562qIna+WZ4wcPHmzOetZPWVlZ5qx1LvIcK8/a2DNvDgSeMXjt2rXm7MaNG005z3WDp316rhs8/dlzveoZq6z3eTzXOJ57R571vWeN0djYaM5a24znuHrmNs862DMPesZgz3Wh9fx67uF5+oJ1TSj5joHnXq71+slTV8/x8mQteGcCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIlWAOJpijqq6uNmcbGxtNufLycnOZBQUF5mxtba05Gxdn33tpa2szZzds2GDONjQ0mHLRaNRcpqeunuNVWVlpzjY1NZmz1vPgOQZdXV3mbCQSMWeDIDBnW1pazNnm5mZz1lrfxMREc5lVVVXmrOc8DBkyxJz1jEllZWWmnLV/SVJubq45O2zYMHN2IBg6dKg562kf9fX1ppynL3nGnqSkJHPWM06kpqaasykpKeastb6eY9DR0WHOZmdnm7Oe+TU+Pt6ctY7tnr7vWY945pb29nZz1tNvPPOQtd2Wlpaay/S0r0GDBpmzycnJ5mx+fr45W1hYaMp51pqe58/JyTFnBwrPeFlTU2POrl271pTz9L3u7m5ztrW11Zz11MGzbveMQWlpaaZcSUmJuUzP8fKsQz3zgGcu2n333U254cOHm8v0jMH/+te/zNn169ebs54246mvdbzytEPP9aNnzvCMw7GYNz3P77nefvHFF83ZgcAzB3iuV+vq6kw5z7nxXDfE6nrE0/c9a3xPuVae60frfCX5zpln3vaMVVaeezzFxcXmrOdehKcdeK6JrNdanvnds37MyMgwZz1zlqfvWnnaYWdnpznrObcWvDMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACEYjMBAAAAAAAAAACESrAGR48ebS40Go2as11dXaZcdXW1uUxPtru725zNysoyZ4MgMGc7OjrM2aSkJFMuOTnZXGZzc7M5W1VVZc42Njaasw0NDeZsenq6KZeYmGgu03MOPMcrPj4+JnXwtFvr8fKUWVZWZs562mJCgnlIUkZGhjmbl5dnynnad21trTnrObcDwdChQ81ZT/uIi7Ptf3vaclNTkzlbUFBgznq0tbWZszU1NTu8XOs87GUdeyQpOzvbnB0xYoQ5az2/b7/9trlMz7hubbOSry942rhnrIpEIqacp66eNaFnXPe0mdzcXHO2sLDQlLPOK5KUn59vznr6zUDhmSOtbdTDM656+rSnn6alpZmznnWo53okJSXFlMvMzDSX6VmLe66z2tvbzVnPuNLZ2WnKvfPOO+YyPXPs+vXrzdmWlhZz1nOd5Tm21nW7px3W19ebs55j4Om7OTk55qx1LvC0A8/85llDDgQffvihOesZ263rBM845WmfHp51qOc6p6KiwpxtbW015WK1DvacB8+Y4pm3refXM/557oV4eM6DZ972HC/r2sUzB3nWzJ66eu4ne9ZkVtb+Je3cNTTvTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKESrMH4+HhzoZFIxJxtbm425SoqKnZ4mZKUnZ1tzqamppqzDQ0N5qzn2HZ3d5uzVnFx9j2lWDy/t1zrsW1sbDSX2draas56zldTU5M566mvp49Z6+Aps6WlxZzNyMgwZz19NyUlxZzNysoy5XJycsxletqM59gOBMnJyeZsQoJ5GlJSUpIp52kbnnPuyXqOQVVVVUyyZWVlppynfXrmwbq6OnPWc7yKi4vN2c7OTlMuVnOb59gOGTLEnPWMwZ5sQUGBKdfR0WEu09oOJSkIAnO2pqbGnLXOAZKUnp5uymVmZprLbG9vN2c964aBwtruJF/by8vLM+U2bdpkLtPTRj3937O2TEtLM2c9Y5v1Wmvt2rXmMnNzc81ZTz+1nlvJd2yt1xie61JPm/XM8bG6fisvLzdnret2zznwrK89x8tTrmedYT2/nuPqWRONGjXKnB0IPGNwLLKevtTW1mbOes65h2et1NXVZc5a15ae+aq6utqc9ZzbWF2zW6+JPOOfZ21pvcaRfHOWh6eNW9fCnnboef76+npz1nMv13OdZT0GnusGj8TExB1aHu9MAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAodhMAAAAAAAAAAAAoRKswdbWVnOhzc3N5mxtba0pV11dHZPnr6urM2fXrFljzqanp5uzQ4YMMWetx2vo0KHmMpOSkszZnJwcc7alpcWc9ZyHzs5OUy4Igpg8f0NDgznrObapqanmbCQSMWetry0lJcVcpud1eeq6ceNGc7apqcmcLS8vN+UaGxvNZdbX15uznr4wEJSVlZmznvPY1dVlyqWlpZnLLC4uNmc9Y7Wnj3R0dJiza9euNWdXr15tymVkZJjLHD9+vDnb3d1tzsZqjrf2U0+ZcXH2v8PwHNu8vDxzNjEx0Zy1rhsk+zzU1tZmLjMrK8uc9cyvycnJ5qx17JCk9vZ2U85zDDzte1fkac+eccW6Fh82bJi5TM/1kKeNeMYVT9u3rpkl+3rRM8d7eNbBnnYQHx9vzlrHCs96sbKy0pyN1VjhWWt51uLW8dJzbj1rZk8f81yPvPfee+ZsQoLtdornOsszZxUWFpqzA4Fn/CsoKDBnret2z/NnZmbu8OeXfPOQh6fvWecWz7rOcz/GM6Z4+p5nLraWG41GzWV67iF65veqqipzNlbzq/V6wHNN5rmG9twb9Lwu6xzgqYOnfVvnYcl3vCx4ZwIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAiVYA3Gx8ebC21paTFnu7u7TbnBgwfv8DIlqbKy0pytqqoyZxsbG83ZSCRiznZ0dJizVtnZ2easpx2kpKSYs0lJSeZsYmKiKdfV1WUus7m52ZxNSDB3G9fr8tTBIxqNmnKecxsXZ9+HrK+vN2dbW1vN2bq6OnPWemw9bcZT11id28+rjz/+2Jz1jMHJycmmnKd9erKe8+jJetpdW1ubOWt9bXl5eeYyPfOFZw7w8MzFa9euNeXa29vNZXrmAGublaQgCGJSh6ysLHO2qanJlPMcL88x8LDObZKUnp5uzlrXm561bllZmTnrWcMOFJ61kqftW9eLHp5z6Vmze857Q0ODOetZK1nrG6v1T3V1tTnruXbyjEHWtXBtba25TE+b8RwvT/vy1NfTbzo7O3f483t4zm1qaqo565njrNf8nmOQkZFhzhYVFZmzA4HnPHr6UyzmC8+a3bMG9GQ9a0DPNVEs5gvPfOU5X2lpaeasZz1iXVvm5uaay/ScL88cYF3fe8v1XOtlZmaactZ5RfLV1dMOPDx1qKmpMeU8a8JYrTUteGcCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIlWANDh06NCYVKC0tNeWSk5PNZUYiEXM2JSXFnO3u7jZn6+vrzdnq6mpzNi0tzZRraGgwl5mammrOxsfHm7NdXV0xqUNWVpYp5zkGHp5j0NraGpNyPce2sLDQlMvPzzeX6XldnqynHcTF2fdC29radniZnvOVkZFhzu5qPOc8IcE2ZXnG6rq6OnO2trbWnG1qajJnq6qqzFnPuJabm2vKWccIyTcXDx482Jz1zNudnZ07PGudWyVfm/W8Lk+7XbdunTnrOQ85OTmmXKxel+c8eNqi55xZj0FZWZm5zObmZnN2V+Rpo9Z5QJKCIDDlPHO0Z/xJSkoyZz1tZOPGjeasdf0j2fuU53V5zpe173nL9axDrXOsZ97u6OgwZz3rUE+5sbomsvax9vZ2c5meNcnw4cPNWc/xsr4uyX4dX1NTYy4zVtdDA4Hn3k16ero5m5iYaMp51imeunrGNM81aKzuoVn7tKcvedqy516I5/rNU25RUZEp5zkHnvsxsRinpNitWa19zJqTfGOlZ+3iOQ+eMcHaviorK81leuZ3T/u22LVmHwAAAAAAAAAA4MZmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACJVgDaanp5sL7e7uNmeHDRtmynV2dprLjEaj5mxmZqY5m5BgPlwuw4cPN2eTk5NNufj4+B1epuQ7D5FIJCZ1yM7ONuXS0tLMZdbV1Zmzra2t5mxLS4s56+k3cXH2fcDc3FxTrrCw0FxmZWWlORur1+URBIEp19XVZS7T87o85Q4E9fX15mxSUpI5W1tba8qtWLHCXObEiRPN2fb2dnO2sbHRnPWMq56sdd5OTEw0l9nW1mbOetq9px14pKSkmHKxmq8884Xn3HrmlurqanPWuh7xHANPm7GeL0lKTU01Zz3nwdp3Pevijo4Oc9YzzgwU1nWK5Gsj1n5SVVVlLtOzVvK0UU+fts6Fkm98t762oqIic5mesWJn92nJvr7duHGjucyGhgZz1jOueM5tTk6OOeupb01NjSnnObeeY2Bd30u+dbsnax2/Bg8ebC6zubnZnPWMBwNBfn6+OeuZL6zn3LNe9Yx/nrbsucfhuSfkeW3WNatnbvNkPeOfZw3mWbNaz295ebm5TM/9Rs/aaciQIeasZx3qmQtLS0tNOU878Nz39Rwv6/1GydfHrPdIPNduTU1N5uyOvp/NOxMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAEAoNhMAAAAAAAAAAECoBGuws7PTXGhycrI5m5eXZ8o1NTWZy+zu7jZnExMTzdnc3FxzNiHBfGg1YcIEc9Z6bDdt2mQus7q62pxtaWkxZ5OSksxZz/GyZoMgMJeZmZlpznpeV0NDgznraYuxOLaeMj19wXMePG3R08/T0tJMOc/YVVNTE5PsQOBpy3Fx9j1taxttbm42l7l69Wpz1tM+PK+rqqrKnPWMlenp6abcRx99ZC7T2pckqaOjw5wdPHiwOes5tvHx8aZcWVlZTJ4/Go2asxkZGeas9dxKvvVbJBIx5Tx93JMdMWKEOeuZs0pLS81Zz/rJqr293Zz1nK+BwrO2tLZRT7ldXV3mMmO1XqyrqzNnPf3fM66MHTvWlCsuLjaX6Tm2nrmovLzcnPXMsRUVFaacZ5zwnNuCggJzdvjw4easp82kpqaas9bz61kH19bWmrOtra3mrGf9kpKSYs4OGjTIlMvOzjaX+fHHH5uzu9o1RqzOuXVs96zDPeNfrNaWnjVYLNYfnuPluc7yvC7PGt+zHrHOF1lZWeYyPfc3rNc4km894llnxWJN1tjYuMPLlHz9ZujQoeasp9161iOxeP4d3cd5ZwIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAiVYA2mpqbaC00wF6uOjg5Trru721xmeXn5Dn9+SYpGo+ZsXl6eOZuWlmbOpqSkmHK1tbXmMj2vKzc315zNzMw0Z2tqaszZpqYmU66hocFcZldXlzkbBIE5O3z4cHPWcx487TYuzrZn2NnZaS4zPj7enPWMB55j6zm/1nZbUFBgLjM5OdmcbWtrM2cHAk9b9hybSCRiyrW2tprLrK+vN2c944RnzrSOaZKvj1jbqKc/e86X59gmJSWZs9Z2INnHyvb29pg8v2dc9awbPHOxZ361ti/P+OcZDzzrBk9/9MyZ1jbu6TeePl5ZWWnODhQtLS0xyVZVVZlynjWz57ynp6ebs562n5GRYc7m5+ebszk5Oaac9VpE8s0Dzc3N5qxnnq+rqzNnre0rVtcYnnbgWWd45jhPuy0sLDTlrH1R8q1zRo4cac4OGTIkJnWwjtmevuDhuUcyEGRlZZmznr5nnaeHDh1qLtN6DS75+ohnTeNZs3rGCev441kDZmdnm7Oeuq5du9ac9azXrMe2oqLCXKZn7InV9bbn3o3nGsN6HGJxb0DyrRs8Y4enn5eVlZlynnHdcy/Z074teGcCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIlWANpqSkxKQCbW1tplx9fb25zOrqanM2IcF8CJSammrONjU1mbMffvihOZuWlmbKRaNRc5lDhgwxZzMzM83ZoUOHmrOrV682Z1etWmXKedpMV1eXORsfH2/O5uTkmLPWcytJ6enp5uywYcNMuZaWFnOZGzZsMGcbGhrM2crKSnO2pqbGnLX2xyAIzGV6xsTk5GRzdiDwjKsZGRnmbHNzsynn6aNJSUnmrGecqK2tNWc97cMzD1nbc2dnp7nMuDj73yB4+misrF+/3pTz1HXMmDHmrOfcetptJBIxZ2PRbru7u81l5uXlmbN1dXXmrGfO8rRx65iQn59vLtOzJrS22YGkvb3dnPWslazt1NPuPGOgZ83saU+edXtWVpY5m5iYaMp5xpSOjg5ztqioyJz1HFtPHazXDp5rDE+b9bSDWM0v1nYg2fvYoEGDYvL8nnK/9KUvmbOeOe6dd94x5Tz9xrM2raqqMmcHAs81hqfvW+chz/rL05Y9rytW15We4+VZV8WiTM/1UKzmTCtPO/CsbTdu3GjOetqth2f8aWxsNOU8x8DTbzzta82aNeash/W1ee4hDh48eIc/vxXvTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKESrMGMjAxzoZFIxJytrq425bq6usxlxsXZ90hycnJiUm5ra6s5W1NTY852dnaacrm5ueYys7Ozzdn8/Hxz1nO86urqzNmmpiZTrqWlxVxmQoK5KygpKcmc9byurKwsc3bw4MHmbGZmpilXVVVlLrOsrMycbWxsNGc9fcFzfq39xjN2ec6Xtc0OFJ5xtbCw0Jy1to/u7m5zmfHx8easp314xr+ioiJz1nNsrcfB0z5TU1PN2ba2NnP2/fffN2fb29vN2crKyh1eZl5enjnrmV89c4v1dUnSpk2bzFnreO2ZMxMTE81Zz3nwzBfNzc3mrPW1edalntdVX19vzg4UnjaalpZmzlrXYJ51SjQaNWc95XquswYNGmTOeuYMK0/f6+joMGdjtQbzjEHW/p+cnGwuMyUlxZz1tBkPz5rEc86s14UNDQ3mMj1rEs/1yNq1a83ZgoICc/bAAw805TzrJ8/rqq2tNWd3NZ5+ah2DPfO551rVcz3iqYP1GthbB+u9rlg9vyfrWbN61hjWOnj6vud4eXjajOd6JBZZz5jmOV6e8cCzbvC0xVjcy/WsHz3rNwvemQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEKxmQAAAAAAAAAAAEIlWIMpKSnmQocPH27OJiYmmnK5ubnmMtevX2/Odnd3m7MNDQ3mbHx8vDkbiUTM2c7OTlOutrZ2h5cpSR0dHeZsW1ubOfvuu++as3V1daac53x52nc0GjVnre1bklpbW81ZTxvfuHGjKVdVVWUus7Gx0Zz1tIPq6mpztrm52ZxNTk425Tz9ZtCgQeZsWlqaOTsQFBUVmbOe45iVlWXKxcXZ98mt44mXZ1z19CdrW5bs85tn/MvIyDBnPf1p1apV5qzneCUk2JY5nj7qeV1BEJizo0aNMmc964by8nJz1lrfmpoac5me89XS0mLOVlZWmrNdXV3mrOecWXnmK0/7Gijeeustc7a9vX2HZ+vr681leua37Oxsc9bzujzrNU+5Vps2bTJnPf3fU1fP2tLTp6x1yMvLM5fpmTc91xhJSUnmrKcteuaX1NRUU86zJvKcL8+ayNMWPce2uLjYlBsxYoS5TM+6LDMz05wdCDz3Ijzt3tqWPfeOrGtQydfvPHXw3JNKT083Z6319fRnT9azXvTw9H3r+OOZAzxjpWdt65lfPX3Mcx1tXWt56uq51+Y5tp725enn1v7oWePEqh1Y8M4EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQKsEaTE5ONhealpZmzhYUFJhy3d3d5jKrq6vN2Y6ODnM2JyfHnE1MTDRnPce2vb3dlGtqajKXuWnTJnP21VdfNWfj4+PN2VicM0+bSUpKMmc952vEiBHmbDQaNWfXrFljzjY0NJhynjbj0draas7G6pxlZ2ebcqmpqTF5fk9fGAhSUlLM2by8PHN2yJAhplxcnH2f/OOPPzZnGxsbzdmurq6YlOt5bUEQmHKFhYXmMq3jieTr+y0tLeZsQoJ56WIer61jhGQ/rpLvfHn6Qn5+vjlbV1dnzlqPV21trblMz9zm4Tm2nj7W3NxsynnmC8/c5skOFJ51lef4WNfinjHFM5+np6ebs54+vXHjRnP2jTfeMGetx8FzDjzjpWce+Oijj8zZDRs2mLPW1+a5zvO0mba2NnPWOlZJvnWZZ31rHVs9awfPeO05tpFIxJz1zHFr16415Tzn1nN/wjN+DgSec+7pp9b24Xl+z9rSU25nZ6c562l3nj5iXQN52nJNTY056xlTYjX+WXnaoef5PXOm5z6P5/rNsx6xXo942qHn2HrWLrG6f+V5bVaeNuN5XRa8MwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRiMwEAAAAAAAAAAIRKsAbj4+PNhba2tpqz1nKTk5PNZaalpZmzzc3N5mxXV5c5m5WVZc4mJSWZs/X19aacp64dHR3mbGNjozkbjUbN2ZSUFHM2EomYcp7X1d3dbc566pqenm7O5uTkmLM1NTXmrLXNeM6ttUzJNx54ZGZmmrN5eXmmnOcceF7Xxo0bzdmBoKWlxZxNTEw0Z7Ozs005T9soKCgwZ+Pi7PvvbW1t5qyn73nGtSAITLlNmzaZy/TMxZ55qKSkxJz1zJmdnZ2mnGesjtXc4mlfnnXOiBEjzFnrmszTZpqamsxZa5uVfMfWsx6x8owznjZbXl6+PdX5QvOMl+3t7eZsfn6+KTdkyBBzmZ7+73ldnnWV5xh46uupg5WnTyckmC9LzedWsl83SPbxKiMjw1ymZ/zxjO2etZanjVvnTcnevjzn1rO+9ozDsbrO+uc//2nKVVVVmcv0zG+e7EDguXfjWV9b14Getuy5xklNTTVnPefc059jcf/IM1951oue+dWzBvO0L+v85inTM194rhtqa2vNWc958IzX1vbludb09DHP/Oo5D9XV1eastc14xhlPO/CsG0zPvUNLAwAAAAAAAAAAAw6bCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIFSCNdjZ2WkudMOGDeZsJBIx5eLi7Pseubm55qxHRUWFOdve3m7OxsfHm7Otra2m3MaNG81ltrW1mbPd3d3mrOececqNRqOmnKfNerJVVVXmrEdTU5M529LSYs5a22Jtba25zOrqanPWY8iQITHJDh061JTztMPKykpz9s033zRnB4KPPvooJuVaz2NjY6O5zCAIzFnPmGad2yQpMTHRnO3q6jJnrfOFR3Jy8g4vU5LS09PN2ZSUFHM2IcG2zPGcr02bNpmzDQ0N5qxnnBg5cqQ5m5OTY85a1yOxmIMk39zi6QuedaG1fQ0ePNhcZnZ2tjnrWWsOFGlpaeasdQ0oSUlJSdtTnVCec+mZXzzn3dP/POtba1+tr683l+npp6mpqebsqFGjzFlPO/j4449NOc+1m2d+8YzXnr7gmTc7OjrMWes589S1ubnZnPWsSTzHwLMus14/WduWJK1Zs8ac9ayfBgJPf/ZcW1vPuactx+pehOd6JCsrKyblWu8feV6XZ27xzIOe8ToW/clzf8EzX2RkZJizntflGYM9ayLrdamnzXjmAM+4XldXZ856rvXy8vJMOc+62DNn7+h7A7wzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhEqwBtva2mJSgSAITLmEBHNVFY1Gzdn4+Hhztqury5xtaWkxZzs7O83Z5ubmHZqTpMTERHM2JSXFnB0xYoQ56zln1dXVplxtba25zO7ubnO2vr7enF23bl1M6uA5XpWVlaZcQ0ODuczk5GRzNikpKSblZmdnm7PWNm49VpK0du1ac9ZzbgeCjo4Oc/bdd981Z6uqqky5nJwcc5lxcfY9dc8c4OEZVz3H1jpve54/Pz/fnC0oKDBnPX2/qanJnLXOr541jnXdIkk1NTXmrKcveI7XxIkTzVlr+/IcL89c7Ml65hZPW8zKyjLlPOvHjIwMc7awsNCcHSg8Y9D69evNWeuYnZ6ebi7TM2dEIhFztq6uzpy1zoXeOljXKp5+6rl+81w7edZVZWVl5mxjY6Mp57nG8cwZnnHNc24958Gz1rGOg545o729PSZZT/vKy8szZ61twdNvPvroI3PWMxcNBJ5z7lmDWdeWnjWo5/k96zrrOkXyrSk8dbDOhZ71j2ct4Llv4WkznvtynnHVynO8Bg0aZM561i6erOe+mPU+i+d+jOe62NNmPK+rtbXVnLW2L8/r8jy/py9Y8M4EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQis0EAAAAAAAAAAAQKsEabGxsNBcajUZ3eDY+Pt5cpifrkZycbM52dHSYs55jW1tba8rl5OSYy0xNTTVnu7u7zdmCggJz1nNsExMTTTlPXTs7O83ZlJQUc9ajoaHBnPW0mba2NlMuKSnJXKbnfHnal6fvevpYTU2NKVdXV2cus6mpyZzt6uoyZwcCax+VpPb2dnPWeh49bS43N9ec9fD0EWsflaSqqipz1tpHWlpazGUmJJiXDa55KBKJmLOescp6DKxzqyQVFRWZs572FRdn//uOIAjM2fLycnPW2ndbW1vNZXrat+d1edqMZ7y2HgNPX/DMV1lZWebsQOFpIx7WucBz3eKZzz3zm6ft19fXm7OedY21X3vqOmjQIHPW06eam5vNWc+8aeXp0x6e6xGPWK2vrX3X0xc81zieNlNZWWnOesYka9azNh45cqQ5G6vr0s8rz3zuOefWtuS5v+BZB3vmFs8azLPGj8X1k+d1ea6dMjIyzFnPnOXp+9bjlZ2dbS5z6NCh5uzYsWPN2Vjd59mwYYM5a223sbon5pmHPNdkHtY1mecceK6HPOtdC96ZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQiVYgykpKeZCo9GoOZucnGzKJSYmxuT5Ozs7zdmEBPPhctWho6Njh2dTU1PNZWZlZZmzSUlJ5uzgwYPN2e7ubnM2EomYcnFx9r2yxsZGc7ahocGc9fQbTzuwHgNPHVpbW81lenjaYmZmpjnrOV6lpaWmnKcdeNpsWlqaOTsQWMd1ScrJyTFnPcfcKggCczY9Pd2c9dS1ra3NnM3IyDBnrfOQZx7Mzs42Zz1zpqcdeMb2iooKU66rq8tcpofnGHjal2ceqq6uNmet47WnzXrGP89ar6qqypz1tPGamhpTLj8/31ymp9961k4DhWfuLSgoMGc969tY8LRRzxps3bp15qxnrLCOrZ4x2HM95BkvPXXwjFfWdWh8fLy5TA/PmsRzvDxZz/rF2m4956Cpqcmc9bSD8vJyc9ZzjWGdXzzrDM+ayHNNOBDU1taas55jbm1LsbqubW5uNmc96zpP+/AcL+tc7Fl/eXjWi541RizWa4WFheYyPffaPOtQzzWGZ/zzzFnWueXzMLd5sp55yHpsPefA8/ye9m167h1aGgAAAAAAAAAAGHDYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKHYTAAAAAAAAAAAAKESrMH09HRzoampqeZsfHy8KZeSkrLDy5SkIAjM2dbWVnO2oaEhJnWIi7Pt/3ie3yM3N9ec9byu7u5uc7arq8uUi0Qi5jLT0tLMWes5kHx9oaioyJz1vLbOzk5Trry83FxmZWWlOTtkyBBz1tPPo9GoOdvc3GzKedpBVlaWObtp0yZzdiDwzBcjRowwZ9vb2005z/hnHU8k39zi6aNtbW3mrGdczc/PN+WSkpLMZRYWFpqzgwYNMmc9fa+xsdGcra6uNuU8Y3VdXZ056zm2Y8aMMWc9c2ZFRYU5a+07nvWQdQ6SpJqaGnM2VuusvLy8HV6mZ93gmVsGCk8/8awTrOOwp93V1taas561kqefrl692pz1zEXWcdDTnj1jlSfraTOeY2B9bU1NTeYyrWsXyTeuJCYmmrOeOdYztlrXL57j5ZnjPW3Rs97z1MF6jeE5BsnJyeas59wOBJ7rP0/Wusb39GfP2OM5557xzzO/eV6bdX3teV2etbinP3vOg4d1zvK0Q88aJzMzMyZZzz0h6/gnSRkZGabc5+G+iWd+zcnJ2eHP7xnXPfcREhLMt/9NeGcCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIxWYCAAAAAAAAAAAIlWANJicn2wtNMBeruDjbfkZ3d7e5TI9IJGLOxsfHx6QOiYmJ5mxGRoYpZz2u3mxLS4s5u27dOnPWc2w7OjpMufb2dnOZQRCYs83Nzeasta6SNHToUHM2KSnJnK2pqTHlPH3BMx6kpKSYs5520NXVZc6mp6ebctFo1FxmY2OjOetpXwNBVlaWOZufn2/O1tbWmnJNTU3mMj1zi6ePFBYWmrPZ2dnmbHV1tTlrPQ9paWnmMvfbbz9z1jOmNTQ0mLPvvvuuOVtXV2fKecb11tZWc9ZzDEaMGGHOetqiZy7897//bcpZ5xXJ12Y947pHbm6uObvPPvuYckVFReYyPfOgZ90wUAwaNMic9czT1r7qaaOVlZXmrHX8kXxzkee6ITMzc4dnPWOKZ37p7Ow0Zz086zXrsfWMVZ75zVNXz1rcug6WfGvWWIzZnr7gGVs9x8vTx6z9oa2tzVym53rbMyYOBJ61kme9Zl3XePqz5/k946pnDbhp0yZz1tNHUlNTTTnP9ZDnnpRnbvHcN/GMf9b1WqzG1PXr15uzsVq7eOYsa1usr683l+lZN1jbrORrM5751cpzP91zvKz3Uqx4ZwIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAjFZgIAAAAAAAAAAAiVYA0mJiaaC41EIuZsEASmXEtLyw4vU5Li4+PN2czMTHO2sbHRnG1tbTVnGxoaTLlYnAPJ1w482trazFlrfbu7u81ltre3m7MennNbVVVlzkajUXN206ZNppznGBQUFJizaWlp5qzneGVkZJiz48ePN+U844HneHV0dJizA4GnfXjGVev4Fxdn3yfv7Ow0Z7OysszZnJwcc9bT7j1je3p6uinnafeeOSAWc5vkmy+sdaivrzeX6dHc3GzO1tTUmLMpKSnmrKc/WNttUlKSuUxPm/H0Mc+x9RyD7OxsU87Tb7q6uszZiooKc3agyM3NNWc959La/z1jVVNTkznrWSd42pNnftlrr73MWWvbX7t2rbnM2tpac9YzD3j6f2pqqjlr7aueNbunfdXV1Zmz1jle8vWbWMyHnrWWp83k5+ebs55507M2tfZzzzHwrPU8r2sgsI5Tkq/vWa/VPPekysrKzFlPH/XMF55j4CnXOgZ77sckJJhvTbqOlyfrqa8167kv6KlrZWWlOeuZBz3zq+ceh7V9ee4deZ7f07486/ZYjO2xaIexwDsTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAKDYTAAAAAAAAAABAqARrsKury1xod3e3OdvW1rbDy2xtbTVnPa/LWlevpKQkczYxMdGU6+zsNJeZlpZmznqOV01NjTnrObbWttDe3m4u09NmGhoazNnKykpz1tPGMzIydng2Ly/PXGZOTo45m5ycbM7Gxdn3N1NSUsxZq4QE85Co4cOHm7OpqanbU50vLM+YNm7cOHO2sbHRlPOMPenp6eZsfn6+OZuVlWXOeuqbnZ1tzsbHx5ty1uMqSa+++qo56xnXW1paYlKudc70tAPPPOiZW1atWmXOeuZtz3htbV/Dhg0zlxmJRMzZ6upqc9Yzv3Z0dJiz1vPr6Tee9Yhn/TZQWMcqydf2rf3PM2d51gme/u+ZM4qLi83ZkpISc9Z6HKqqqsxlevppc3OzOes5tp7xynoePK/L02Y8bdHTbzzzpidrXb94xjXP2OoZDzzXWZ5rFyvPXOi5bojF9dDnWX19vTkbi2trz32ANWvWmLOeug4aNMic9fSRaDRqzlrH6/LycnOZnn6Xm5trznr6nmcesta3qanJXKbnXsjatWvN2ViMaZJvvLbyjH+eedB6TShJZWVl5qyn31jHGU87qK2tNWc9fcGCdyYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQbCYAAAAAAAAAAIBQCdZgS0uLudCurq4dno2PjzeX6alrR0eHORuL1yVJnZ2d5mxra+sOzUm+49XW1mbOJiUlmbOeY9De3m7KeerqOV9lZWUxySYmJpqzKSkp5mxhYaEpl5ycbC7T029qa2vN2Vj1XWu5nnZYU1NjzkajUXN22rRp5uznVRAE5mxqaqo5m5WVZcoNGTLEXGZaWpo5m5OTY856joGnLTU2Npqz1vZcVVVlLtPTlj1jmnVc92atY7tnPKmoqDBnu7u7zVnP+JOenm7Oeo5XUVGRKbfbbruZy9x9993N2bffftuc/fe//23Oeo5BXJzt72w8axxPm2loaDBnB4p169aZs562Z+0n1nYv+dpSU1OTOevp/57s+vXrzdn6+npTrrS01FymZ11n7XuSlJmZac56xnfrOcvPzzeX6VlneNqMZ33t6WNr1qzZ4dmEBPMtB/N1i5fn2slTX+u6zNMXYtVmBgLPPQ4P63rRc3/BM6Z5xilPtqCgwJz19BHrefCMU8OHDzdn8/LyzFnPdZann1pfm+c+k6c/e+7deK7fIpGIOeupr7XveJ7fsybz9F3POsvTz619zDMHxWrssOCdCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIBSbCQAAAAAAAAAAIFSCNdja2mouNC7OvkeRkpJiyiUkmKuq+Ph4c7ahocGcbW9vN2czMzPNWc/xsqqpqTFn29radvjzS77j1dXVZc52dHSYco2NjeYyPdmqqqqYlFtSUmLODh482JyNRqOmXCQSMZdZV1dnzjY1NZmz9fX15qz1dUlSdna2KZeammouMzk52ZxNS0szZwcCz3l8++23zVnrMS8sLDSXaR1PJKmzs9Oc9fDMb55239LSYsp52v2YMWPM2cTERHO2oqLCnPXMb9ZjkJSUZC7T02Y8Y6VnTIlVtru725TzjOueNY5nbhs2bJg561nrefqYlafN1NbW7vDn/7zbuHGjOeuZT61jq2ds96yVgiAwZz3XWZ6x1bNmtbZTzxzvuRawXhNK0ogRI8zZWKyZc3JyzGV6rks97WDt2rXmbHl5uTmbkZFhzo4aNcqU84yrnvnY03c985ZnXWbt5+np6eYyPfcRPOdrIPCcG89Yae2nnj7qmQM86wTPus7TnzxjsLVczz0ez/WIp9xYXWNY6+CZB63XLVLszq1nnWO9bpDsfdczZ8bqPqanj8Wivs3NzeYyPVnP+bLgnQkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACAUmwkAAAAAAAAAACBUgjXY0NBgLjQ7O9ucjUajplxSUpK5zPT0dHM2JSXFnK2vrzdnPZKTk83Zzs5OU66lpcVcZldXlzkbBEFMyvW0Gesx8BzX7u5uczYrK8uc9bQvzzGIRCLmbEdHhynX1tZmLrO8vNycra2tjUk2MTHRnLW2W884U1NTs8Off1e0bt06czYvL8+U84w91vFE8vWRhATz9Ooafzz1jY+PN+Xi4mLzdwXW+V3y9eecnBxz1tpmrMdK8r0uT9Y6Vnu1traas01NTaZcaWmpuUxPX/AcL0+5nrHdWoeKigpzmZ65zTN+DRSeNY1n7rWu1xobG81l1tXVmbOevucZ2z1jRVpamjlr7VNr1qwxl+mZN1NTU83Z9vZ2c9Yzx2ZkZJhynnnTc249c6Hn3HquXTznzDpeFhQUmMv0HC/PeOkZZzxtxnq8PO3bI1ZruM8rzzWVZ6y09j1P2/Dcj8nMzDRn8/PzzVnP/RBPH7GOKZ71tWfOfPPNN83Z6upqc9Yzt1jboqfNeHj6Qqzq4Gnj1vtinjV7c3OzOeu5brCuBSTfefCsN61idf1osWvNPgAAAAAAAAAAwI3NBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAEIrNBAAAAAAAAAAAECoSBEGwsysBAAAAAAAAAAA+v3hnAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAgAAAAAAAAAACMVmAvA5VVpaqkgkogceeGBnVwUA8DnHnAEAsGC+AABYMF9ga9hMGGAeeOABRSKR3n8JCQkqKirSzJkztX79+p1dvR1u7ty5O31g+zzU4Y477tCJJ56oQYMGKRKJ6Oabb96p9QHwxcCcsevV4d1339V1112nffbZRxkZGRoyZIiOP/54vfbaazutTgA+/5gvdr06bNiwQV/72tc0btw4ZWRkKDs7WwcccIAefPBBBUGw0+oF4PON+WLXrMPmHnroIUUiEaWnp+/sqiBGEnZ2BRAbt956q0aOHKnW1lb9z//8jx544AG98MILevPNN5WcnLyzq7fDzJ07V/n5+Zo5c+YuXYfvfve7Gjx4sPbdd189+eSTO60eAL6YmDN2nTr893//t+bPn6/p06frsssuU11dnebNm6cDDzxQf/3rX3XUUUftlHoB+GJgvth16lBZWal169bptNNO04gRI9TR0aHly5dr5syZeu+99/T9739/p9QLwBcD88WuVYcejY2Nuu6665SWlrazq4IYYjNhgPrqV7+q/fffX5L0X//1X8rPz9ddd92lZcuW6YwzztjJtds5mpqaBuyAtmbNGpWUlKiyslIFBQU7uzoAvmCYM/obqHPG2WefrZtvvrnPXwpdeOGFGj9+vG6++WY2EwCEYr7ob6DOF3vttZf+8Y9/9Hnsiiuu0AknnKCf/vSnuu222xQfH79zKgfgc4/5or+BOl9s7vbbb1dGRoamTp2qpUuX7uzqIEb4mKNdxOTJkyVJq1at6vP4u+++q9NOO025ublKTk7W/vvvr2XLlvX7/draWn3jG99QSUmJotGohg0bpvPPP1+VlZW9mfLycs2aNUuDBg1ScnKy9t57bz344IN9yun5zLUf/vCH+tWvfqXRo0crGo3qS1/6kl599dU+2bKyMl1wwQUaNmyYotGohgwZopNOOkmlpaWSpJKSEr311lt69tlne99Cd/jhh0v6v7fWPfvss7rssstUWFioYcOGSZJmzpypkpKSfq/x5ptvViQS6ff4woULdcABByg1NVU5OTk67LDD9NRTT22zDj3H7aqrrtLw4cMVjUY1ZswY3XXXXeru7u53fGfOnKmsrCxlZ2drxowZqq2t7VeXrdnS6wGA7cWcMXDnjEmTJvV7y3FeXp4mT56sd955x1QGAPRgvhi488XWlJSUqLm5We3t7Z+pHAC7FuaLgT9ffPDBB7r33nv14x//WAkJ/O36QMbZ3UX0DHY5OTm9j7311ls65JBDVFRUpOuvv15paWlavHixTj75ZD322GM65ZRTJH3yNqWemwwXXnih9ttvP1VWVmrZsmVat26d8vPz1dLSosMPP1wffvihrrjiCo0cOVKPPPKIZs6cqdraWn3961/vU5+HH35YDQ0NuuSSSxSJRHT33Xfr1FNP1erVq5WYmChJmj59ut566y3Nnj1bJSUlKi8v1/Lly/Xxxx+rpKREc+bM0ezZs5Wenq7vfOc7kqRBgwb1eZ7LLrtMBQUF+t73vqempib3cbvlllt088036+CDD9att96qpKQkvfLKK/r73/+uY445JrQOzc3NmjJlitavX69LLrlEI0aM0EsvvaQbbrhBGzdu1Jw5cyRJQRDopJNO0gsvvKBLL71U48eP15IlSzRjxgx3fQFgR2DO2PXmjLKyMuXn53+mMgDsepgvBv580dLSoqamJjU2NurZZ5/VggULdNBBByklJcX9ugHsupgvBv58cdVVV2nq1Kk67rjjtHjxYvdrxRdIgAFlwYIFgaTgb3/7W1BRURGsXbs2ePTRR4OCgoIgGo0Ga9eu7c0eeeSRwcSJE4PW1tbex7q7u4ODDz44GDt2bO9j3/ve9wJJweOPP97v+bq7u4MgCII5c+YEkoKFCxf2/qy9vT046KCDgvT09KC+vj4IgiBYs2ZNICnIy8sLqqure7NPPPFEICn4wx/+EARBENTU1ASSgnvuuSf09e6xxx7BlClTtnocDj300KCzs7PPz2bMmBEUFxf3+52bbrop2LxLfPDBB0FcXFxwyimnBF1dXVt83WF1uO2224K0tLTg/fff7/P49ddfH8THxwcff/xxEARBsHTp0kBScPfdd/dmOjs7g8mTJweSggULFmzt5fdTUVERSApuuukm8+8A2HUxZ/Q9DrvanNHjueeeCyKRSHDjjTe6fxfAroH5ou9x2JXmizvvvDOQ1PvvyCOP7H0OAPg05ou+x2FXmS/++Mc/BgkJCcFbb73V+xrT0tK2+Xv4YuJjjgaoo446SgUFBRo+fLhOO+00paWladmyZb1vq6qurtbf//53nXHGGWpoaFBlZaUqKytVVVWlY489Vh988IHWr18vSXrssce099579+4Kb67nLVh//vOfNXjwYJ199tm9P0tMTNSVV17Z+1csmzvzzDP77Ej3vOVt9erVkqSUlBQlJSXpH//4h2pqarb7OFx00UXb/VmeS5cuVXd3t773ve8pLq5vV9nSW88+7ZFHHtHkyZOVk5PTe3wrKyt11FFHqaurS88995ykT45dQkKC/t//+3+9vxsfH6/Zs2dvV70BwIs54xO74pxRXl6uc845RyNHjtR11123XWUA2HUwX3xiV5ovzj77bC1fvlwPP/ywzjnnHEmfvFsBAMIwX3xiV5gv2tvb9Y1vfEOXXnqpJkyY4HiF+KLiY44GqF/84hfabbfdVFdXp/vvv1/PPfecotFo788//PBDBUGgG2+8UTfeeOMWyygvL1dRUZFWrVql6dOnhz7fRx99pLFjx/Yb4MaPH9/7882NGDGiz//3DOI9g3Q0GtVdd92lq6++WoMGDdKBBx6oadOm6fzzz9fgwYMNR+ATI0eONGc/bdWqVYqLi9vuwfCDDz7QypUrt/qFyOXl5ZI+OTZDhgzp9xnW48aN267nBQAv5oxP7GpzRlNTk6ZNm6aGhga98MIL/coEgE9jvvjErjRfFBcXq7i4WNInGwsXX3yxjjrqKL333nt81BGArWK++MSuMF/ce++9qqys1C233LJd9cQXD5sJA9QBBxyg/fffX5J08skn69BDD9U555yj9957T+np6b1ftnLNNdfo2GOP3WIZY8aMiVn9trYzGwRB739fddVVOuGEE7R06VI9+eSTuvHGG3XnnXfq73//u/bdd1/T82xpgbu1Hdyuri5TmVbd3d06+uijt/qXnrvtttsOfT4A2F7MGZ/YleaM9vZ2nXrqqVq5cqWefPJJ7bnnnju0fAADE/PFJ3al+eLTTjvtNP3617/Wc889t9VzDADMF58Y6PNFXV2dbr/9dl122WWqr69XfX29pE++5yIIApWWlio1NVWFhYWf+bnw+cFmwi4gPj5ed955p6ZOnaqf//znuv766zVq1ChJn7zt66ijjgr9/dGjR+vNN98MzRQXF2vlypXq7u7usxP87rvv9v58e4wePVpXX321rr76an3wwQfaZ5999KMf/UgLFy6UZHtr16fl5ORs8VvpP71TPXr0aHV3d+vtt9/WPvvss9XytlaH0aNHq7GxcZvHt7i4WE8//bQaGxv77AS/9957ob8HALHAnNHXQJwzuru7df755+vpp5/W4sWLNWXKFPPvAkAP5ou+BuJ8sSU9H3FUV1f3mcoBsOtgvuhrIM0XNTU1amxs1N1336277767389Hjhypk046SUuXLt1mWfji4DsTdhGHH364DjjgAM2ZM0etra0qLCzU4Ycfrnnz5mnjxo398hUVFb3/PX36dL3++utasmRJv1zPru1xxx2nsrIyLVq0qPdnnZ2d+tnPfqb09HT3jYrm5ma1trb2eWz06NHKyMhQW1tb72NpaWlbHITDjB49WnV1dVq5cmXvYxs3buz3+k4++WTFxcXp1ltv7d0177H5bvXW6nDGGWfo5Zdf1pNPPtnvZ7W1ters7JT0ybHr7OzUfffd1/vzrq4u/exnP3O9LgDYUZgz+pYz0OaM2bNna9GiRZo7d65OPfVU8+8BwKcxX/QtZyDNF5ufq83Nnz9fkUhE++23n6kcAJCYLz5dzkCZLwoLC7VkyZJ+/6ZOnark5GQtWbJEN9xwwzbLwRcL70zYhVx77bU6/fTT9cADD+jSSy/VL37xCx166KGaOHGiLrroIo0aNUqbNm3Syy+/rHXr1un111/v/b1HH31Up59+ui688EJNmjRJ1dXVWrZsmX75y19q77331sUXX6x58+Zp5syZWrFihUpKSvToo4/qxRdf1Jw5c5SRkeGq6/vvv68jjzxSZ5xxhiZMmKCEhAQtWbJEmzZt0llnndWbmzRpku677z7dfvvtGjNmjAoLC3XEEUeEln3WWWfpW9/6lk455RRdeeWVam5u1n333afddttN//znP3tzY8aM0Xe+8x3ddtttmjx5sk499VRFo1G9+uqrGjp0qO68887QOlx77bVatmyZpk2bppkzZ2rSpElqamrSG2+8oUcffVSlpaXKz8/XCSecoEMOOUTXX3+9SktLNWHCBD3++OOuv/b57W9/q48++kjNzc2SpOeee0633367JOm8887b7l14ALsu5oxPDLQ5Y86cOZo7d64OOuggpaam9v5VVY9TTjlFaWlp1kMPAMwX/7+BNl/ccccdevHFF/WVr3xFI0aMUHV1tR577DG9+uqrmj17dkw/fgTAwMR88YmBNF+kpqbq5JNP7vf40qVL9b//+79b/BkGgAADyoIFCwJJwauvvtrvZ11dXcHo0aOD0aNHB52dnUEQBMGqVauC888/Pxg8eHCQmJgYFBUVBdOmTQseffTRPr9bVVUVXHHFFUFRUVGQlJQUDBs2LJgxY0ZQWVnZm9m0aVNwwQUXBPn5+UFSUlIwceLEYMGCBX3KWbNmTSApuOeee/rVT1Jw0003BUEQBJWVlcHll18e7L777kFaWlqQlZUVfPnLXw4WL17c53fKysqC448/PsjIyAgkBVOmTNnmcQiCIHjqqaeCPffcM0hKSgrGjRsXLFy4MLjpppuCLXWJ+++/P9h3332DaDQa5OTkBFOmTAmWL1++zToEQRA0NDQEN9xwQzBmzJggKSkpyM/PDw4++ODghz/8YdDe3t7n+J533nlBZmZmkJWVFZx33nnBv/71r0BSv2O4JVOmTAkkbfHfM888s83fB7BrYs6Yss3jEAQDa86YMWPGVucLScGaNWtCfx/Aron5Yso2j0MQDKz54qmnngqmTZsWDB06NEhMTAwyMjKCQw45JFiwYEHQ3d0d+rsAdl3MF1O2eRyCYGDNF1syY8aMIC0tzf17+GKIBMFm740BAAAAAAAAAAD4FL4zAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhGIzAQAAAAAAAAAAhEqwBh977DFzoQUFBeZsXBz7GQAGtvj4eHP2oIMOimFN/jPWrFljziYkmKchSOrq6trhZSYnJ5uzdXV15mxjY6M5297ebs561g319fWmXFtbm7nM1tZWc9bzuqqrq83Zjo4Oc9bTZqyvzVNmWlqaOZubmxuTcnfffXdzNisry5TzjF2ebCQSMWcHDRpkzn6elZeXm7Oe+dRzLK26u7vNWU//9/CU6xkrEhMTTbkgCMxldnZ2mrOeccVzHjxtxtpXPc/vyXrmt+bmZnPW0w48c1xLS4sp56mr5xh45gFPuZ5zFotxxlNmSUlJTLKfV4888og5m5SUZM5axwnPeOIZKz1iNVZ6WF/bzp6zAfwfzzx43HHHbbu8z1IZAAAAAAAAAAAw8LGZAAAAAAAAAAAAQrGZAAAAAAAAAAAAQrGZAAAAAAAAAPx/7dtJj1z3eTb8U11DV8/dnESKtkQNoWQ7cRDDjgEvgqySbIIg3y6fI18gyCoIkJ3twHFsSzEpiRanJtlTdU3v4l08m+f557qNPiZN/X7rC3edOuc/nbq7AWjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmkZpcG9vLy86ist2g8EgzgK8KZbLZZzd2Phm9W37+r72i67b2dmJs7PZLMq9fPkyrnl5eRln5/N5L9nFYhFnz87OotzFxUVcs3K/Xr161Uu2sv5Unll6H65fvx7XHI/HcXZ7ezvOHh0d9ZI9Pz+PcpVnUBnflTP026KyZ7zufaDyfCrrSrped13XrdfrODuZTOJsOvYrz6Cy/qSf33W1MTMcDuPs5uZmlKs8r9PT0zhbWSsqdSsqzyzdYyvrZeV5VeZYJVs5a02n0yi3tbUV19zd3Y2zh4eHcfZtUFmD+3hXq9SsODg4iLOVOVJZV1erVZytrIGpyveq7IPwNqvM26v2zfqFCwAAAAAAKNNMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJpGaXA4HOZFR3FZgD9KGxt6sf8vg8GglyxdN5/P4+xisYhylf19tVrF2b7qzmazOHt8fBzlzs/P45onJydx9smTJ3H266+/jrMXFxdx9tWrV3H28vIyyn355ZdxzW9/+9tx9tNPP42zd+7c6SV7eHgY5SpjZr1ex9nKvOEPr/IsK/raN5fL5ZXXrdSsrNenp6dxtqLyXvr06dMoV9mLK/drc3MzzlZU1pXKXpQ+sy+++CKumZ5duq7rDg4O4mxlfJ2dncXZra2tKHd0dBTX3N/fj7OVM8lf//Vfx9k31Xg87qXu3t5elKvMpel0GmcrdV+8eBFnK++r6RmwL5W10lkJ/n+v8zcpv4YBAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABA0+h1XwAAkFmtVnH28vIyyi0Wi7jmxcVFnF2v13H2/Pw8zm5s5H8HsbOzE+XSe9V1XTefz+NspW7l3h4fH8fZ58+fX3l2b28vrjka5UfNg4ODOHvz5s04u729HWfTsTgYDOKak8kkzlbGN394lXWtkq0899ls1ss1VLKpyjzZ3d2Ns5X18sWLF3E2VVlTKntsZa04PT2Ns19//XWcrexx6V40HA7jmtPp9Mo/v+u67uTkJM5ev349zqZ73NbWVlyzshfu7+/H2bdBZT5V1rQnT55EuaOjo7jmcrmMs+PxOM5W1p/K+lc5r6X3tnIPKusE8Pp5YwEAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaRmlwvV7HRS8uLuLsdDqNck+fPo1rLpfLOHt4eBhnK27evBlnX758GWfT57BYLOKaq9Uqzk4mkzgLwOszGAyi3Hg8jmtubW39vpfT1Neele6vlZrpfe26rru8vIyzjx8/jrO/+tWv4uxXX30VZ2ezWZRLz25d13UbG/nfrXz++edxtnLWvH//fpzd3d2NcpUxU8n2NcfeFpX3kXSuVp5PRWWtqGRHo/j1reTs7CzKVebe+fl5nK286x0dHcXZ27dvx9l0zd7c3Ixrnp6extl0De662nMYDodxtjK+0vWq8mwfPXoUZ7/44os4++zZszj74MGDOJvO3Xv37sU179y5E2cr9+AHP/hBnH1TzefzOFvZL9I5Xfn8yllpf38/zlbmc+W3rsqZ9eTkJMpV9oDKmga8fv4zAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJpGafD8/DwuOp/P4+xvfvObKPfy5cu4ZsXJyUmc3dnZibP37t2Ls8PhMM4eHh5Guf39/bhm5R4A8MdhNMq2+OVyGdfc2Mj/BiH9/K7ruslkEmefPn0aZyvnkdTl5WWcrVzr8+fP42zlex0cHMTZO3fuRLnKmbDybNPP77que+edd+LsdDqNs9euXYtyr169imtW5k0l+7ZYr9e91E3nyWq1imtWzuyVtbVyD2azWZytrBWLxSLKDQaDuObW1lac/fM///M4e3x8HGcra3Y6FirvTpUxU8lWxm1lHT47O4uz6XN49uxZXLOyF1buQcXHH38cZ9PfByp7VuXzf/jDH8bZt0Flj6yM+3S9rqypp6encbYyRyp7wK1bt+JsZf1J94vt7e24ZuVsWdmHgH58895YAAAAAACAEs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoGmUBre3t+OiT58+jbOLxSLKnZ+fxzUr1zqdTuPsBx98EGf//u//Ps4ul8s4OxwOo9yvfvWruObW1lacff78eZwF4O0ymUzibGVvm81mv8/l/K8Gg0GUq5wbKtl33nknzv7mN7+Js7u7u3H24cOHcfazzz6Lcvfv349r7u3txdnKmJnP53G2ci7d2Mj+zma1WsU1b968GWe/idJ52nW1+/46a3Zd7Xul70NV6XtDJXt5eRnXrMzpyjtG5XtV1uF0fT89PY1rPnv2LM5W9sJK3YrKd0ufWWUfevToUZz96quv4uzZ2VmcrdjZ2YlyJycncc0XL17E2cr+8nd/93dx9k1V2fsr62r6W9NoFP981l27di3OpmePruu6Tz75JM5+97vf7eUa+pj7lb2lMp+AfvjPBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGgapcHJZBIXPT8/j7OPHj2Kci9evIhrbmzkPZL//u//jrNff/11nB2N4lvbPXv2LM7evXs3yr3//vtxzcr3qtxbAN58lf2qkq2cBba2tuLsYrGIs8vlMsoNBoO4ZuU8VDm7zGazXrKHh4dx9lvf+laU29/fj2tWrFarODudTuNs5R7s7e1Fufl8Htc8PT2Ns7u7u3H2bVGZf5Uxkq4VlZqVbLr+VOv2Jb1fledVeW/Y3NyMs5X5v7OzE2cvLy+j3MnJyZXX7LquW6/XcbbyvSrrVWXvTq+3MmauX78eZytzrPLMKvcg3TPG43Fc886dO71k3waV+1iZe+n6Uzl/PX78OM6enZ3F2YuLizh7cHAQZ7/66qs4m87pyvn+1atXV/75QH/8MgwAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNoz6KjsfjOLu1tRXljo+P45rn5+dx9quvvoqz77zzTpxdLpdx9saNG3H27t27V16z8rwq9wuAq7Vara68ZmW/Gg6HcXa9XsfZ3d3dOFtx/fr1KFf5XpUzxt7eXpy9du1anN3c3IyzFQcHB1Huzp07cc30nNd1Xfcnf/Incfb999+Ps+n36rqu297ejnIXFxdxzcr4qmS/iQaDQZzd2Mj+ZqqyrlbWtUq2Iv1eXVe7X6nRqJfXx25nZyfOVubJ5eVlnJ3NZlGucg8mk0mcXSwWcbavsVjZ487OzqJcZY9Pa3Zd7X338PAwzk6n0zib7vOVa71//36crdR9G1TOP5V1Ih13lflRmc8nJydxtjKfKr+hVaRzpLL+VuZdulYD/fGfCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANA0SoPb29tx0cvLyzi7s7MT5SaTSVxzOp3G2R//+Mdx9oMPPoizP/rRj+Ls7373uzg7HA6j3MZG3ieaz+dxFoDXZzAYXHm2sr+me1DXdd3+/n6cff78eZyt7PGjUXbMqXx+5R5cv349zlbuV+UePHv2LM4+fPgwyu3t7cU1Dw8P42zlPHRwcBBnX758GWdfvXoV5XZ3d+OaFZU5/raonFnX63WcTe9lX/e8slZU9HW9ad3KM6g824uLizibru1dV7ve1Hg8vvKaXVd7tsvlMs6enZ3F2fTdvOu67r333ouzqcrzOj09jbOVMXN8fBxnNzc3o9xisYhrfvnll3G2sm++DSrramXcp8+n8ptY5TeWyhnw008/jbNHR0dxtvIbXpqtPIPZbBZngdfPfyYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATaM0+OTJk7jo/v5+nJ3NZlHuxo0bcc1nz57F2ZOTkzj7r//6r3H2F7/4RZy9detWnH3vvfei3Pe///24ZuUe7OzsxFkArtZgMLjy7Hq9jmuuVqs4u1wu4+xoFB9HusvLyzg7nU6j3NHRUVxzPB7H2bOzszj7wx/+MM6enp7G2Zs3b8bZ9H5dXFzENSvnhr29vTj7/vvvx9nbt2/H2Y2N7O9sKmcn2iprRfp8KnWHw2Fcs7IGVlTqVq63sr6ndSeTSVyz8mz7UrmGxWJx5TUr2coev7W11cs1VNa29J17c3Mzrrm7uxtnKypnh8qZIP2N4tvf/nZc86OPPoqzP/nJT+Ls2+D8/DzOVvaLNPvq1au4ZmX9rcy7f//3f4+zlblX2Ye2t7ejXGU+V55XZR8C+uE/EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBplAbX63VcdHt7O85+/PHHUe6zzz6La96+fTvOvvPOO3H23r17cXZ/f7+X7NHRUZT76quvevn85XIZZwF4fYbD4ZXX3Njo528QKnUnk0mcHY/HUa6yt6U1u67rPvzwwzj705/+NM5WzjlPnjyJs48fP45yN2/ejGteXl7G2c3NzTj7u9/9Ls7u7u7G2XQsVMbhaBQft/lfVNaKNLtYLOKalfm/Wq3i7NbWVpytrFeVa0jvw6tXr+KafexDXVe7B5VsOmYqc3o+n8fZvlxcXMTZyhy7ceNGlPvyyy/jmpUxW5mPlX2gIr0HlblQWZN+/etfx9nvfOc7cfZNVRmflXs+nU5/n8tpqpxpKutUZSxXfpfr43ee2WwWZyvrauW3SaAf/jMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmkZpcDAYxEXn83mcnUwmUe6DDz6Ia25vb8fZv/zLv4yzv/3tb+Pso0eP4uzZ2Vmc3drainLT6TSuuV6v4yz9GY3i6VgaM+nzXSwWcc3lchlnb9++HWdns1mcHQ6HcXZzczPKPX36NK5ZWefG43Gc5c1VWSsre2ZlPvV1DanKtfZVtzL3Uhsb+d9WVNaeypr2/vvvx9nKHv+d73wnzqbXm57dqtnK+L5161acTc9OXdd15+fnUa6yZ3J1KmMkndeV81dlXa2sKxV9XUN6VqrUPDk5ibN97ZuVuZqugZVxWLkHq9UqzlbuQWXPqNyvdG09OjqKa1bG1+XlZZyt3NudnZ04u7+/H+U++eSTuOa3v/3tOPu9730vzn7TVOZIuv7s7e3FNSvvf5U5WllTnjx5Emcr15veh0pNv0nBHxf/mQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNozQ4HA7jont7e3F2Pp9HufF4HNecTqdx9smTJ3F2uVzG2YODgzh7586dOLtYLKLc8fFxXHNnZyfO0p90LnRd152ensbZp0+fXnnNypit1P3iiy/i7Icffhhn07lbWbu2t7fj7PPnz+Msf1ir1SrOrtfrOJuu1X1eQzruK3tbxWgUHzG6y8vLOFu53sFgEOU2NvK/rUhrdl3X3bhxI84+evQozlbOOX2MrxcvXsQ1K3Nhf38/zn7++edxtrJfpPegMg4q44u2yvtI+iz7ej6VMdJXdjKZxNmzs7Mod3FxEdesXOtsNouzlX2gj/24smf1tRdWnm1l3pycnMTZ9DlUnm1F5feByl7Yxxyr/OZQecd4+PBhnL1582acfVP1tf71sV9U5l1lXa28C1TmSGXcpddQmfuVawVeP283AAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0DRKgxsbed+hj+zm5mZc8/Hjx3F2MBjE2b29vTi7Wq3i7HA4jLPr9TrKbW9vxzUvLy/j7GQyibPUVMZBZYy/fPkyyj19+jSueXx8HGd/8YtfxNkbN27E2ffeey/OLhaLKHfr1q245qtXr+Ls7u5unOUPq7JfzefzXq6hsl/0sb/2ta5XrnU0io8j3dnZ2ZXXrTyDynwej8dx9vbt23G2ssdXpGP82rVrcc3K+W25XMbZ9DzUdfkeUNHXOa9yLn1bvO7vXPn8yrirjJG+7sHFxcWVX0PlWivZyj5QOYNV9oyvvvoqylXWlNlsFmcr96uyx1YcHR3F2Z2dnSj36aefxjUrc6wyZirPrJJN71dl3/7444+v/PPfFn3NkXS9roy58/PzOFv5XpWzZV/vGOk8rdyvyhmwcq4C+uE/EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBp9LovYDAYRLmNjbzvcfv27Ti7u7vbS/bRo0dx9vT0NM6m92G9Xsc1x+NxnKU/k8kkzp6fn8fZhw8fXnnN2WwWZ589exZn/+Iv/iLO/vM//3OcXa1WUe7u3btxzfv378fZr7/+Os7euXMnzvKHlY6jruu6xWIRZ5fLZS/XkO4Dlfmc7tnVbOUaKueB9H5V9sHhcBhnR6P8mFV5tpVzw+HhYZydTqdRrvK8KtdaObvM5/M4u7e3F2fTZ3Z5eRnXrKwHXJ3KGpSqrD+V8dzHtVbrpntRuk50XdddXFzE2cqcrqjcg8p6mepjz+q62rpS+V6VM0m6H1beBV6+fBlnz87O4mwf79td18/c/e1vfxtnK+Ogshfyf1d53tvb23G2cg6tnC0r7/eVNfh176/A62fGAgAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2jNLher+OiGxt5j2IymUS5+Xwe16xkLy4u4uyTJ0/ibMVgMIiz6XOoPIO31WKxiLNnZ2dxdnNzM84+evQozqZzoSq93i+++CKuWZk3//mf/xlnj4+P4+xPfvKTOPvgwYMo91d/9Vdxze3t7Tg7HA7jLG+uylpdyVYsl8te6r5uo1F8HOmlbuWMU5nPlXFQ2QO2trZ6uYZ03zw6Orryml1Xew6VbOUepOenyjgYj8dx9pu4X1SeZUX63Cvral/vQ32M0ao+9pfKtVbO17u7u3G2sl5Op9Mo19eYqWTTa+262r1drVZxNn1/unPnTlzz1q1bcfbg4CDO9nV+St8HLi8v45qV73X9+vU4+01TWVfTvbcyPyrehPN9H+8ufb0PAa+fX5wBAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgafS6LwD6slwu4+zjx4/j7PHxcZxdLBZxdnNzM87+4he/iLMPHz6McicnJ3HNV69exdmKDz74IM5ub2/H2W9961tR7s/+7M/imoPBIM5OJpM4yx/WcDiMs+v1Os5ubOS9+spYqlxvqnKtb4LVahVnR6OrP+ZU1vVKtvIcLi8ve7mGdIzPZrMrr1nNVvbMynmgMr76+Pw/tvl4FSrPvWI+n0e5Pp55tW7luY/H4zhbubfp/aqo3IPK/lbZN/vau1OV9Xp3dzfOXlxcxNnT09NerqEPlbX9xYsXcbayDlfGV/peWHlvmU6ncbbybCvXAAAt37w3FgAAAAAAoEQzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGgave4LgL4MBoM4O5vN4ux8Po+zjx49irOjUT4dLy8v4+zGRtYzvHfvXlzz4uIizv7yl7+Ms//xH/8RZ//pn/4pzn788cdRbjKZxDXfeeedOPunf/qncZY/rMViEWfTudR1XbdcLuPscDiMs6vVKs6mKvdgvV7H2cq19vG9ui5/ZpVnUFFZU/oai5Vnltat7FeVuVDZt7e3t+NsZd9O70HledFWee59rBWVmpW1ojJPx+NxnN3Z2YmzlfmXzutKzb5U7kFlfG1ubka5yj2ojIPKtVZU9oHpdBpn0zGzu7sb1+zr/e3k5CTOVp5vus9X9qHK2aHybAHgqvjPBAAAAAAAoEkzAQAAAAAAaNJMAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGgave4LgL4cHh7G2UePHsXZ3/zmN3H25OQkzl5eXsbZly9fxtl79+5FufF4HNc8Pj6Os0+ePImz7733Xpz9x3/8xzj7ySefRLmf/OQncc31eh1np9NpnOXNtbGR99+Hw2Gcrcz9yWQSZ9MxurW1deU1u67rBoNBnK3cg0rdzc3NK695cXERZ8/OzuJsReV6K9nRKDsWVmpW5kL6vLquNhYrFotFL3X5f6uMp9VqFWfTZ1mpWRl3lXVtuVzG2YrKd0tVnldlPlX22L6uYT6fR7nKXnx+fh5nK/txXyr3K70Pfe1Z6fPqutr3qmTTd73KmKmcM3Z2duIsAFwV/5kAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATaPXfQHQl42NvFd27969OHt4eBhn/+3f/i3OXl5extkHDx7E2c8++yzK3bhxI675s5/9LM7++Mc/jrMfffRRnL1+/XqcvXv3bpQ7OjqKa67X6zg7mUziLN88lfExGAyu/PMrY7mvbGW9rtyv6XQa5YbDYVxzNMqPTpXndX5+Hmcr92u5XMbZ1Wp1pbmuq92vyveqjK8+5g2vR2WMpCrjuTKfZrNZnF0sFnF2Pp/H2cra1sfnV/R1by8uLuJsumdUPr+yZ1XuQeXZ9pVNVeZYJdvXXKjsW+mYqaxd4/E4zlbu19vg5OQkzu7s7MTZ/f39KPfixYu4ZuU3g8r4OD4+jrN9ndvTc1Vl3u3t7cXZs7OzOFtRWYNTfayp8CbwnwkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQNHrdFwB92drairMXFxdx9mc/+1mcPT4+jrPPnj2LsxWjUTbNP/3007jm97///Ti7Wq3i7HQ6jbPp9+q6rjs9PY1ys9ksrvn8+fM4u7+/H2f5wxoMBnF2Y+P1998r15saDodxtjKfl8tlnK3c20rd1Hg8jrOVdb1yv9brdZyt7FmVdTW9D5ubm3HNyrOtjO++7i1vj3RtWywWcc3JZBJn+9pfdnd342xl7FfOQKnKva2sw5V7e3h4GGfT+9XXvtnXGljJVr7byclJlDs/P49rVsZhZS+sjK/KPdje3o6zqcr3qlzr26Cvs2U6Rs/OzuKalfX35cuXcfbVq1e9XEPl3qZ19/b24prpetJ1tb24r+zl5WWchbfR6/9lBAAAAAAAeKNpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNo9d9AdCXX//613F2YyPvq41G+bS5vLyMs9PptJdrODg4iHL/8i//EtccDAZx9saNG3H2H/7hH+Ls3/7t38bZ69evR7nVahXXrIyZs7OzOMubqzLu1+v1a6+bqoz7isr3Go/HcbayVh4eHsbZ1NHRUZw9PT2Ns5XnULm3i8Uizs7n8yuvWdmvJpNJL3UrKms7V2M4HPZSN51TfZ3rKvP04uKil2uo3NvZbHblNSvSz++62jx9+fLl73M5TZubm3G2Mg4qdft6b6jc2+VyGWdTlfNA5Vor+8v29nacTffDyjM4Pz+Ps5Xx9TaorD8PHjyIszdv3oxyu7u7cc1K9vnz53G2Mu4r57XKO0a6Vu3v78c179+/H2fT82rX1ebIixcv4my6TlT2Nvhj4o0JAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaBq97guAvuzv78fZp0+fxtk7d+7E2UePHsXZxWIRZ8/OzuLsF198EeVWq1Vc89133+0lOx6P42zlel++fBnlKs/g/Pw8zk4mkzgLLcPhMM6mc2Rjo5+/KxiN8iNGJbu5uXnlddfrdVyzsk5sbW3F2eVyGWcra2Vl/UmvoTJmBoNBnK3UrTyHvsY4V6My/yp7/3w+j3KVsVSZp7PZLM5W5unl5WWc7UPlGVTuV1/zdHd3N85WxkKqr+9Vubenp6dxtnK96dzt6xzc17vTxcVFnE2/W+W+Vs5ElXHwNnjx4kWcffbsWZx99epVlPv1r38d10z3oK7ruul0Gmcr56rKebGPd4y+9oDKvd3Z2Ymze3t7cTb9LaDyvOCPibcrAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJo0EwAAAAAAgCbNBAAAAAAAoEkzAQAAAAAAaBq97guAvgyHwzh7fHwcZ09OTuLs5uZmnL28vIyzu7u7cfZHP/pRnE394Ac/iLOV5/A3f/M3cfbdd9+Ns5999lmcTU0mkyuvydtjMBj0Une1Wv1R1Oyz7nq9jrPpc6jUvLi4uPLP77quWywWcbZiPB5fec3KtVbuwcZG/jculSxvtul0GmcrZ6Xlchnl+hrPlbmXXmvX1c5VlXkym82iXGVtr9zbyjpcOYNV6va1b6Uq46Ci8r362OPm83lcczTKf56ojK/KmKnM3XSOVda5yvPq67z5pqq8A1fWyoODgyj3+PHjuObTp0/j7MOHD+NsZT5du3YtzvZxbyvrSWWOVtaJnZ2dOFuZp+ncPzs7i2vCHxNvYgAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANA0et0XAH0ZDAZxdmMj76sdHR3F2ZOTkzh7eHgYZ09PT6+87nK5jGv+/Oc/j7Pf+c534uxisYizs9kszgJvtsp6PR6P4+xqtbrSXNd13WQyibN9fa/KnlX5bmm28vmVbEXle/V1DVyNyjx5+fJlnE3PSpeXl3HNSnY0yl+ztra24uyrV6/ibOVsl65t8/k8rllRWQMrzs7O4mz6zCrrT+V+VcZB5X6t1+tesun96uvZDofDOFuZjxV97C+VNbEyx98GlXvz05/+NM6mY/Tzzz+/8ppdV3uOFxcXcbayVlXq3r9/P8q9//77cc1r167F2cq6XhkzlWzl3sLbyNsVAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANI1e9wVAX2azWZy9du1anF2v13H2vffei7PPnj2Ls8fHx3H28ePHUW65XMY1P/nkkzj74YcfxtnJZBJnP//8817qAn94q9Uqzp6cnFz5529s5H9bMZ/P42xlv6hcQ2W9rtQdDAZxto/P76tu5Tn0cQ+4OqPR1b+6XFxcxNnKuDs7O4uzlXNd5Xor2cvLyyg3Ho/jmpX5VHm229vbcbayXqZrxenpaVyz8r02NzfjbOXZVvatyjNL61Zqbm1txdm9vb04WxkzlXubXkNlHFbu13A4jLNvg3Sd6rraPU/n9C9/+cu45s9//vM4e3h4GGfPz8/jbGX9qfwe8ujRoyhXeV+/f/9+nD04OIizOzs7cfb69etxNl3/Knsm/DHxnwkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQNHrdFwB9Wa/XcXY4HMbZ999/P84+ePAgzt68eTPOfvzxx3H2zp07Ue7p06dxzXfeeSfO3rhxI87+13/9V5x999134yzwh1dZg/tar9O6q9Wql8/vy3g8jrN93Nv5fB7XrNzbvgwGg9d9CTRUns9sNouz29vbUW6xWMQ1K9nNzc04e3x8HGcnk0mcraxXe3t7Ua4ypyvrT+V7LZfLODsaXf3rbuXMfn5+Hmcr42tjI/+bwHQudF3t3qZ1K5+/tbUVZyt7YWUc7O/vx9nd3d04m+prjn3TTKfTOJuOj7t378Y1K+vEkydPesl++OGHcfZ73/tenP3oo4+i3AcffBDXrMy7yrmhsrdU1uu0rjnK28p/JgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAEDT6HVfAPRltVrF2dEonwrj8TjOfvDBB3F2e3s7zt68eTPO/va3v41yGxt5b3E2m8XZyWQSZz/66KM4u16v4+xisYizwNUYDAZxtrJe03XL5bKXup4Dr8N8Po+zlbNKmt3c3IxrVlTmU+UMeHZ2Fmcra0V6Vqrcr8p5sZLd2tqKs5W9aDqdRrnKGbTyvCrju6JyDyrvRKenp1Hu2bNncc3KenBwcBBnK9/r7t27cfb4+DjKVeZNZXynY/ZtUXmOlfUvfY6VOfro0aM4e/v27Thb8eDBgzj74sWLOPs///M/Ue773/9+XPO73/1unD08PIyzw+Gwl2xlH4C3kf9MAAAAAAAAmjQTAAAAAACAJs0EAAAAAACgSTMBAAAAAABo0kwAAAAAAACaNBMAAAAAAIAmzQQAAAAAAKBJMwEAAAAAAGjSTAAAAAAAAJpGr/sCoC+jUT68r127Fmfn83mcHY/HcXY6ncbZJ0+exNnlchnlDg4O4pp37tyJs4vFIs4eHx/H2Z2dnTgLALyZKueE3d3dOJueaypnj9lsFmcrZ7WTk5M4OxgM4mzlLHx6ehrl0nNl19WudTgcxtnK99ra2oqz6Xdbr9dXXrPranOh8j5ycXERZyvvI69evbrymkdHR3G28u6yt7cXZyvvhZVxm9rYyP/es5J9G1TGcmUsbW9vR7nK/f7yyy/j7MOHD+NsX/P5vffei7O3b9+OcpU5mj6DarayXlf2+MrvPPA2+mbtPgAAAAAAQJlmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANA0SoOr1SouOpvN4ux6vY6zUDEYDOLscrmMs/P5PM4Oh8M4++DBgzhb+W57e3tRrnKti8Uizl5eXsbZjY28v/nq1as4Ox6P42yqsnZVntc3jT0A4JutrzPYkydPrrxmxbVr1+JselbrutoZrPLdKue11NnZWZytnAF3dnZ+n8v5X+3u7ka5yrVWzoCVd+iKyjiYTqdx9t13341ylbNeZXxvbW3F2cp7TuUdYzTKfk6pvItsbm7G2W+aw8PDOPv111/H2YcPH0a5ypp269atOHvz5s04e/v27ThbmU+V603n9L179+Kad+/ejbMnJydxtnIPKut1uq76HYA3xVWPRf+ZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2jNHh+fh4XPT4+jrPr9TrOQsVgMIizq9Uqzs7n8zi7u7sbZ4fDYZzd2dmJs9vb21Hud7/7XVzzyZMncXZjI+9ZLhaLXur2obJ2LZfLHq/kzVOZT/YAgP/jm7gmVr7zdDqNs+mZonJenEwmcfbly5dxtnIPxuNxnK1cb3pevLy8jGseHBzE2a2trThbMRrFr7vxmKncg2vXrsXZzc3NOFs5W1beMSrn67Ozsyv//Mr3qqwHlXlTGTPp/aq841TuQeXM/TZ4+vRpnL1161acPTw8/D2upu3evXtxtvJbW+W3iHSOdl3XXVxcxNk+9ovKb4iV79XXb0KVuvAmuOr9wn8mAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0aSYAAAAAAABNmgkAAAAAAECTZgIAAAAAANCkmQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQNNgvV6vX/dFAAAAAAAAby7/mQAAAAAAADRpJgAAAAAAAE2aCQAAAAAAQJNmAgAAAAAA0KSZAAAAAAAANGkmAAAAAAAATZoJAAAAAABAk2YCAAAAAADQpJkAAAAAAAA0/X+ECVjdTXKuxgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1600x800 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Test reconstruction quality\n",
        "def visualize_reconstructions(val_ds, num_samples=4):\n",
        "    \"\"\"\n",
        "    Visualize original vs reconstructed images\n",
        "    \"\"\"\n",
        "    # Get a batch from validation set\n",
        "    for x_batch in val_ds.take(1):\n",
        "        sample_batch = x_batch[:num_samples]\n",
        "        break\n",
        "    \n",
        "    # Get reconstructions\n",
        "    D, bpp, bits_per_symbol, x_reconstructed = val_step(sample_batch)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(4*num_samples, 8))\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Original\n",
        "        axes[0,i].imshow(sample_batch[i,:,:,0], cmap='gray')\n",
        "        axes[0,i].set_title(f'Original {i+1}')\n",
        "        axes[0,i].axis('off')\n",
        "        \n",
        "        # Reconstructed\n",
        "        axes[1,i].imshow(x_reconstructed[i,:,:,0], cmap='gray')\n",
        "        axes[1,i].set_title(f'Reconstructed {i+1}')\n",
        "        axes[1,i].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Original vs Reconstructed (D={D:.6f}, BPP={bpp:.6f})')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_reconstructions(val_ds)\n",
        "\n",
        "# Optional: Train with different lambda values for rate-distortion curve\n",
        "def sweep_lambda_values(lambdas=[0.001, 0.01, 0.1, 1.0]):\n",
        "    \"\"\"\n",
        "    Train with different lambda values to create rate-distortion curve\n",
        "    \"\"\"\n",
        "    rd_points = []\n",
        "    \n",
        "    for lambda_val in lambdas:\n",
        "        print(f\"\\nTraining with lambda = {lambda_val}\")\n",
        "        \n",
        "        # Reset models (you might want to use different initializations)\n",
        "        encoder_temp = make_simple_encoder(latent_dim)\n",
        "        decoder_temp = make_simple_decoder(latent_dim)\n",
        "        \n",
        "        # Create new forward function with these models\n",
        "        @tf.function\n",
        "        def temp_forward(x, training=True):\n",
        "            y, mu, log_scale = encoder_temp(x, training=training)\n",
        "            bits_map, bits_per_symbol = rate_bits(y, mean=mu, log_scale=log_scale, training=training)\n",
        "            y_q = quantize_or_add_noise(y, training=training)\n",
        "            x_hat = decoder_temp(y_q, training=training)\n",
        "            D = tf.reduce_mean(tf.square(x - x_hat))\n",
        "            latent_size = tf.cast(tf.reduce_prod(tf.shape(y)[1:]), tf.float32)\n",
        "            image_size = tf.cast(H * W, tf.float32)\n",
        "            bpp = bits_per_symbol * (latent_size / image_size)\n",
        "            return D, bits_per_symbol, bpp, x_hat\n",
        "        \n",
        "        # Train for fewer epochs for sweep\n",
        "        history_temp = train_compression_network(train_ds, val_ds, epochs=10, lambda_rd=lambda_val)\n",
        "        \n",
        "        # Record final performance\n",
        "        final_rate = history_temp['val_rates'][-1]\n",
        "        final_distortion = history_temp['val_distortions'][-1]\n",
        "        rd_points.append((final_rate, final_distortion, lambda_val))\n",
        "    \n",
        "    # Plot rate-distortion curve\n",
        "    rates, distortions, lambdas_used = zip(*rd_points)\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(rates, distortions, 'o-', linewidth=2, markersize=8)\n",
        "    for i, lambda_val in enumerate(lambdas_used):\n",
        "        plt.annotate(f'={lambda_val}', (rates[i], distortions[i]), \n",
        "                    xytext=(5, 5), textcoords='offset points')\n",
        "    \n",
        "    plt.xlabel('Rate (Bits per Pixel)')\n",
        "    plt.ylabel('Distortion (MSE)')\n",
        "    plt.title('Rate-Distortion Curve')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    \n",
        "    return rd_points\n",
        "\n",
        "# Uncomment to run lambda sweep\n",
        "# rd_curve = sweep_lambda_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211622e8",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e582660a",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf_metal_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
